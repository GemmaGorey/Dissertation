{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfcEmShJ9RI9MrsmlbXN6A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Audio_Processing_V2_Dissertation_GG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Colab setup below - Run  top two cells once only per session**\n"
      ],
      "metadata": {
        "id": "71xmFnae_UIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# install mamba to use instead of pip"
      ],
      "metadata": {
        "id": "xKTq_n_S8czf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the config file and build the environment.\n",
        "yaml_content = \"\"\"\n",
        "name: dissertation\n",
        "channels:\n",
        "  - pytorch\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - pytorch=2.2.2\n",
        "  - torchvision=0.17.2\n",
        "  - torchaudio\n",
        "  - librosa\n",
        "  - numpy<2\n",
        "  - pandas\n",
        "  - jupyter\n",
        "  - wandb\n",
        "\"\"\"\n",
        "\n",
        "# Write the string content to a file -  'environment.yml'.\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"environment.yml file created successfully.\")\n",
        "\n",
        "# create the environment using mamba from the yml file.\n",
        "print(\"\\n Creating environment\")\n",
        "\n",
        "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
      ],
      "metadata": {
        "id": "tL3F9ih6E6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and setting up of GitHub and W&B\n",
        "\n",
        "# clone project repository from GitHub\n",
        "print(\"â³ Cloning GitHub repository...\")\n",
        "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
        "print(\"Repository cloned.\")\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import glob\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
        "print(\"Tokenizer loaded.\")"
      ],
      "metadata": {
        "id": "T9C4Mbhquwxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading in the dataset -using complete which has the lyrics and music.\n",
        "\n",
        "# create string for path to where dataset lives\n",
        "data_path = '/content/drive/MyDrive/dissertation/audio/'\n",
        "\n",
        "# load the files\n",
        "\n",
        "print(\"Loading MERGE Metadata \")\n",
        "merge_df = pd.read_csv(data_path + 'merge_bimodal_complete_metadata.csv')\n",
        "\n",
        "print(\"\\n Loading Valence-Arousal values\")\n",
        "av_df = pd.read_csv(data_path + 'merge_bimodal_complete_av_values.csv')\n",
        "\n",
        "\n",
        "print(\"\\n Datasets loaded successfully.\")\n",
        "\n",
        "# Inspect the files\n",
        "print(\"\\n First 5 rows of the data\")\n",
        "display(merge_df.head())\n",
        "\n",
        "print(\"\\n First 5 rows of the MERGE AV Values\")\n",
        "display(av_df.head())"
      ],
      "metadata": {
        "id": "IwjuR2S2Qx_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging the two dataframes using the common column\n",
        "\n",
        "final_df = pd.merge(merge_df, av_df, left_on='Audio_Song', right_on='Audio_Song')\n",
        "\n",
        "print(\" DataFrames merged\")\n",
        "\n",
        "print(\"\\n First 5 rows  MASTER DataFrame\")\n",
        "display(final_df.head())"
      ],
      "metadata": {
        "id": "elfwAb3oY9BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data - in different quadrants\n",
        "print(final_df['Quadrant'].value_counts())"
      ],
      "metadata": {
        "id": "Rz53JQWGZerE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data\n",
        "print(final_df[['Valence', 'Arousal']].describe())"
      ],
      "metadata": {
        "id": "krvFUFn4FOOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking no blank entries\n",
        "print(final_df[['Quadrant', 'Valence', 'Arousal']].info())"
      ],
      "metadata": {
        "id": "Dlk93Mn9Znyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#global parameters for the audio processing\n",
        "\n",
        "TARGET_SR = 22050 #used in the AllMusic dataset, which is where this original data is from.(may try 16k to see if it lowers accuracy)\n",
        "N_FFT = 2048#  using Librosa's (for music) standard values but can change to test\n",
        "HOP_LENGTH = 512 # using Librosa's (for music) standard values but can change to test\n",
        "N_MELS = 128 # using Librosa's (for music) standard values but can change to test\n",
        "\n",
        "def find_global_max_power(dataframe, base_audio_path):\n",
        "    \"\"\"\n",
        "    Loops through all audio files in the audio dataframe to find the\n",
        "    single maximum power value in any Mel spectrogram. This is used as a form of normalisation\n",
        "    \"\"\"\n",
        "    print(\"Finding global max\")\n",
        "    # Start off with a very small number\n",
        "    global_max_power = -np.inf\n",
        "\n",
        "    # Use tqdm for a progress bar\n",
        "    for index, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
        "        try:\n",
        "            song_id = row['Audio_Song']\n",
        "            quadrant = row['Quadrant']\n",
        "\n",
        "            # Construct the audio file path\n",
        "            audio_path = os.path.join(base_audio_path, quadrant, f\"{song_id}.mp3\")\n",
        "\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=TARGET_SR)\n",
        "\n",
        "            # Get spectrogram using the global variables\n",
        "            mel_spec_power = librosa.feature.melspectrogram(\n",
        "                y=y,\n",
        "                sr=sr,\n",
        "                n_fft=N_FFT,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                n_mels=N_MELS\n",
        "            )\n",
        "\n",
        "            # Find max in each one\n",
        "            current_max_power = np.max(mel_spec_power)\n",
        "\n",
        "            # Update global max if this is higher\n",
        "            if current_max_power > global_max_power:\n",
        "                global_max_power = current_max_power\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nSkipping file {song_id} due to error: {e}\")\n",
        "\n",
        "    print(\"\\n Global max found\")\n",
        "    return global_max_power\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s8O5f_1GPsdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_MAX = find_global_max_power(final_df, data_path)"
      ],
      "metadata": {
        "id": "FpRKASmeYxvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(GLOBAL_MAX)"
      ],
      "metadata": {
        "id": "63QhVUPqjpwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_MAX = 13400.846\n",
        "\n",
        "#Output from before so don't have to run each time - This is the power output max"
      ],
      "metadata": {
        "id": "xA16jQBMt98v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_global_max_and_outlier(dataframe, base_audio_path):\n",
        "    \"\"\"\n",
        "    Loops through all audio files to find the single maximum power value\n",
        "    and prints the song ID responsible for each new maximum.\n",
        "    \"\"\"\n",
        "\n",
        "    # Start off with a very small number\n",
        "    global_max_power = -np.inf\n",
        "    outlier_file = \"None\"\n",
        "    outlier_quadrant = \"None\"\n",
        "\n",
        "    # Use tqdm for a progress bar\n",
        "    for index, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
        "        try:\n",
        "            song_id = row['Audio_Song']\n",
        "            quadrant = row['Quadrant']\n",
        "\n",
        "            # Construct the audio file path\n",
        "            audio_path = os.path.join(base_audio_path, quadrant, f\"{song_id}.mp3\")\n",
        "\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=TARGET_SR)\n",
        "\n",
        "            # Check for silent/corrupt files\n",
        "            if np.max(np.abs(y)) < 0.001:\n",
        "                # This skips (near) silent files, which can also be a problem\n",
        "                continue\n",
        "\n",
        "            # Get spectrogram using the global variables\n",
        "            mel_spec_power = librosa.feature.melspectrogram(\n",
        "                y=y,\n",
        "                sr=sr,\n",
        "                n_fft=N_FFT,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                n_mels=N_MELS\n",
        "            )\n",
        "\n",
        "            # Find max in each one\n",
        "            current_max_power = np.max(mel_spec_power)\n",
        "\n",
        "            # Update global max if this is higher\n",
        "            if current_max_power > global_max_power:\n",
        "                global_max_power = current_max_power\n",
        "                outlier_file = song_id\n",
        "                outlier_quadrant = quadrant\n",
        "                print(f\"\\nNew Max Found: {global_max_power:.2f}\")\n",
        "                print(f\"   (From Song ID: {outlier_file} in Quadrant: {outlier_quadrant})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # This will also catch corrupt files that librosa can't even load\n",
        "            print(f\"\\nSKIPPING FILE {song_id} due to error: {e}\")\n",
        "\n",
        "    print(\"\\nComplete\")\n",
        "    print(f\"The HIGHEST max power was: {global_max_power:.2f}\")\n",
        "    print(f\"This value came from the file: {outlier_file} (Quadrant: {outlier_quadrant})\")\n",
        "\n",
        "    return global_max_power, outlier_file\n",
        "\n",
        "    #DECIDED OFF THE BACK OF THIS EXPERIMENT NOT TO USE THE GLOBAL MAX. PUSHES THE SCALE TO NEGATIVE WHICH IS NEARER THE FLOOR OF -80."
      ],
      "metadata": {
        "id": "Du3jkFEHqQpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_MAX_OUTLIER = find_global_max_and_outlier(final_df, data_path)"
      ],
      "metadata": {
        "id": "mG2DFUJ3ql-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_song_data(song_id, lyric_id, quadrant):\n",
        "    \"\"\"\n",
        "    Loads the audio and lyrics for a given song ID\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load song: {song_id}\")\n",
        "    try:\n",
        "        # Construct the file path to google drive\n",
        "        base_path = '/content/drive/MyDrive/dissertation/'\n",
        "\n",
        "        # Audio files url addition as these are in subfolders for each emotion quadrant\n",
        "        audio_path = os.path.join(base_path, 'audio', quadrant, f\"{song_id}.mp3\")\n",
        "\n",
        "        # Lyric files url addition as these are in a separate main folder\n",
        "        lyrics_path = os.path.join(base_path, 'lyrics', quadrant, f\"{lyric_id}.txt\")\n",
        "\n",
        "        # load the audio file\n",
        "        audio_waveform, sample_rate = librosa.load(audio_path, sr=TARGET_SR)\n",
        "\n",
        "        #Process Audio into a Mel Spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_waveform, sr=sample_rate, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
        "\n",
        "        # Convert to decibels\n",
        "        db_spectrogram = librosa.power_to_db(mel_spectrogram, ref=GLOBAL_MAX)\n",
        "\n",
        "        target_length = 1292  # A fixed length for all spectrograms, so same length tensors later - chosen for 30 seconds, and sr of 22050, and hop_length of 512\n",
        "        current_length = db_spectrogram.shape[1]\n",
        "\n",
        "        if current_length > target_length:\n",
        "            # If it is too long, truncate it\n",
        "            db_spectrogram = db_spectrogram[:, :target_length]\n",
        "        elif current_length < target_length:\n",
        "            # If too short, pad it\n",
        "            padding_needed = target_length - current_length\n",
        "            db_spectrogram = np.pad(db_spectrogram,\n",
        "                                    ((0, 0), (0, padding_needed)),\n",
        "                                    mode='constant',\n",
        "                                    constant_values=-80.0) #close to silence so doesn't impact\n",
        "\n",
        "        # load lyrics text\n",
        "        with open(lyrics_path, 'r', encoding='utf-8') as f:\n",
        "            lyrics_text = f.read()\n",
        "\n",
        "        #Tokenise raw text\n",
        "        encoded_lyrics = tokenizer(\n",
        "            lyrics_text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        print(f\"Successfully loaded and processed {song_id}\")\n",
        "        #returns the spectrogram and the tokenised lyrics\n",
        "        return db_spectrogram, encoded_lyrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for {song_id}: {e}\")\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "sZN4X9A7X7Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#testing the output of the above function by picking a song.\n",
        "\n",
        "#pick a song between 0 and 2215\n",
        "test_song_index = 1111\n",
        "\n",
        "test_audio_id = final_df.iloc[test_song_index]['Audio_Song']\n",
        "test_lyric_id = final_df.iloc[test_song_index]['Lyric_Song_x']\n",
        "test_quadrant = final_df.iloc[test_song_index]['Quadrant']\n",
        "\n",
        "print(f\"Audio ID: {test_audio_id}\")\n",
        "print(f\"Lyric ID: {test_lyric_id}\")\n",
        "print(f\"Quadrant: {test_quadrant}\")\n",
        "\n",
        "#variables for the spectorgram and tokenised lyrics loaded.\n",
        "spectrogram, encoded_lyrics = load_song_data(test_audio_id, test_lyric_id, test_quadrant)\n",
        "\n",
        "\n",
        "#check the output of the lyrics\n",
        "if encoded_lyrics is not None:\n",
        "    print(\"\\n--- Tokenized Lyrics Output ---\")\n",
        "\n",
        "    #dictionary of tensors\n",
        "    print(\"Encoded Tensors:\")\n",
        "    display(encoded_lyrics)\n",
        "\n",
        "    #readable tokens to check\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded_lyrics['input_ids'][0])\n",
        "\n",
        "    print(\"\\nSample of Tokens Produced:\")\n",
        "    print(tokens[:50])\n",
        "\n",
        "#check the output of the audio\n",
        "if spectrogram is not None:\n",
        "    print(\"\\n--- Spectrogram Data ---\")\n",
        "    print(f\"Shape: {spectrogram.shape}\")\n",
        "\n",
        "    #show spectogram\n",
        "    print(\"\\nDisplaying Spectrogram:\")\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    #get the sample rate for the axis display\n",
        "    audio_file_path = os.path.join('/content/drive/MyDrive/dissertation/audio', test_quadrant, f'{test_audio_id}.mp3')\n",
        "    sr = librosa.get_samplerate(path=audio_file_path)\n",
        "\n",
        "    librosa.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='mel')\n",
        "\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(f'Mel Spectrogram for {test_audio_id}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jjoQwck_G-qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#define where to save the files\n",
        "base_path = '/content/drive/MyDrive/dissertation/merge_dataset/output_from_code/'\n",
        "spectrogram_save_dir = os.path.join(base_path, 'processed_spectrograms/')\n",
        "lyrics_save_dir = os.path.join(base_path, 'processed_lyrics/')\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(spectrogram_save_dir, exist_ok=True)\n",
        "os.makedirs(lyrics_save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Spectrograms will be saved in: {spectrogram_save_dir}\")\n",
        "print(f\"Lyric tensors will be saved in: {lyrics_save_dir}\")\n",
        "\n",
        "#process the audio and lyrics storing info as processed\n",
        "processed_records = []\n",
        "\n",
        "#iterate over all rows and get origress bar\n",
        "for index, row in tqdm(final_df.iterrows(), total=final_df.shape[0], desc=\"Processing Songs\"):\n",
        "\n",
        "    #get song info from each row\n",
        "    song_id = row['Audio_Song']\n",
        "    lyric_id = row['Lyric_Song_x']\n",
        "    quadrant = row['Quadrant']\n",
        "\n",
        "    #call function to transform\n",
        "    spectrogram, encoded_lyrics = load_song_data(song_id, lyric_id, quadrant)\n",
        "\n",
        "    #check if it worked\n",
        "    if spectrogram is not None and encoded_lyrics is not None:\n",
        "        try:\n",
        "            #save the spectorgram\n",
        "            spectrogram_path = os.path.join(spectrogram_save_dir, f\"{song_id}.npy\")\n",
        "            np.save(spectrogram_path, spectrogram)\n",
        "\n",
        "            #save tokenised lyric tensors\n",
        "            lyrics_path = os.path.join(lyrics_save_dir, f\"{song_id}.pt\")\n",
        "            torch.save(encoded_lyrics, lyrics_path)\n",
        "\n",
        "            #record file path\n",
        "            processed_records.append({\n",
        "                'song_id': song_id,\n",
        "                'spectrogram_path': spectrogram_path,\n",
        "                'lyrics_path': lyrics_path,\n",
        "                'valence': row['Valence'],\n",
        "                'arousal': row['Arousal'],\n",
        "                'quadrant': quadrant\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving files for {song_id}: {e}\")\n",
        "\n",
        "\n",
        "#save new dataframe for using in experiments\n",
        "processed_df = pd.DataFrame(processed_records)\n",
        "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
        "processed_df.to_csv(master_file_path, index=False)\n",
        "\n",
        "print(\"\\n PROCESSING COMPLETE!\")\n",
        "print(f\"Saved {len(processed_df)} records.\")\n",
        "print(f\"Master file list saved to: {master_file_path}\")\n",
        "display(processed_df.head())"
      ],
      "metadata": {
        "id": "6lPRDJvGl4Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#path to where this was saved\n",
        "base_path = '/content/drive/MyDrive/dissertation/merge_dataset/output_from_code/'\n",
        "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
        "\n",
        "#load the master csv with all the paths and VA values\n",
        "master_df = pd.read_csv(master_file_path)\n",
        "\n",
        "print(\"Master csv loaded\")\n",
        "display(master_df.head())\n",
        "\n",
        "#check data for 1 song\n",
        "#pick a song between 0 and 2215\n",
        "test_final_song_index = 1111\n",
        "song_info = master_df.iloc[test_final_song_index]\n",
        "\n",
        "print(f\"\\nLoading data for song: {song_info['song_id']}\")\n",
        "\n",
        "#Load the spectrogram from the file\n",
        "spectrogram = np.load(song_info['spectrogram_path'])\n",
        "\n",
        "#load the lyric tensors\n",
        "encoded_lyrics = torch.load(song_info['lyrics_path'], weights_only=False)\n",
        "\n",
        "#Get the labels\n",
        "valence = song_info['valence']\n",
        "arousal = song_info['arousal']\n",
        "\n",
        "#check the data\n",
        "print(\"Spectrogram Shape:\", spectrogram.shape)\n",
        "print(\"Encoded Lyrics Tensors:\", encoded_lyrics)\n",
        "print(f\"Labels - Valence: {valence}, Arousal: {arousal}\")\n"
      ],
      "metadata": {
        "id": "Li06X-Gly-KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "\n",
        "# Load Master CSV File\n",
        "print(\"Loading Master CSV...\")\n",
        "base_path = '/content/drive/MyDrive/dissertation/merge_dataset/output_from_code/'\n",
        "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
        "\n",
        "try:\n",
        "    master_df = pd.read_csv(master_file_path)\n",
        "    print(f\"Successfully loaded {len(master_df)} file records.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Could not find master file at {master_file_path}\")\n",
        "    master_df = pd.DataFrame(columns=['song_id', 'spectrogram_path'])\n",
        "\n",
        "if not master_df.empty:\n",
        "    BATCH_SIZE = 24\n",
        "    COLS = 6\n",
        "    ROWS_PER_BATCH = math.ceil(BATCH_SIZE / COLS) # This will be 4\n",
        "\n",
        "    num_batches = math.ceil(len(master_df) / BATCH_SIZE)\n",
        "\n",
        "    print(f\"Total items: {len(master_df)}. Batch size: {BATCH_SIZE}. Total batches: {num_batches}.\")\n",
        "\n",
        "    for batch_num in range(num_batches):\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Displaying Batch {batch_num + 1} of {num_batches}\")\n",
        "\n",
        "        start_index = batch_num * BATCH_SIZE\n",
        "        end_index = min((batch_num + 1) * BATCH_SIZE, len(master_df))\n",
        "        batch_df = master_df.iloc[start_index:end_index]\n",
        "\n",
        "        fig, axs = plt.subplots(ROWS_PER_BATCH, COLS,\n",
        "                                figsize=(COLS * 3.5, ROWS_PER_BATCH * 3),\n",
        "                                constrained_layout=True)\n",
        "\n",
        "        if not isinstance(axs, np.ndarray):\n",
        "            axs = np.array([axs])\n",
        "        axs_flat = axs.flatten()\n",
        "\n",
        "        img = None\n",
        "\n",
        "        for index, (master_idx, row) in enumerate(batch_df.iterrows()):\n",
        "            ax = axs_flat[index]\n",
        "            song_id = row['song_id']\n",
        "            spec_path = row['spectrogram_path']\n",
        "\n",
        "            try:\n",
        "                spectrogram = np.load(spec_path)\n",
        "\n",
        "                img = librosa.display.specshow(\n",
        "                    spectrogram,\n",
        "                    sr=TARGET_SR,\n",
        "                    hop_length=HOP_LENGTH,\n",
        "                    x_axis='time',\n",
        "                    y_axis='mel',\n",
        "                    ax=ax\n",
        "                )\n",
        "\n",
        "                ax.set_title(song_id, fontsize=8)\n",
        "\n",
        "            except Exception as e:\n",
        "                ax.set_title(f\"Error loading {song_id}\", fontsize=8)\n",
        "                ax.text(0.5, 0.5, str(e), ha='center', va='center', wrap=True, fontsize=6, color='red')\n",
        "\n",
        "            ax.set_xlabel(None)\n",
        "            ax.set_ylabel(None)\n",
        "\n",
        "        num_items_in_batch = len(batch_df)\n",
        "        for i in range(num_items_in_batch, len(axs_flat)):\n",
        "            axs_flat[i].axis('off')\n",
        "\n",
        "        if img is not None:\n",
        "            fig.colorbar(img, ax=axs.ravel().tolist(), format='%+2.0f dB', shrink=0.8)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. No plots to generate.\")\n"
      ],
      "metadata": {
        "id": "yLev7E60IH2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}