{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNwvG34ndTuO+tvb3j7M16K",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Dissertation_GG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Initial Colab setup below - Run  top two cells once only per session**\n"
   ],
   "metadata": {
    "id": "71xmFnae_UIi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "# install mamba to use instead of pip"
   ],
   "metadata": {
    "id": "xKTq_n_S8czf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the config file and build the environment.\n",
    "yaml_content = \"\"\"\n",
    "name: dissertation\n",
    "channels:\n",
    "  - pytorch\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pytorch=2.2.2\n",
    "  - torchvision=0.17.2\n",
    "  - torchaudio\n",
    "  - librosa\n",
    "  - numpy<2\n",
    "  - pandas\n",
    "  - jupyter\n",
    "  - wandb\n",
    "\"\"\"\n",
    "\n",
    "# Write the string content to a file -  'environment.yml'.\n",
    "with open('environment.yml', 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"environment.yml file created successfully.\")\n",
    "\n",
    "# create the environment using mamba from the yml file.\n",
    "print(\"\\n Creating environment\")\n",
    "\n",
    "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
   ],
   "metadata": {
    "id": "tL3F9ih6E6ih"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# imports and setting up of GitHub and W&B\n",
    "\n",
    "# clone project repository from GitHub\n",
    "print(\"⏳ Cloning GitHub repository...\")\n",
    "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
    "print(\"Repository cloned.\")\n",
    "\n",
    "#Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
    "print(\"Tokenizer loaded.\")"
   ],
   "metadata": {
    "id": "T9C4Mbhquwxi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#loading in the dataset -using complete which has the lyrics and music.\n",
    "\n",
    "\n",
    "\n",
    "# create string for path to where dataset lives\n",
    "data_path = '/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete/'\n",
    "\n",
    "# load the files\n",
    "\n",
    "print(\"Loading MERGE Metadata \")\n",
    "merge_df = pd.read_csv(data_path + 'merge_bimodal_complete_metadata.csv')\n",
    "\n",
    "print(\"\\n Loading Valence-Arousal values\")\n",
    "av_df = pd.read_csv(data_path + 'merge_bimodal_complete_av_values.csv')\n",
    "\n",
    "\n",
    "print(\"\\n Datasets loaded successfully.\")\n",
    "\n",
    "# Inspect the files\n",
    "print(\"\\n First 5 rows of the data\")\n",
    "display(merge_df.head())\n",
    "\n",
    "print(\"\\n First 5 rows of the MERGE AV Values\")\n",
    "display(av_df.head())"
   ],
   "metadata": {
    "id": "IwjuR2S2Qx_v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# merging the two dataframes using the common column\n",
    "\n",
    "final_df = pd.merge(merge_df, av_df, left_on='Audio_Song', right_on='Audio_Song')\n",
    "\n",
    "print(\" DataFrames merged\")\n",
    "\n",
    "print(\"\\n First 5 rows  MASTER DataFrame\")\n",
    "display(final_df.head())"
   ],
   "metadata": {
    "id": "elfwAb3oY9BU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#checking data - in different quadrants\n",
    "print(final_df['Quadrant'].value_counts())"
   ],
   "metadata": {
    "id": "Rz53JQWGZerE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#checking data\n",
    "print(final_df[['Valence', 'Arousal']].describe())"
   ],
   "metadata": {
    "id": "krvFUFn4FOOu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#checking no blank entries\n",
    "print(final_df[['Quadrant', 'Valence', 'Arousal']].info())"
   ],
   "metadata": {
    "id": "Dlk93Mn9Znyz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_song_data(song_id, lyric_id, quadrant):\n",
    "    \"\"\"\n",
    "    Loads the audio and lyrics for a given song ID\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load song: {song_id}\")\n",
    "    try:\n",
    "        # Construct the file path to google drive\n",
    "        base_path = '/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete'\n",
    "\n",
    "        # Audio files url addition as these are in subfolders for each emotion quadrant\n",
    "        audio_path = os.path.join(base_path, 'audio', quadrant, f\"{song_id}.mp3\")\n",
    "\n",
    "        # Lyric files url addition as these are in a separate main folder\n",
    "        lyrics_path = os.path.join(base_path, 'lyrics', quadrant, f\"{lyric_id}.txt\")\n",
    "\n",
    "        # load the audio file\n",
    "        TARGET_SR = 22050 #used in the AllMusic dataset, which is where this original data is from.\n",
    "        audio_waveform, sample_rate = librosa.load(audio_path, sr=TARGET_SR) # might try changing to 16khz later on as read that this does not impact the model but helps performance\n",
    "\n",
    "        #Process Audio into a Mel Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_waveform, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128) # using Librosa's (for music) standard values but can change to test\n",
    "\n",
    "        # Convert to decibels\n",
    "        db_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        target_length = 1292  # A fixed length for all spectrograms, so same length tensors later - chosen for 30 seconds, and sr of 22050, and hop_length of 512\n",
    "        current_length = db_spectrogram.shape[1]\n",
    "\n",
    "        if current_length > target_length:\n",
    "            # If it is too long, truncate it\n",
    "            db_spectrogram = db_spectrogram[:, :target_length]\n",
    "        elif current_length < target_length:\n",
    "            # If too short, pad it\n",
    "            padding_needed = target_length - current_length\n",
    "            db_spectrogram = np.pad(db_spectrogram, ((0, 0), (0, padding_needed)), mode='constant')\n",
    "\n",
    "        # load lyrics text\n",
    "        with open(lyrics_path, 'r', encoding='utf-8') as f:\n",
    "            lyrics_text = f.read()\n",
    "\n",
    "        #Tokenise raw text\n",
    "        encoded_lyrics = tokenizer(\n",
    "            lyrics_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        print(f\"Successfully loaded and processed {song_id}\")\n",
    "        #returns the spectrogram and the tokenised lyrics\n",
    "        return db_spectrogram, encoded_lyrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {song_id}: {e}\")\n",
    "        return None, None\n"
   ],
   "metadata": {
    "id": "sZN4X9A7X7Qg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#testing the output of the above function by picking a song.\n",
    "\n",
    "#pick a song between 0 and 2215\n",
    "test_song_index = 1111\n",
    "\n",
    "test_audio_id = final_df.iloc[test_song_index]['Audio_Song']\n",
    "test_lyric_id = final_df.iloc[test_song_index]['Lyric_Song_x']\n",
    "test_quadrant = final_df.iloc[test_song_index]['Quadrant']\n",
    "\n",
    "print(f\"Audio ID: {test_audio_id}\")\n",
    "print(f\"Lyric ID: {test_lyric_id}\")\n",
    "print(f\"Quadrant: {test_quadrant}\")\n",
    "\n",
    "#variables for the spectorgram and tokenised lyrics loaded.\n",
    "spectrogram, encoded_lyrics = load_song_data(test_audio_id, test_lyric_id, test_quadrant)\n",
    "\n",
    "\n",
    "#check the output of the lyrics\n",
    "if encoded_lyrics is not None:\n",
    "    print(\"\\n--- Tokenized Lyrics Output ---\")\n",
    "\n",
    "    #dictionary of tensors\n",
    "    print(\"Encoded Tensors:\")\n",
    "    display(encoded_lyrics)\n",
    "\n",
    "    #readable tokens to check\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_lyrics['input_ids'][0])\n",
    "\n",
    "    print(\"\\nSample of Tokens Produced:\")\n",
    "    print(tokens[:50])\n",
    "\n",
    "#check the output of the audio\n",
    "if spectrogram is not None:\n",
    "    print(\"\\n--- Spectrogram Data ---\")\n",
    "    print(f\"Shape: {spectrogram.shape}\")\n",
    "\n",
    "    #show spectogram\n",
    "    print(\"\\nDisplaying Spectrogram:\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    #get the sample rate for the axis display\n",
    "    audio_file_path = os.path.join('/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete/audio', test_quadrant, f'{test_audio_id}.mp3')\n",
    "    sr = librosa.get_samplerate(path=audio_file_path)\n",
    "\n",
    "    librosa.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='mel')\n",
    "\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Mel Spectrogram for {test_audio_id}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "jjoQwck_G-qt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#define where to save the files\n",
    "base_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
    "spectrogram_save_dir = os.path.join(base_path, 'processed_spectrograms/')\n",
    "lyrics_save_dir = os.path.join(base_path, 'processed_lyrics/')\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(spectrogram_save_dir, exist_ok=True)\n",
    "os.makedirs(lyrics_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Spectrograms will be saved in: {spectrogram_save_dir}\")\n",
    "print(f\"Lyric tensors will be saved in: {lyrics_save_dir}\")\n",
    "\n",
    "#process the audio and lyrics storing info as processed\n",
    "processed_records = []\n",
    "\n",
    "#iterate over all rows and get origress bar\n",
    "for index, row in tqdm(final_df.iterrows(), total=final_df.shape[0], desc=\"Processing Songs\"):\n",
    "\n",
    "    #get song info from each row\n",
    "    song_id = row['Audio_Song']\n",
    "    lyric_id = row['Lyric_Song_x']\n",
    "    quadrant = row['Quadrant']\n",
    "\n",
    "    #call function to transform\n",
    "    spectrogram, encoded_lyrics = load_song_data(song_id, lyric_id, quadrant)\n",
    "\n",
    "    #check if it worked\n",
    "    if spectrogram is not None and encoded_lyrics is not None:\n",
    "        try:\n",
    "            #save the spectorgram\n",
    "            spectrogram_path = os.path.join(spectrogram_save_dir, f\"{song_id}.npy\")\n",
    "            np.save(spectrogram_path, spectrogram)\n",
    "\n",
    "            #save tokenised lyric tensors\n",
    "            lyrics_path = os.path.join(lyrics_save_dir, f\"{song_id}.pt\")\n",
    "            torch.save(encoded_lyrics, lyrics_path)\n",
    "\n",
    "            #record file path\n",
    "            processed_records.append({\n",
    "                'song_id': song_id,\n",
    "                'spectrogram_path': spectrogram_path,\n",
    "                'lyrics_path': lyrics_path,\n",
    "                'valence': row['Valence'],\n",
    "                'arousal': row['Arousal'],\n",
    "                'quadrant': quadrant\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving files for {song_id}: {e}\")\n",
    "\n",
    "\n",
    "#save new dataframe for using in experiments\n",
    "processed_df = pd.DataFrame(processed_records)\n",
    "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
    "processed_df.to_csv(master_file_path, index=False)\n",
    "\n",
    "print(\"\\n PROCESSING COMPLETE!\")\n",
    "print(f\"Saved {len(processed_df)} records.\")\n",
    "print(f\"Master file list saved to: {master_file_path}\")\n",
    "display(processed_df.head())"
   ],
   "metadata": {
    "id": "6lPRDJvGl4Gl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#path to where this was saved\n",
    "base_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
    "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
    "\n",
    "#load the master csv with all the paths and VA values\n",
    "master_df = pd.read_csv(master_file_path)\n",
    "\n",
    "print(\"Master csv loaded\")\n",
    "display(master_df.head())\n",
    "\n",
    "#check data for 1 song\n",
    "#pick a song between 0 and 2215\n",
    "test_final_song_index = 1111\n",
    "song_info = master_df.iloc[test_final_song_index]\n",
    "\n",
    "print(f\"\\n--- Loading data for song: {song_info['song_id']} ---\")\n",
    "\n",
    "#Load the spectrogram from the file\n",
    "spectrogram = np.load(song_info['spectrogram_path'])\n",
    "\n",
    "#load the lyric tensors\n",
    "encoded_lyrics = torch.load(song_info['lyrics_path'], weights_only=False)\n",
    "\n",
    "#Get the labels\n",
    "valence = song_info['valence']\n",
    "arousal = song_info['arousal']\n",
    "\n",
    "#check the data\n",
    "print(\"Spectrogram Shape:\", spectrogram.shape)\n",
    "print(\"Encoded Lyrics Tensors:\", encoded_lyrics)\n",
    "print(f\"Labels - Valence: {valence}, Arousal: {arousal}\")\n"
   ],
   "metadata": {
    "id": "Li06X-Gly-KP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model Training Notebooks\n\nAfter preprocessing the data using this notebook, five different model architectures were developed and trained. Each model builds upon the previous one with incremental improvements:\n\n## Model Progression Summary\n\n### MODEL 1: Baseline\n- Basic VGG-style CNN (single conv per block: 64→128→256→512)\n- Frozen BERT for lyrics processing\n- Basic training loop with no advanced features\n- **Purpose**: Establish baseline performance\n\n### MODEL 2: Training Stability\n**Additions over Model 1:**\n- ✅ Batch normalization after each convolutional layer\n- ✅ Gradient clipping (max_norm=1.0) for training stability\n- ✅ Early stopping (patience=10) to prevent overfitting\n- ✅ Learning rate scheduler (StepLR: step_size=15, gamma=0.5)\n- **Purpose**: Improve training stability and convergence\n\n### MODEL 3: Attention Mechanism\n**Additions over Model 2:**\n- ✅ Attention module on 256-dimensional features\n- ✅ Squeeze-and-excitation style attention (256→64→256)\n- ✅ Retains all Model 2 stability features\n- **Purpose**: Learn to weight feature importance\n\n### MODEL 4: True VGG Architecture\n**Additions over Model 3:**\n- ✅ Double convolutions per block (true VGG-style)\n  - Each block: Conv→BN→ReLU→Conv→BN→ReLU→Pool\n- ✅ Deeper feature extraction\n- ✅ Retains attention and all stability features\n- **Purpose**: Increase model capacity with deeper architecture\n\n### MODEL 5: Fine-tuned BERT\n**Additions over Model 3:**\n- ✅ **Unfrozen BERT** (trainable lyrics tower)\n- ✅ **Differential learning rates**:\n  - Audio tower: 0.001\n  - BERT lyrics tower: 0.00001 (100x smaller for careful fine-tuning)\n  - Classifier head: 0.001\n- ✅ Increased patience to 25 epochs (for BERT adaptation)\n- ✅ Single-conv blocks (focus on BERT improvements)\n- **Purpose**: Allow BERT to adapt to music emotion task\n\n## Feature Comparison Table\n\n| Feature | M1 | M2 | M3 | M4 | M5 |\n|---------|----|----|----|----|---- |\n| Batch Normalization | ❌ | ✅ | ✅ | ✅ | ✅ |\n| Gradient Clipping | ❌ | ✅ | ✅ | ✅ | ✅ |\n| Early Stopping | ❌ | ✅ | ✅ | ✅ | ✅ |\n| LR Scheduler | ❌ | ✅ | ✅ | ✅ | ✅ |\n| Attention | ❌ | ❌ | ✅ | ✅ | ✅ |\n| Double Conv Blocks | ❌ | ❌ | ❌ | ✅ | ❌ |\n| Trainable BERT | ❌ | ❌ | ❌ | ❌ | ✅ |\n| Diff. Learning Rates | ❌ | ❌ | ❌ | ❌ | ✅ |\n\n## Training Configuration\n\nAll models use:\n- **Loss Function**: MSE (Mean Squared Error) for regression\n- **Optimizer**: Adam (with differential rates in Model 5)\n- **Batch Size**: 16\n- **Max Epochs**: 50\n- **Dataset Split**: 70/15/15 (train/val/test) using predefined MERGE splits\n- **Evaluation Metrics**: MSE, MAE, R² for both Valence and Arousal\n\n## Model Files\n\nThe trained models are saved in separate notebooks:\n- `MODEL_1_Dissertation_model_training_basic_VGGish.ipynb`\n- `MODEL_2_Dissertation_model_training_basic_VGGish.ipynb`\n- `MODEL_3_Dissertation_model_training_basic_VGGish.ipynb`\n- `MODEL_4_Dissertation_model_training_basic_VGGish.ipynb`\n- `MODEL_5_Dissertation_model_training_basic_VGGish_.ipynb`",
   "metadata": {}
  }
 ]
}