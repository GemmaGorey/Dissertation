{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2sFPcvwnKZj+Fwxs27HRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Dissertation_GG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Colab setup below - Run  top two cells once only per session**\n"
      ],
      "metadata": {
        "id": "71xmFnae_UIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# install mamba to use instead of pip"
      ],
      "metadata": {
        "id": "xKTq_n_S8czf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the config file and build the environment.\n",
        "yaml_content = \"\"\"\n",
        "name: dissertation\n",
        "channels:\n",
        "  - pytorch\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - pytorch=2.2.2\n",
        "  - torchvision=0.17.2\n",
        "  - torchaudio\n",
        "  - librosa\n",
        "  - numpy<2\n",
        "  - pandas\n",
        "  - jupyter\n",
        "  - wandb\n",
        "\"\"\"\n",
        "\n",
        "# Write the string content to a file -  'environment.yml'.\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"environment.yml file created successfully.\")\n",
        "\n",
        "# create the environment using mamba from the yml file.\n",
        "print(\"\\n Creating environment\")\n",
        "\n",
        "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
      ],
      "metadata": {
        "id": "tL3F9ih6E6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and setting up of GitHub and W&B\n",
        "\n",
        "# clone project repository from GitHub\n",
        "print(\"⏳ Cloning GitHub repository...\")\n",
        "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
        "print(\"Repository cloned.\")\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
        "print(\"Tokenizer loaded.\")"
      ],
      "metadata": {
        "id": "T9C4Mbhquwxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading in the dataset -using complete which has the lyrics and music.\n",
        "\n",
        "\n",
        "\n",
        "# create string for path to where dataset lives\n",
        "data_path = '/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete/'\n",
        "\n",
        "# load the files\n",
        "\n",
        "print(\"Loading MERGE Metadata \")\n",
        "merge_df = pd.read_csv(data_path + 'merge_bimodal_complete_metadata.csv')\n",
        "\n",
        "print(\"\\n Loading Valence-Arousal values\")\n",
        "av_df = pd.read_csv(data_path + 'merge_bimodal_complete_av_values.csv')\n",
        "\n",
        "\n",
        "print(\"\\n Datasets loaded successfully.\")\n",
        "\n",
        "# Inspect the files\n",
        "print(\"\\n First 5 rows of the data\")\n",
        "display(merge_df.head())\n",
        "\n",
        "print(\"\\n First 5 rows of the MERGE AV Values\")\n",
        "display(av_df.head())"
      ],
      "metadata": {
        "id": "IwjuR2S2Qx_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging the two dataframes using the common column\n",
        "\n",
        "final_df = pd.merge(merge_df, av_df, left_on='Audio_Song', right_on='Audio_Song')\n",
        "\n",
        "print(\" DataFrames merged\")\n",
        "\n",
        "print(\"\\n First 5 rows  MASTER DataFrame\")\n",
        "display(final_df.head())"
      ],
      "metadata": {
        "id": "elfwAb3oY9BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data - in different quadrants\n",
        "print(final_df['Quadrant'].value_counts())"
      ],
      "metadata": {
        "id": "Rz53JQWGZerE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data\n",
        "print(final_df[['Valence', 'Arousal']].describe())"
      ],
      "metadata": {
        "id": "krvFUFn4FOOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking no blank entries\n",
        "print(final_df[['Quadrant', 'Valence', 'Arousal']].info())"
      ],
      "metadata": {
        "id": "Dlk93Mn9Znyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_song_data(song_id, lyric_id, quadrant):\n",
        "    \"\"\"\n",
        "    Loads the audio and lyrics for a given song ID\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load song: {song_id}\")\n",
        "    try:\n",
        "        # Construct the file path to google drive\n",
        "        base_path = '/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete'\n",
        "\n",
        "        # Audio files url addition as these are in subfolders for each emotion quadrant\n",
        "        audio_path = os.path.join(base_path, 'audio', quadrant, f\"{song_id}.mp3\")\n",
        "\n",
        "        # Lyric files url addition as these are in a separate main folder\n",
        "        lyrics_path = os.path.join(base_path, 'lyrics', quadrant, f\"{lyric_id}.txt\")\n",
        "\n",
        "        # load the audio file\n",
        "        audio_waveform, sample_rate = librosa.load(audio_path, sr=None) #preserve original sample rate\n",
        "\n",
        "        #Process Audio into a Mel Spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_waveform, sr=sample_rate)\n",
        "\n",
        "        # Convert to decibels\n",
        "        db_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "        # load lyrics text\n",
        "        with open(lyrics_path, 'r', encoding='utf-8') as f:\n",
        "            lyrics_text = f.read()\n",
        "#TODO CHECK ALL THESE PARAMETERS AND SEE IF THESE ARE STANDARD OR IF I NEED TO CHANGE ANYTHING\n",
        "\n",
        "        #Tokenise raw text\n",
        "        encoded_lyrics = tokenizer(\n",
        "            lyrics_text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        print(f\"Successfully loaded and processed {song_id}\")\n",
        "        #returns the spectrogram and the tokenised lyrics\n",
        "        return db_spectrogram, encoded_lyrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for {song_id}: {e}\")\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "sZN4X9A7X7Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#testing the output of the above function by picking a song.\n",
        "\n",
        "#pick a song between 0 and 2215\n",
        "test_song_index = 1111\n",
        "\n",
        "test_audio_id = final_df.iloc[test_song_index]['Audio_Song']\n",
        "test_lyric_id = final_df.iloc[test_song_index]['Lyric_Song_x']\n",
        "test_quadrant = final_df.iloc[test_song_index]['Quadrant']\n",
        "\n",
        "print(f\"Audio ID: {test_audio_id}\")\n",
        "print(f\"Lyric ID: {test_lyric_id}\")\n",
        "print(f\"Quadrant: {test_quadrant}\")\n",
        "\n",
        "#variables for the spectorgram and tokenised lyrics loaded.\n",
        "spectrogram, encoded_lyrics = load_song_data(test_audio_id, test_lyric_id, test_quadrant)\n",
        "\n",
        "\n",
        "#check the output of the lyrics\n",
        "if encoded_lyrics is not None:\n",
        "    print(\"\\n--- Tokenized Lyrics Output ---\")\n",
        "\n",
        "    #dictionary of tensors\n",
        "    print(\"Encoded Tensors:\")\n",
        "    display(encoded_lyrics)\n",
        "\n",
        "    #readable tokens to check\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoded_lyrics['input_ids'][0])\n",
        "\n",
        "    print(\"\\nSample of Tokens Produced:\")\n",
        "    print(tokens[:50])\n",
        "\n",
        "#check the output of the audio\n",
        "if spectrogram is not None:\n",
        "    print(\"\\n--- Spectrogram Data ---\")\n",
        "    print(f\"Shape: {spectrogram.shape}\")\n",
        "\n",
        "    #show spectogram\n",
        "    print(\"\\nDisplaying Spectrogram:\")\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    #get the sample rate for the axis display\n",
        "    audio_file_path = os.path.join('/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete/audio', test_quadrant, f'{test_audio_id}.mp3')\n",
        "    sr = librosa.get_samplerate(path=audio_file_path)\n",
        "\n",
        "    librosa.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='mel')\n",
        "\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(f'Mel Spectrogram for {test_audio_id}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jjoQwck_G-qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#define where to save the files\n",
        "base_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
        "spectrogram_save_dir = os.path.join(base_path, 'processed_spectrograms/')\n",
        "lyrics_save_dir = os.path.join(base_path, 'processed_lyrics/')\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(spectrogram_save_dir, exist_ok=True)\n",
        "os.makedirs(lyrics_save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Spectrograms will be saved in: {spectrogram_save_dir}\")\n",
        "print(f\"Lyric tensors will be saved in: {lyrics_save_dir}\")\n",
        "\n",
        "#process the audio and lyrics storing info as processed\n",
        "processed_records = []\n",
        "\n",
        "#iterate over all rows and get origress bar\n",
        "for index, row in tqdm(final_df.iterrows(), total=final_df.shape[0], desc=\"Processing Songs\"):\n",
        "\n",
        "    #get song info from each row\n",
        "    song_id = row['Audio_Song']\n",
        "    lyric_id = row['Lyric_Song_x']\n",
        "    quadrant = row['Quadrant']\n",
        "\n",
        "    #call function to transform\n",
        "    spectrogram, encoded_lyrics = load_song_data(song_id, lyric_id, quadrant)\n",
        "\n",
        "    #check if it worked\n",
        "    if spectrogram is not None and encoded_lyrics is not None:\n",
        "        try:\n",
        "            #save the spectorgram\n",
        "            spectrogram_path = os.path.join(spectrogram_save_dir, f\"{song_id}.npy\")\n",
        "            np.save(spectrogram_path, spectrogram)\n",
        "\n",
        "            #save tokenised lyric tensors\n",
        "            lyrics_path = os.path.join(lyrics_save_dir, f\"{song_id}.pt\")\n",
        "            torch.save(encoded_lyrics, lyrics_path)\n",
        "\n",
        "            #record file path\n",
        "            processed_records.append({\n",
        "                'song_id': song_id,\n",
        "                'spectrogram_path': spectrogram_path,\n",
        "                'lyrics_path': lyrics_path,\n",
        "                'valence': row['Valence'],\n",
        "                'arousal': row['Arousal'],\n",
        "                'quadrant': quadrant\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving files for {song_id}: {e}\")\n",
        "\n",
        "\n",
        "#save new dataframe for using in experiments\n",
        "processed_df = pd.DataFrame(processed_records)\n",
        "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
        "processed_df.to_csv(master_file_path, index=False)\n",
        "\n",
        "print(\"\\n PROCESSING COMPLETE!\")\n",
        "print(f\"Saved {len(processed_df)} records.\")\n",
        "print(f\"Master file list saved to: {master_file_path}\")\n",
        "display(processed_df.head())"
      ],
      "metadata": {
        "id": "6lPRDJvGl4Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#path to where this was saved\n",
        "base_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
        "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
        "\n",
        "#load the master csv with all the paths and VA values\n",
        "master_df = pd.read_csv(master_file_path)\n",
        "\n",
        "print(\"Master csv loaded\")\n",
        "display(master_df.head())\n",
        "\n",
        "#check data for 1 song\n",
        "#pick a song between 0 and 2215\n",
        "test_final_song_index = 1111\n",
        "song_info = master_df.iloc[test_final_song_index]\n",
        "\n",
        "print(f\"\\n--- Loading data for song: {song_info['song_id']} ---\")\n",
        "\n",
        "#Load the spectrogram from the file\n",
        "spectrogram = np.load(song_info['spectrogram_path'])\n",
        "\n",
        "#load the lyric tensors\n",
        "encoded_lyrics = torch.load(song_info['lyrics_path'], weights_only=False)\n",
        "\n",
        "#Get the labels\n",
        "valence = song_info['valence']\n",
        "arousal = song_info['arousal']\n",
        "\n",
        "#check the data\n",
        "print(\"Spectrogram Shape:\", spectrogram.shape)\n",
        "print(\"Encoded Lyrics Tensors:\", encoded_lyrics)\n",
        "print(f\"Labels - Valence: {valence}, Arousal: {arousal}\")\n"
      ],
      "metadata": {
        "id": "Li06X-Gly-KP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}