{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN4j2otu6EiI6PXXeqXdp7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Dissertation_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB327CIPG0v4"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# install mamba to use instead of pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the config file and build the environment.\n",
        "yaml_content = \"\"\"\n",
        "name: dissertation\n",
        "channels:\n",
        "  - pytorch\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - pytorch=2.2.2\n",
        "  - torchvision=0.17.2\n",
        "  - torchaudio\n",
        "  - librosa\n",
        "  - numpy<2\n",
        "  - pandas\n",
        "  - jupyter\n",
        "  - wandb\n",
        "\"\"\"\n",
        "\n",
        "# Write the string content to a file -  'environment.yml'.\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"environment.yml file created successfully.\")\n",
        "\n",
        "# create the environment using mamba from the yml file.\n",
        "print(\"\\n Creating environment\")\n",
        "\n",
        "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
      ],
      "metadata": {
        "id": "NqkjmK3RHcYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and setting up of GitHub and W&B\n",
        "\n",
        "# clone project repository from GitHub\n",
        "print(\"⏳ Cloning GitHub repository...\")\n",
        "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
        "print(\"Repository cloned.\")\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
        "print(\"Tokenizer loaded.\")"
      ],
      "metadata": {
        "id": "tG0a7AkQHf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data from the saved file\n",
        "#path to where this was saved\n",
        "base_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
        "master_file_path = os.path.join(base_path, 'master_processed_file_list.csv')\n",
        "\n",
        "#load the master csv with all the paths and VA values\n",
        "master_df = pd.read_csv(master_file_path)\n",
        "\n",
        "print(\"Master csv loaded\")\n",
        "display(master_df.head())\n",
        "\n",
        "#check data for 1 song\n",
        "#pick a song between 0 and 2215\n",
        "test_final_song_index = 9\n",
        "song_info = master_df.iloc[test_final_song_index]\n",
        "\n",
        "print(f\"\\n--- Loading data for song: {song_info['song_id']} ---\")\n",
        "\n",
        "#Load the spectrogram from the file\n",
        "spectrogram = np.load(song_info['spectrogram_path'])\n",
        "\n",
        "#load the lyric tensors\n",
        "encoded_lyrics = torch.load(song_info['lyrics_path'], weights_only=False)\n",
        "\n",
        "#Get the labels\n",
        "valence = song_info['valence']\n",
        "arousal = song_info['arousal']\n",
        "\n",
        "#check the data\n",
        "print(\"Spectrogram Shape:\", spectrogram.shape)\n",
        "print(\"Encoded Lyrics Tensors:\", encoded_lyrics)\n",
        "print(f\"Labels - Valence: {valence}, Arousal: {arousal}\")\n"
      ],
      "metadata": {
        "id": "KUbPRcEUHrTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MER_Dataset(Dataset):\n",
        "    \"\"\" Custom PyTorch Dataset for loading MER data. \"\"\"\n",
        "    def __init__(self, annotations_df, tokenizer):\n",
        "        \"\"\" Creation of the Dataset from the dataframe (predefined splits in MERGE dataset) \"\"\"\n",
        "        self.annotations = annotations_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Function to return the total number of songs in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Function to get a song from the dataset.\n",
        "        \"\"\"\n",
        "        song_info = self.annotations.iloc[index] #which song ID/row is picked from the dataset as per the index\n",
        "\n",
        "        spectrogram_path = song_info['spectrogram_path'] # columns from the df\n",
        "        lyrics_path = song_info['lyrics_path'] # columns from the df\n",
        "        valence = song_info['valence'] # columns from the df\n",
        "        arousal = song_info['arousal'] # columns from the df\n",
        "\n",
        "        #change spectorgram into a tensor\n",
        "        spectrogram = np.load(spectrogram_path) #loading spectorgram from path saved in df\n",
        "        spectrogram_tensor = torch.from_numpy(spectrogram).float() # changing the np array to tensor\n",
        "        spectrogram_tensor = spectrogram_tensor.unsqueeze(0) #Adding a \"channel\" dimension for CNN\n",
        "\n",
        "        #Load the lyric tokens\n",
        "        encoded_lyrics = torch.load(lyrics_path, weights_only=False)\n",
        "        input_ids = encoded_lyrics['input_ids'].squeeze(0) #remove the batch dimension from input ids so 1d\n",
        "        attention_mask = encoded_lyrics['attention_mask'].squeeze(0) #remove the batch dimension from attention mask so 1d\n",
        "\n",
        "\n",
        "        labels = torch.tensor([valence, arousal], dtype=torch.float32) # extract labels\n",
        "\n",
        "\n",
        "        return spectrogram_tensor, input_ids, attention_mask, labels"
      ],
      "metadata": {
        "id": "RWoc_3CmRvx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the csv for where the predefined splits are located in Google drive\n",
        "split_folder_path = '/content/drive/MyDrive/dissertation/MERGE_Bimodal_Complete/tvt_dataframes/tvt_70_15_15/'\n",
        "\n",
        "#read the files and load into variables\n",
        "train_split_df = pd.read_csv(os.path.join(split_folder_path, 'tvt_70_15_15_train_bimodal_complete.csv'))\n",
        "val_split_df = pd.read_csv(os.path.join(split_folder_path, 'tvt_70_15_15_validate_bimodal_complete.csv'))\n",
        "test_split_df = pd.read_csv(os.path.join(split_folder_path, 'tvt_70_15_15_test_bimodal_complete.csv'))\n",
        "\n",
        "id_column_name = 'song_id' #match the naming in the master data and replace in test/train/split\n",
        "train_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "val_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "test_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "\n",
        "#filter the master dataset to create the three smaller datasets\n",
        "train_df = pd.merge(master_df, train_split_df, on=id_column_name)\n",
        "val_df = pd.merge(master_df, val_split_df, on=id_column_name)\n",
        "test_df = pd.merge(master_df, test_split_df, on=id_column_name)\n",
        "\n",
        "print(f\"Total training samples: {len(train_df)}\") # check these against the csv train should have 1552 songs\n",
        "print(f\"Total validation samples: {len(val_df)}\") # check these against the csv train should have 332 songs\n",
        "print(f\"Total test samples: {len(test_df)}\") # check these against the csv train should have 332 songs\n",
        "\n",
        "# Create separate dataloaders and datasets.\n",
        "train_dataset = MER_Dataset(annotations_df=train_df, tokenizer=tokenizer)\n",
        "val_dataset = MER_Dataset(annotations_df=val_df, tokenizer=tokenizer)\n",
        "test_dataset = MER_Dataset(annotations_df=test_df, tokenizer=tokenizer)\n",
        "\n",
        "BATCH_SIZE = 16 #using for now, but can change this in the future\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) #shuffle training dataset but not others.\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "#verify a batch\n",
        "spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch = next(iter(train_loader))\n",
        "\n",
        "print(\"\\Verifying a batch from the new train_loader ---\")\n",
        "print(f\"Spectrogram batch shape: {spectrogram_batch.shape}\")\n",
        "print(f\"Input IDs batch shape: {input_ids_batch.shape}\")\n",
        "print(f\"Labels batch shape: {labels_batch.shape}\")"
      ],
      "metadata": {
        "id": "4F5-XOUmY2VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MERGE_Audio_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    The audio model, following the architecture and parameters\n",
        "    from the MERGE paper (Lima Louro).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MERGE_Audio_Model, self).__init__()\n",
        "\n",
        "        #Feature extractor - Frontend\n",
        "        # Block 1\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(3, 2), stride=2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Block 4\n",
        "        self.conv_block4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(3, 2), stride=2)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x): #path for the audio tower\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.conv_block4(x)\n",
        "        x = self.flatten(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pM1B4zwQcdnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BimodalClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The final bimodal model. No longer using MERGE archtecture as\n",
        "    transformer would be better. Also due to mistakes in the paper it is\n",
        "    unclear what some of the parameters are.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BimodalClassifier, self).__init__()\n",
        "\n",
        "        #initiate audio tower\n",
        "        self.audio_tower = MERGE_Audio_Model()\n",
        "\n",
        "        #use transformer for lyrics\n",
        "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        #Audio features: 71680 + Lyric features: 768 = 72448\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=72448, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=100, out_features=2) # Output for Valence and Arousal\n",
        "        )\n",
        "\n",
        "    def forward(self, x_audio, input_ids, attention_mask):\n",
        "        #process audio input\n",
        "        audio_features = self.audio_tower(x_audio)\n",
        "\n",
        "        #get lyric features\n",
        "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        #use the embedding of the [CLS] token as the feature vector for whole lyrics\n",
        "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        #combine the features from both towers\n",
        "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
        "\n",
        "        #pass the combined features to the final classifier head\n",
        "        output = self.classifier_head(combined_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "r1llg53WvWDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a CUDA-enabled GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Using CUDA device.\")\n",
        "else:\n",
        "    # If no GPU is found, print an error and stop execution by raising an error.\n",
        "    raise RuntimeError(\"Error: No GPU found. This script requires a GPU to run.\")\n"
      ],
      "metadata": {
        "id": "sKtbxvlbtVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final output model\n",
        "model = BimodalClassifier()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss() # Using Mean Squared Error for regression\n",
        "\n",
        "#training\n",
        "wandb.init(project=\"dissertation-mer-regression\")\n",
        "\n",
        "NUM_EPOCHS = 1 #initial testing only\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    #training\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        spectrogram_batch = spectrogram_batch.to(device)\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(spectrogram_batch, input_ids_batch, attention_mask_batch)\n",
        "        loss = loss_fn(outputs, labels_batch)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "    wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss})\n",
        "\n",
        "    #vaidate\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
        "            spectrogram_batch = spectrogram_batch.to(device)\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_mask_batch = attention_mask_batch.to(device)\n",
        "            labels_batch = labels_batch.to(device)\n",
        "\n",
        "            outputs = model(spectrogram_batch, input_ids_batch, attention_mask_batch)\n",
        "            loss = loss_fn(outputs, labels_batch)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    wandb.log({\"val_loss\": avg_val_loss})\n",
        "\n",
        "print(\"--- Pipeline Test Complete ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x2r9lbd-SH7j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}