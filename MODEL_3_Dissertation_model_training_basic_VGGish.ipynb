{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyNlE1Fle6736SEoZ8Coi7RQ",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/MODEL_3_Dissertation_model_training_basic_VGGish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB327CIPG0v4"
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "# install mamba to use instead of pip"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the config file and build the environment.\n",
    "yaml_content = \"\"\"\n",
    "name: dissertation\n",
    "channels:\n",
    "  - pytorch\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pytorch=2.2.2\n",
    "  - torchvision=0.17.2\n",
    "  - torchaudio\n",
    "  - librosa\n",
    "  - numpy<2\n",
    "  - pandas\n",
    "  - jupyter\n",
    "  - wandb\n",
    "\"\"\"\n",
    "\n",
    "# Write the string content to a file -  'environment.yml'.\n",
    "with open('environment.yml', 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"environment.yml file created successfully.\")\n",
    "\n",
    "# create the environment using mamba from the yml file.\n",
    "print(\"\\n Creating environment\")\n",
    "\n",
    "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
   ],
   "metadata": {
    "id": "NqkjmK3RHcYW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# imports and setting up of GitHub and W&B\n",
    "\n",
    "# clone project repository from GitHub\n",
    "print(\"⏳ Cloning GitHub repository...\")\n",
    "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
    "print(\"Repository cloned.\")\n",
    "\n",
    "#Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import subprocess\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
    "print(\"Tokenizer loaded.\")"
   ],
   "metadata": {
    "id": "tG0a7AkQHf2F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MER_Dataset(Dataset):\n",
    "    \"\"\" Custom PyTorch Dataset for loading MER data. \"\"\"\n",
    "    def __init__(self, annotations_df, tokenizer):\n",
    "        \"\"\" Creation of the Dataset from the dataframe (predefined splits in MERGE dataset) \"\"\"\n",
    "        self.annotations = annotations_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function to return the total number of songs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Function to get a song from the dataset.\n",
    "        \"\"\"\n",
    "        song_info = self.annotations.iloc[index] #which song ID/row is picked from the dataset as per the index\n",
    "\n",
    "        spectrogram_path = song_info['spectrogram_path'] # columns from the df\n",
    "        lyrics_path = song_info['lyrics_path'] # columns from the df\n",
    "        valence = song_info['valence'] # columns from the df\n",
    "        arousal = song_info['arousal'] # columns from the df\n",
    "\n",
    "        #change spectorgram into a tensor\n",
    "        spectrogram = np.load(spectrogram_path) #loading spectorgram from path saved in df\n",
    "        spectrogram_tensor = torch.from_numpy(spectrogram).float() # changing the np array to tensor\n",
    "        spectrogram_tensor = spectrogram_tensor.unsqueeze(0) #Adding a \"channel\" dimension for CNN\n",
    "\n",
    "        #Load the lyric tokens\n",
    "        encoded_lyrics = torch.load(lyrics_path, weights_only=False)\n",
    "        input_ids = encoded_lyrics['input_ids'].squeeze(0) #remove the batch dimension from input ids so 1d array\n",
    "        attention_mask = encoded_lyrics['attention_mask'].squeeze(0) #remove the batch dimension from attention mask so 1d\n",
    "\n",
    "        labels = torch.tensor([valence, arousal], dtype=torch.float32) # extract labels\n",
    "\n",
    "        return spectrogram_tensor, input_ids, attention_mask, labels"
   ],
   "metadata": {
    "id": "vmJVKE2BAbTZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AttentionModule(nn.Module): #Addition from V1\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        '''\n",
    "        Attention mechanism to weight the importance of different features\n",
    "        '''\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 4),  # input is 64 will map to16\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim // 4, feature_dim),  #reverts back to 64 from 16\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 64]\n",
    "        attention_weights = self.attention(x)  # [batch_size, 64]\n",
    "        weighted_features = x * attention_weights  # Element-wise multiplication\n",
    "        return weighted_features"
   ],
   "metadata": {
    "id": "uQy6xyLPJxAc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class VGGish_Audio_Model(nn.Module):\n    '''As previous VQ but adding in the following\n      - Batch normalisation\n      - Attention mechanism\n      - Learning rate scheduling\n      - early stopping'''\n\n    def __init__(self):\n        super(VGGish_Audio_Model, self).__init__()\n        '''\n        A VGG-style model for the audio tower for a starting model.\n        No longer trying to implement the method from MERGE paper as this had mistakes in the paper\n        V1.1 includes attention to see if this improves performance.\n        '''\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), #Addition from V1\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), #Addition from V1\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), #Addition from V1\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512), #Addition from V1\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(256, 64) # Final feature vector size should be 64 - needs to match input of combined\n        )\n\n        self.attention = AttentionModule(64) #Addition from V1.1 CALLED THIS HOWEVER DIDN'T ON MODEL 2 ONLY MODEL 3\n\n    def forward(self, x):\n        x = self.features(x)\n        #flatten the features for the classifier\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        x = self.attention(x) #Addition from V1\n        return x\n",
   "metadata": {
    "id": "pM1B4zwQcdnb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BimodalClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The final bimodal model. No longer using MERGE archtecture as\n",
    "    transformer would be better. Also due to mistakes in the paper it is\n",
    "    unclear what some of the parameters are.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BimodalClassifier, self).__init__()\n",
    "\n",
    "        #initiate audio tower\n",
    "        self.audio_tower = VGGish_Audio_Model()\n",
    "\n",
    "        #use transformer for lyrics (using bert base uncased for now, but may change)\n",
    "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.lyrics_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define feature sizes from the previous step and from bert\n",
    "        AUDIO_FEATURES_OUT = 64\n",
    "        LYRICS_FEATURES_OUT = 768\n",
    "        COMBINED_FEATURES = AUDIO_FEATURES_OUT + LYRICS_FEATURES_OUT\n",
    "\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=COMBINED_FEATURES, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=100, out_features=2) # 2 Outputs for Valence and Arousal\n",
    "        )\n",
    "\n",
    "    def forward(self, x_audio, input_ids, attention_mask):\n",
    "        #process audio input\n",
    "        audio_features = self.audio_tower(x_audio)\n",
    "\n",
    "        #get lyric features\n",
    "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #use the embedding of the [CLS] token as the feature vector for whole lyrics\n",
    "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        #combine the features from both towers\n",
    "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
    "\n",
    "        #pass the combined features to the final classifier head\n",
    "        output = self.classifier_head(combined_features)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "id": "r1llg53WvWDy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Data loading and prep\n",
    "\n",
    "#get the paths to dissertation folder and new folder on colab\n",
    "print(\"Starting data transfer from Google Drive to local Colab storage...\")\n",
    "\n",
    "#get paths for old file location and new colab one\n",
    "gdrive_zip_path = '/content/drive/MyDrive/dissertation/merge_dataset_zipped.zip'\n",
    "local_storage_path = '/content/local_dissertation_data/'\n",
    "local_zip_path = os.path.join(local_storage_path, 'merge_dataset_zipped.zip')\n",
    "os.makedirs(local_storage_path, exist_ok=True) # Ensure the destination directory exists\n",
    "\n",
    "#Copy zip file from Drive to Colab\n",
    "print(\"Copying single archive file from Google Drive...\")\n",
    "!rsync -ah --progress \"{gdrive_zip_path}\" \"{local_storage_path}\"\n",
    "\n",
    "#get total number of files for progress\n",
    "total_files = int(subprocess.check_output(f\"zipinfo -1 {local_zip_path} | wc -l\", shell=True))\n",
    "\n",
    "#unzip the file\n",
    "print(\"Extracting files locally\")\n",
    "!unzip -o \"{local_zip_path}\" -d \"{local_storage_path}\" | tqdm --unit=files --total={total_files} > /dev/null\n",
    "\n",
    "print(\"Data transfer and extraction complete.\")\n",
    "\n",
    "#load master data from new location\n",
    "local_output_path = os.path.join(local_storage_path, 'merge_dataset/output_from_code/')\n",
    "master_file_path = os.path.join(local_output_path, 'master_processed_file_list.csv')\n",
    "master_df = pd.read_csv(master_file_path)\n",
    "\n",
    "#checking the valence and arousal range in the dataset\n",
    "print(f\"\\nValence range in data: [{master_df['valence'].min()}, {master_df['valence'].max()}]\")\n",
    "print(f\"Arousal range in data: [{master_df['arousal'].min()}, {master_df['arousal'].max()}]\")\n",
    "print(f\"Valence mean: {master_df['valence'].mean():.4f}, std: {master_df['valence'].std():.4f}\")\n",
    "print(f\"Arousal mean: {master_df['arousal'].mean():.4f}, std: {master_df['arousal'].std():.4f}\")\n",
    "print(f\"Total samples in master_df: {len(master_df)}\")\n",
    "\n",
    "# Verify its the right column - not quadrants\n",
    "print(f\"\\nNumber of unique valence values: {master_df['valence'].nunique()}\")\n",
    "print(f\"Number of unique arousal values: {master_df['arousal'].nunique()}\")\n",
    "print(f\"Number of unique quadrant values: {master_df['quadrant'].nunique()}\")\n",
    "\n",
    "# Sample some actual values\n",
    "print(f\"\\nSample valence values: {master_df['valence'].sample(10).values}\")\n",
    "print(f\"Sample arousal values: {master_df['arousal'].sample(10).values}\")\n",
    "\n",
    "#update the paths in the csv\n",
    "print(\"\\nUpdating dataframe paths to use fast local storage...\")\n",
    "gdrive_output_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
    "master_df['spectrogram_path'] = master_df['spectrogram_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
    "master_df['lyrics_path'] = master_df['lyrics_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
    "print(\"Dataframe paths updated.\")\n",
    "\n",
    "#load the data splits from the new path in the predefined splits folder tvt\n",
    "local_split_folder_path = os.path.join(local_storage_path, 'merge_dataset/MERGE_Bimodal_Complete/tvt_dataframes/tvt_70_15_15/')\n",
    "train_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_train_bimodal_complete.csv'))\n",
    "val_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_validate_bimodal_complete.csv'))\n",
    "test_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_test_bimodal_complete.csv'))\n",
    "print(\"\\nSplit files loaded from local storage.\")\n",
    "\n",
    "#merge the files\n",
    "id_column_name = 'song_id'\n",
    "train_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "val_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "test_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "\n",
    "train_df = pd.merge(master_df, train_split_df, on=id_column_name)\n",
    "val_df = pd.merge(master_df, val_split_df, on=id_column_name)\n",
    "test_df = pd.merge(master_df, test_split_df, on=id_column_name)\n",
    "\n",
    "#checking no files are lost in merging - and checking length of the dataframes.\n",
    "print(\"\\nchecking data\")\n",
    "\n",
    "#check no data lost in merge\n",
    "if len(train_df) == len(train_split_df):\n",
    "    print(\"\\nTraining split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Training split lost {len(train_split_df) - len(train_df)} songs during merge.\")\n",
    "\n",
    "if len(val_df) == len(val_split_df):\n",
    "    print(\"Validation split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"WARNING: Validation split lost {len(val_split_df) - len(val_df)} songs during merge.\")\n",
    "\n",
    "if len(test_df) == len(test_split_df):\n",
    "    print(\"Test split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"WARNING: Test split lost {len(test_split_df) - len(test_df)} songs during merge.\")\n",
    "\n",
    "#check length\n",
    "expected_train_len = 1552\n",
    "expected_val_len = 332\n",
    "expected_test_len = 332\n",
    "\n",
    "assert len(train_df) == expected_train_len, f\"Expected {expected_train_len} training samples, but found {len(train_df)}\"\n",
    "assert len(val_df) == expected_val_len, f\"Expected {expected_val_len} validation samples, but found {len(val_df)}\"\n",
    "assert len(test_df) == expected_test_len, f\"Expected {expected_test_len} test samples, but found {len(test_df)}\"\n",
    "\n",
    "print(f\"\\nFinal dataset lengths are correct: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
    "print(\"Data Check Complete\")\n",
    "\n",
    "#createthe datasets and loaders\n",
    "train_dataset = MER_Dataset(annotations_df=train_df, tokenizer=tokenizer)\n",
    "val_dataset = MER_Dataset(annotations_df=val_df, tokenizer=tokenizer)\n",
    "test_dataset = MER_Dataset(annotations_df=test_df, tokenizer=tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")"
   ],
   "metadata": {
    "id": "KUbPRcEUHrTM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA device.\")\n",
    "else:\n",
    "    # If no GPU is found, print an error and stop execution by raising an error.\n",
    "    raise RuntimeError(\"Error: No GPU found. This script requires a GPU to run.\")\n"
   ],
   "metadata": {
    "id": "sKtbxvlbtVsv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Final output model\n",
    "model = BimodalClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss() # Using Mean Squared Error for regression\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5) #Addition from V1\n",
    "\n",
    "#training\n",
    "wandb.init(project=\"dissertation-mer-regression\")\n",
    "\n",
    "# Early Stopping Setup - Addition from V1\n",
    "best_val_loss = float('inf') #Addition from V1\n",
    "patience = 10  # Stop if no improvement for 10 epochs Addition from V1\n",
    "patience_counter = 0 #Addition from V1\n",
    "best_model_state = None #Addition from V1\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    #training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        spectrogram_batch = spectrogram_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrogram_batch, input_ids_batch, attention_mask_batch)\n",
    "        loss = loss_fn(outputs, labels_batch)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss})\n",
    "\n",
    "    #vaidate\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            spectrogram_batch = spectrogram_batch.to(device)\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            outputs = model(spectrogram_batch, input_ids_batch, attention_mask_batch)\n",
    "            loss = loss_fn(outputs, labels_batch)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    wandb.log({\"val_loss\": avg_val_loss})\n",
    "\n",
    "    # Early Stopping Logic - Addition from V1\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model state\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"✓ New best validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epochs (patience: {patience})\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered! Best validation loss: {best_val_loss:.4f}\")\n",
    "            # Restore the best model\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "    scheduler.step() # Addition from V1\n",
    "    current_lr = scheduler.get_last_lr()[0] #Addition from V1\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\") #Addition from V1\n",
    "    wandb.log({\"learning_rate\": current_lr}) #Addition from V1\n",
    "\n",
    "if patience_counter >= patience:  #Addition from V1\n",
    "    print(\"--- Training Stopped Early ---\") #Addition from V1\n",
    "    print(f\"Best model restored from epoch {epoch + 1 - patience}\") #Addition from V1\n",
    "else:\n",
    "    print(\"--- Training Completed All Epochs ---\") #Addition from V1\n",
    "\n",
    "print(f\"Final best validation loss: {best_val_loss:.4f}\") #Addition from V1\n",
    "print(\"--- Pipeline Test Complete ---\") #Addition from V1\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "x2r9lbd-SH7j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test set evaluation\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "\n",
    "with torch.no_grad(): # No need to track gradients for evaluation\n",
    "    for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        spectrogram_batch = spectrogram_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(spectrogram_batch, input_ids_batch, attention_mask_batch)\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels_batch.cpu().numpy())\n",
    "\n",
    "# Combine predictions and labels from all batches\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Calculate metrics using scikit-learn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Separate Valence and Arousal for individual analysis of songs to inspect\n",
    "valence_true, arousal_true = all_labels[:, 0], all_labels[:, 1]\n",
    "valence_pred, arousal_pred = all_predictions[:, 0], all_predictions[:, 1]\n",
    "\n",
    "# final results\n",
    "print(\"\\n--- Test Set Evaluation Results ---\")\n",
    "\n",
    "# Valence Metrics\n",
    "mse_v = mean_squared_error(valence_true, valence_pred)\n",
    "mae_v = mean_absolute_error(valence_true, valence_pred)\n",
    "r2_v = r2_score(valence_true, valence_pred)\n",
    "print(f\"Valence  -> MSE: {mse_v:.4f}, MAE: {mae_v:.4f}, R-squared: {r2_v:.4f}\")\n",
    "\n",
    "# Arousal Metrics\n",
    "mse_a = mean_squared_error(arousal_true, arousal_pred)\n",
    "mae_a = mean_absolute_error(arousal_true, arousal_pred)\n",
    "r2_a = r2_score(arousal_true, arousal_pred)\n",
    "print(f\"Arousal  -> MSE: {mse_a:.4f}, MAE: {mae_a:.4f}, R-squared: {r2_a:.4f}\")\n",
    "\n",
    "# Log final metrics to wandb\n",
    "wandb.log({\n",
    "    \"test_mse_valence\": mse_v, \"test_mae_valence\": mae_v, \"test_r2_valence\": r2_v,\n",
    "    \"test_mse_arousal\": mse_a, \"test_mae_arousal\": mae_a, \"test_r2_arousal\": r2_a\n",
    "})\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "\n"
   ],
   "metadata": {
    "id": "MDVgjdR9R0IV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# baseline mode 1 - always predicts the mean\n",
    "train_valence_mean = train_df['valence'].mean()\n",
    "train_arousal_mean = train_df['arousal'].mean()\n",
    "\n",
    "# Predict the same mean for all test\n",
    "mean_baseline_val = np.full(len(valence_true), train_valence_mean)\n",
    "mean_baseline_ar = np.full(len(arousal_true), train_arousal_mean)\n",
    "\n",
    "mean_mae_v = mean_absolute_error(valence_true, mean_baseline_val)\n",
    "mean_mae_a = mean_absolute_error(arousal_true, mean_baseline_ar)\n",
    "\n",
    "print(f\"Mean Baseline MAE - Valence: {mean_mae_v:.4f}, Arousal: {mean_mae_a:.4f}\")\n",
    "print(f\"VGGish Model MAE - Valence: {mae_v:.4f}, Arousal: {mae_a:.4f}\")\n",
    "\n",
    "# baseine model 2 predicts center of each quadrant\n",
    "quadrant_centers = {} #calculate quadrant centers from training data\n",
    "for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "    q_data = train_df[train_df['quadrant'] == q]\n",
    "    quadrant_centers[q] = {\n",
    "        'valence': q_data['valence'].mean(),\n",
    "        'arousal': q_data['arousal'].mean()\n",
    "    }\n",
    "\n",
    "# For test set, predict based on quadrant\n",
    "quadrant_baseline_val = []\n",
    "quadrant_baseline_ar = []\n",
    "for q in test_df['quadrant']:\n",
    "    quadrant_baseline_val.append(quadrant_centers[q]['valence'])\n",
    "    quadrant_baseline_ar.append(quadrant_centers[q]['arousal'])\n",
    "\n",
    "quad_mae_v = mean_absolute_error(valence_true, quadrant_baseline_val)\n",
    "quad_mae_a = mean_absolute_error(arousal_true, quadrant_baseline_ar)\n",
    "\n",
    "print(f\"Quadrant-Center Baseline MAE - Valence: {quad_mae_v:.4f}, Arousal: {quad_mae_a:.4f}\")\n",
    "\n",
    "# baseline model 3 - random valid predictions avoiding the hole\n",
    "np.random.seed(42)\n",
    "random_val = []\n",
    "random_ar = []\n",
    "for _ in range(len(valence_true)):\n",
    "    #generate random values avoiding the center hole\n",
    "    v = np.random.uniform(0, 1)\n",
    "    a = np.random.uniform(0, 1)\n",
    "    #if in hole redo\n",
    "    while (0.4 <= v <= 0.6) and (0.4 <= a <= 0.6):\n",
    "        v = np.random.uniform(0, 1)\n",
    "        a = np.random.uniform(0, 1)\n",
    "    random_val.append(v)\n",
    "    random_ar.append(a)\n",
    "\n",
    "random_mae_v = mean_absolute_error(valence_true, random_val)\n",
    "random_mae_a = mean_absolute_error(arousal_true, random_ar)\n",
    "\n",
    "print(f\"Random Baseline MAE - Valence: {random_mae_v:.4f}, Arousal: {random_mae_a:.4f}\")"
   ],
   "metadata": {
    "id": "XGdBwR6oJdYO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Diagnostic code based on the very low MAE scores using the MERGE dataset.\n",
    "# Identifying the data - Ambiguous songs are removed, so should see a doughnut shape on the scatter plot and missing sections on histograms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "#Histogram of valence - showing missing parts in the middle (0.4 to 0.6 from mappings)\n",
    "axes[0].hist(master_df['valence'], bins=50, edgecolor='black')\n",
    "axes[0].axvline(x=0.4, color='r', linestyle='--', label='Expected gap start')\n",
    "axes[0].axvline(x=0.6, color='r', linestyle='--', label='Expected gap end')\n",
    "axes[0].set_xlabel('Valence')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Valence Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "#Histogram of arousal - showing missing parts in the middle (0.4 to 0.6 from mappings)\n",
    "axes[1].hist(master_df['arousal'], bins=50, edgecolor='black')\n",
    "axes[1].axvline(x=0.4, color='r', linestyle='--', label='Expected gap start')\n",
    "axes[1].axvline(x=0.6, color='r', linestyle='--', label='Expected gap end')\n",
    "axes[1].set_xlabel('Arousal')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Arousal Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "#2D scatter - see hole in the middle. This dataset is no good for VA\n",
    "quadrant_map = {'Q1': 1, 'Q2': 2, 'Q3': 3, 'Q4': 4}\n",
    "quadrant_numeric = master_df['quadrant'].map(quadrant_map)\n",
    "\n",
    "scatter = axes[2].scatter(master_df['valence'], master_df['arousal'],\n",
    "                         c=quadrant_numeric, cmap='viridis', alpha=0.5)\n",
    "axes[2].add_patch(plt.Rectangle((0.4, 0.4), 0.2, 0.2,\n",
    "                               fill=False, edgecolor='red', linewidth=2,\n",
    "                               label='Expected removed region'))\n",
    "axes[2].set_xlabel('Valence')\n",
    "axes[2].set_ylabel('Arousal')\n",
    "axes[2].set_title('Distribution in AV Space')\n",
    "axes[2].legend()\n",
    "plt.colorbar(scatter, ax=axes[2], label='Quadrant')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Checking how many points fall in the \"Expected to be removed\" section\n",
    "in_center = ((master_df['valence'] >= 0.4) & (master_df['valence'] <= 0.6) &\n",
    "             (master_df['arousal'] >= 0.4) & (master_df['arousal'] <= 0.6))\n",
    "print(f\"\\nPoints in center region [0.4-0.6]: {in_center.sum()} ({in_center.mean()*100:.1f}%)\")\n",
    "\n",
    "# Also checking just outside this area to see if any other points are affected\n",
    "in_broader_center = ((master_df['valence'] >= 0.3) & (master_df['valence'] <= 0.7) &\n",
    "                     (master_df['arousal'] >= 0.3) & (master_df['arousal'] <= 0.7))\n",
    "print(f\"Points in broader center [0.3-0.7]: {in_broader_center.sum()} ({in_broader_center.mean()*100:.1f}%)\")"
   ],
   "metadata": {
    "id": "Rd5zGbLS7lCn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test on the mean and quadratn predictions\n",
    "\n",
    "#Mean baseline\n",
    "train_val_mean = train_df['valence'].mean()\n",
    "train_ar_mean = train_df['arousal'].mean()\n",
    "print(f\"Training set means: V={train_val_mean:.3f}, A={train_ar_mean:.3f}\")\n",
    "\n",
    "#Quick quadrant check\n",
    "for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "    q_data = train_df[train_df['quadrant'] == q]\n",
    "    print(f\"{q}: V={q_data['valence'].mean():.3f}±{q_data['valence'].std():.3f}, \"\n",
    "          f\"A={q_data['arousal'].mean():.3f}±{q_data['arousal'].std():.3f}\")\n",
    "\n",
    "#How spread out are values within each quadrant?\n",
    "print(\"\\nWithin-quadrant variance:\")\n",
    "for q in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "    q_data = test_df[test_df['quadrant'] == q]\n",
    "    print(f\"{q}: V_std={q_data['valence'].std():.3f}, A_std={q_data['arousal'].std():.3f}\")"
   ],
   "metadata": {
    "id": "eOzPllUIKdAb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Saving to CSV - copying the original CSV and adding in the results to inspect manually\n",
    "\n",
    "print(\"\\nSaving Detailed song by song CSV\")\n",
    "\n",
    "# new DF with the song ID arousal and valence\n",
    "results_df = test_df[['song_id', 'valence', 'arousal']].copy()\n",
    "results_df['valence_predicted'] = valence_pred\n",
    "results_df['arousal_predicted'] = arousal_pred\n",
    "\n",
    "# path for saving new CSV in drive\n",
    "gdrive_output_path = '/content/drive/MyDrive/dissertation/merge_dataset/output_from_code/'\n",
    "output_filename = 'evaluation_results_with_predictions.csv'\n",
    "output_filepath = os.path.join(gdrive_output_path, output_filename)\n",
    "\n",
    "# Save CSV file\n",
    "try:\n",
    "    results_df.to_csv(output_filepath, index=False)\n",
    "    print(f\"Successfully saved detailed results to: {output_filepath}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ],
   "metadata": {
    "id": "mjrohCHKQ-bh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(\"\\n💾 Saving the trained model weights...\")\n",
    "\n",
    "#Define the path to save the model\n",
    "save_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
    "\n",
    "#Save the parameters\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved successfully to: {save_path}\")"
   ],
   "metadata": {
    "id": "cmASY2G2eaDG"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}