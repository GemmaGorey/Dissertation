{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "toc_visible": true,
   "gpuType": "A100",
   "authorship_tag": "ABX9TyP+8OBlwZLEWUMCaGW1ZhcQ",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Similarity_Analysis_Audio_vs_Lyrics_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB327CIPG0v4"
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "# install mamba to use instead of pip"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the config file and build the environment.\n",
    "yaml_content = \"\"\"\n",
    "name: dissertation\n",
    "channels:\n",
    "  - pytorch\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pytorch=2.2.2\n",
    "  - torchvision=0.17.2\n",
    "  - torchaudio\n",
    "  - librosa\n",
    "  - numpy<2\n",
    "  - pandas\n",
    "  - jupyter\n",
    "  - wandb\n",
    "\"\"\"\n",
    "\n",
    "# Write the string content to a file -  'environment.yml'.\n",
    "with open('environment.yml', 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"environment.yml file created successfully.\")\n",
    "\n",
    "# create the environment using mamba from the yml file.\n",
    "print(\"\\n Creating environment\")\n",
    "\n",
    "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
   ],
   "metadata": {
    "id": "NqkjmK3RHcYW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# imports and setting up of GitHub and W&B\n",
    "\n",
    "# clone project repository from GitHub\n",
    "print(\"⏳ Cloning GitHub repository...\")\n",
    "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
    "print(\"Repository cloned.\")\n",
    "\n",
    "#Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import subprocess\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import types\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ],
   "metadata": {
    "id": "tG0a7AkQHf2F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MER_Dataset(Dataset):\n",
    "    \"\"\" Custom PyTorch Dataset for loading MER data. \"\"\"\n",
    "    def __init__(self, annotations_df, tokenizer):\n",
    "        \"\"\" Creation of the Dataset from the dataframe (predefined splits in MERGE dataset) \"\"\"\n",
    "        self.annotations = annotations_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function to return the total number of songs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Function to get a song from the dataset.\n",
    "        \"\"\"\n",
    "        song_info = self.annotations.iloc[index] #which song ID/row is picked from the dataset as per the index\n",
    "\n",
    "        spectrogram_path = song_info['spectrogram_path'] # columns from the df\n",
    "        lyrics_path = song_info['lyrics_path'] # columns from the df\n",
    "        valence = song_info['valence'] # columns from the df\n",
    "        arousal = song_info['arousal'] # columns from the df\n",
    "\n",
    "        #change spectorgram into a tensor\n",
    "        spectrogram = np.load(spectrogram_path) #loading spectorgram from path saved in df\n",
    "        spectrogram_tensor = torch.from_numpy(spectrogram).float() # changing the np array to tensor\n",
    "        spectrogram_tensor = spectrogram_tensor.unsqueeze(0) #Adding a \"channel\" dimension for CNN\n",
    "\n",
    "        #Load the lyric tokens\n",
    "        encoded_lyrics = torch.load(lyrics_path, weights_only=False)\n",
    "        input_ids = encoded_lyrics['input_ids'].squeeze(0) #remove the batch dimension from input ids so 1d array\n",
    "        attention_mask = encoded_lyrics['attention_mask'].squeeze(0) #remove the batch dimension from attention mask so 1d\n",
    "\n",
    "        labels = torch.tensor([valence, arousal], dtype=torch.float32) # extract labels\n",
    "\n",
    "        return spectrogram_tensor, input_ids, attention_mask, labels"
   ],
   "metadata": {
    "id": "vmJVKE2BAbTZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AttentionModule(nn.Module): #Addition from V1\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        '''\n",
    "        Attention mechanism to weight the importance of different features\n",
    "        '''\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 4),  # input is 64 will map to16\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim // 4, feature_dim),  #reverts back to 64 from 16\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 64]\n",
    "        attention_weights = self.attention(x)  # [batch_size, 64]\n",
    "        weighted_features = x * attention_weights  # Element-wise multiplication\n",
    "        return weighted_features"
   ],
   "metadata": {
    "id": "uQy6xyLPJxAc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class VGGish_Audio_Model(nn.Module):\n",
    "    '''As previous vERSION but adding in the following\n",
    "      - Batch normalisation\n",
    "      - Attention mechanism\n",
    "      - Learning rate scheduling\n",
    "      - early stopping'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGish_Audio_Model, self).__init__()\n",
    "        '''\n",
    "        A VGG-style model for the audio tower for a starting model.\n",
    "        No longer trying to implement the method from MERGE paper as this had mistakes in the paper\n",
    "        V1.1 includes attention to see if this improves performance.\n",
    "        V1.2  implements true VGG-style blocks with multiple convolutions per block.\n",
    "        '''\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1 - 2 convolutions\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2 - 2 convolutions\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3 - 2 convolutions\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4 - 2 convolutions\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.attention = AttentionModule(256) #Add attention here from v2 (model 3)\n",
    "        self.fc2 = nn.Linear(256, 64) # Final feature vector size should be 64 - needs to match input of combined\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the features for the classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "pM1B4zwQcdnb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BimodalClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The final bimodal model. No longer using MERGE archtecture as\n",
    "    transformer would be better. Also due to mistakes in the paper it is\n",
    "    unclear what some of the parameters are.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BimodalClassifier, self).__init__()\n",
    "\n",
    "        #initiate audio tower\n",
    "        self.audio_tower = VGGish_Audio_Model()\n",
    "\n",
    "        #use transformer for lyrics (using bert base uncased for now, but may change)\n",
    "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.lyrics_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define feature sizes from the previous step and from bert\n",
    "        AUDIO_FEATURES_OUT = 64\n",
    "        LYRICS_FEATURES_OUT = 768\n",
    "        COMBINED_FEATURES = AUDIO_FEATURES_OUT + LYRICS_FEATURES_OUT\n",
    "\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=COMBINED_FEATURES, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=100, out_features=2) # 2 Outputs for Valence and Arousal\n",
    "        )\n",
    "\n",
    "    def forward(self, x_audio, input_ids, attention_mask):\n",
    "        #process audio input\n",
    "        audio_features = self.audio_tower(x_audio)\n",
    "\n",
    "        #get lyric features\n",
    "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #use the embedding of the [CLS] token as the feature vector for whole lyrics\n",
    "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        #combine the features from both towers\n",
    "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
    "\n",
    "        #pass the combined features to the final classifier head\n",
    "        output = self.classifier_head(combined_features)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "id": "r1llg53WvWDy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features(self, x_audio, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Extract audio and lyrics features separately (before fusion).\n",
    "    Returns: (audio_features, lyrics_features, predictions)\n",
    "    \"\"\"\n",
    "\n",
    "    # Process audio input\n",
    "    audio_features = self.audio_tower(x_audio)  # [batch_size, 64]\n",
    "\n",
    "    # Get lyric features\n",
    "    lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
    "\n",
    "    # Combine features and get predictions\n",
    "    combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
    "    predictions = self.classifier_head(combined_features)\n",
    "\n",
    "    return audio_features, lyrics_features, predictions"
   ],
   "metadata": {
    "id": "hGyfADadkeTn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Data loading and prep\n",
    "\n",
    "#get the paths to dissertation folder and new folder on colab\n",
    "print(\"Starting data transfer from Google Drive to local Colab storage...\")\n",
    "\n",
    "#get paths for old file location and new colab one\n",
    "gdrive_zip_path = '/content/drive/MyDrive/dissertation/merge_dataset_zipped.zip'\n",
    "local_storage_path = '/content/local_dissertation_data/'\n",
    "local_zip_path = os.path.join(local_storage_path, 'merge_dataset_zipped.zip')\n",
    "os.makedirs(local_storage_path, exist_ok=True) # Ensure the destination directory exists\n",
    "\n",
    "#Copy zip file from Drive to Colab\n",
    "print(\"Copying single archive file from Google Drive...\")\n",
    "!rsync -ah --progress \"{gdrive_zip_path}\" \"{local_storage_path}\"\n",
    "\n",
    "#get total number of files for progress\n",
    "total_files = int(subprocess.check_output(f\"zipinfo -1 {local_zip_path} | wc -l\", shell=True))\n",
    "\n",
    "#unzip the file\n",
    "print(\"Extracting files locally\")\n",
    "!unzip -o \"{local_zip_path}\" -d \"{local_storage_path}\" | tqdm --unit=files --total={total_files} > /dev/null\n",
    "\n",
    "print(\"Data transfer and extraction complete.\")\n",
    "\n",
    "#load master data from new location\n",
    "local_output_path = os.path.join(local_storage_path, 'merge_dataset/output_from_code/')\n",
    "master_file_path = os.path.join(local_output_path, 'master_processed_file_list.csv')\n",
    "master_df = pd.read_csv(master_file_path)\n",
    "\n",
    "#checking the valence and arousal range in the dataset\n",
    "print(f\"\\nValence range in data: [{master_df['valence'].min()}, {master_df['valence'].max()}]\")\n",
    "print(f\"Arousal range in data: [{master_df['arousal'].min()}, {master_df['arousal'].max()}]\")\n",
    "print(f\"Valence mean: {master_df['valence'].mean():.4f}, std: {master_df['valence'].std():.4f}\")\n",
    "print(f\"Arousal mean: {master_df['arousal'].mean():.4f}, std: {master_df['arousal'].std():.4f}\")\n",
    "print(f\"Total samples in master_df: {len(master_df)}\")\n",
    "\n",
    "# Verify its the right column - not quadrants\n",
    "print(f\"\\nNumber of unique valence values: {master_df['valence'].nunique()}\")\n",
    "print(f\"Number of unique arousal values: {master_df['arousal'].nunique()}\")\n",
    "print(f\"Number of unique quadrant values: {master_df['quadrant'].nunique()}\")\n",
    "\n",
    "# Sample some actual values\n",
    "print(f\"\\nSample valence values: {master_df['valence'].sample(10).values}\")\n",
    "print(f\"Sample arousal values: {master_df['arousal'].sample(10).values}\")\n",
    "\n",
    "#update the paths in the csv\n",
    "print(\"\\nUpdating dataframe paths to use fast local storage...\")\n",
    "gdrive_output_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
    "master_df['spectrogram_path'] = master_df['spectrogram_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
    "master_df['lyrics_path'] = master_df['lyrics_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
    "print(\"Dataframe paths updated.\")\n",
    "\n",
    "#load the data splits from the new path in the predefined splits folder tvt\n",
    "local_split_folder_path = os.path.join(local_storage_path, 'merge_dataset/MERGE_Bimodal_Complete/tvt_dataframes/tvt_70_15_15/')\n",
    "train_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_train_bimodal_complete.csv'))\n",
    "val_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_validate_bimodal_complete.csv'))\n",
    "test_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_test_bimodal_complete.csv'))\n",
    "print(\"\\nSplit files loaded from local storage.\")\n",
    "\n",
    "#merge the files\n",
    "id_column_name = 'song_id'\n",
    "train_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "val_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "test_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "\n",
    "train_df = pd.merge(master_df, train_split_df, on=id_column_name)\n",
    "val_df = pd.merge(master_df, val_split_df, on=id_column_name)\n",
    "test_df = pd.merge(master_df, test_split_df, on=id_column_name)\n",
    "\n",
    "#checking no files are lost in merging - and checking length of the dataframes.\n",
    "print(\"\\nchecking data\")\n",
    "\n",
    "#check no data lost in merge\n",
    "if len(train_df) == len(train_split_df):\n",
    "    print(\"\\nTraining split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Training split lost {len(train_split_df) - len(train_df)} songs during merge.\")\n",
    "\n",
    "if len(val_df) == len(val_split_df):\n",
    "    print(\"Validation split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"WARNING: Validation split lost {len(val_split_df) - len(val_df)} songs during merge.\")\n",
    "\n",
    "if len(test_df) == len(test_split_df):\n",
    "    print(\"Test split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"WARNING: Test split lost {len(test_split_df) - len(test_df)} songs during merge.\")\n",
    "\n",
    "#check length\n",
    "expected_train_len = 1552\n",
    "expected_val_len = 332\n",
    "expected_test_len = 332\n",
    "\n",
    "assert len(train_df) == expected_train_len, f\"Expected {expected_train_len} training samples, but found {len(train_df)}\"\n",
    "assert len(val_df) == expected_val_len, f\"Expected {expected_val_len} validation samples, but found {len(val_df)}\"\n",
    "assert len(test_df) == expected_test_len, f\"Expected {expected_test_len} test samples, but found {len(test_df)}\"\n",
    "\n",
    "print(f\"\\nFinal dataset lengths are correct: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
    "print(\"Data Check Complete\")\n",
    "\n",
    "#createthe datasets and loaders\n",
    "train_dataset = MER_Dataset(annotations_df=train_df, tokenizer=tokenizer)\n",
    "val_dataset = MER_Dataset(annotations_df=val_df, tokenizer=tokenizer)\n",
    "test_dataset = MER_Dataset(annotations_df=test_df, tokenizer=tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")"
   ],
   "metadata": {
    "id": "KUbPRcEUHrTM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#select dataset for similarity analysis\n",
    "\n",
    "analysis_df = test_df.copy()  #can change to train_df or val_df\n",
    "\n",
    "print(f\"\\n✓ Selected dataset for similarity analysis: TEST SET\")\n",
    "print(f\"  Total songs to analyze: {len(analysis_df)}\")\n",
    "print(f\"  Song IDs: {analysis_df[id_column_name].head(10).tolist()}...\")"
   ],
   "metadata": {
    "id": "6ngnjHAHktgZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA device.\")\n",
    "else:\n",
    "    # If no GPU is found, print an error and stop execution by raising an error.\n",
    "    raise RuntimeError(\"Error: No GPU found. This script requires a GPU to run.\")\n"
   ],
   "metadata": {
    "id": "sKtbxvlbtVsv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BimodalClassifier()\n",
    "model.to(device)\n",
    "#load model 4\n",
    "model_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "# Add the method wfor getting features\n",
    "model.get_features = types.MethodType(get_features, model)\n",
    "\n",
    "print(\"Feature extraction added to model.\")\n",
    "\n"
   ],
   "metadata": {
    "id": "x2r9lbd-SH7j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_features_from_dataset(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extract audio and lyrics features for all songs in the dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    #Create lists to store results\n",
    "    audio_features_list = []\n",
    "    lyrics_features_list = []\n",
    "    predictions_list = []\n",
    "    ground_truth_list = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Extract features without computing gradients\n",
    "    with torch.no_grad():\n",
    "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            # Move data to device\n",
    "            spectrogram_batch = spectrogram_batch.to(device)\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "\n",
    "            # Extract features\n",
    "            audio_feat, lyrics_feat, preds = model.get_features(\n",
    "                spectrogram_batch,\n",
    "                input_ids_batch,\n",
    "                attention_mask_batch\n",
    "            )\n",
    "\n",
    "            # Move to CPU and convert to numpy\n",
    "            audio_features_list.append(audio_feat.cpu().numpy())\n",
    "            lyrics_features_list.append(lyrics_feat.cpu().numpy())\n",
    "            predictions_list.append(preds.cpu().numpy())\n",
    "            ground_truth_list.append(labels_batch.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    audio_features = np.concatenate(audio_features_list, axis=0)      # [N, 64]\n",
    "    lyrics_features = np.concatenate(lyrics_features_list, axis=0)    # [N, 768]\n",
    "    predictions = np.concatenate(predictions_list, axis=0)            # [N, 2]\n",
    "    ground_truth = np.concatenate(ground_truth_list, axis=0)          # [N, 2]\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n✓ Feature extraction complete!\")\n",
    "    print(f\"  Total songs processed: {len(audio_features)}\")\n",
    "    print(f\"  Audio features shape:  {audio_features.shape}\")\n",
    "    print(f\"  Lyrics features shape: {lyrics_features.shape}\")\n",
    "    print(f\"  Predictions shape:     {predictions.shape}\")\n",
    "    print(f\"  Ground truth shape:    {ground_truth.shape}\")\n",
    "\n",
    "    return {\n",
    "        'audio_features': audio_features,\n",
    "        'lyrics_features': lyrics_features,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "\n",
    ""
   ],
   "metadata": {
    "id": "BAcQ0UJ_mAHh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract features and predictions from the bimodal model\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXTRACTING BIMODAL FEATURES AND PREDICTIONS\")\nprint(\"=\"*70)\n\nfeatures_dict = extract_features_from_dataset(model, test_loader, device)\naudio_features = features_dict['audio_features']\nlyrics_features = features_dict['lyrics_features']\npredictions = features_dict['predictions']  # Needed for comparison in Cell 17\nground_truth_bimodal = features_dict['ground_truth']\n\nprint(f\"\\n✓ Bimodal predictions extracted and ready for comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class AudioOnlyModel(nn.Module):\n",
    "    \"\"\"Audio-only model for MER prediction.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(AudioOnlyModel, self).__init__()\n",
    "        self.audio_tower = VGGish_Audio_Model()  # Same as bimodal\n",
    "\n",
    "        # Classifier head for audio only\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 2)  # Valence and Arousal\n",
    "        )\n",
    "\n",
    "    def forward(self, x_audio):\n",
    "        audio_features = self.audio_tower(x_audio)\n",
    "        predictions = self.classifier(audio_features)\n",
    "        return predictions\n",
    "\n",
    "class LyricsOnlyModel(nn.Module):\n",
    "    \"\"\"Lyrics-only model for MER prediction.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LyricsOnlyModel, self).__init__()\n",
    "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.lyrics_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Classifier head for lyrics only\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(768, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 2)  # Valence and Arousal\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        predictions = self.classifier(lyrics_features)\n",
    "        return predictions\n"
   ],
   "metadata": {
    "id": "KQzznhhbZ1Rn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Initialise models\naudio_only_model = AudioOnlyModel().to(device)\nlyrics_only_model = LyricsOnlyModel().to(device)\n\n# Try to load pre-trained weights if they exist\naudio_model_path = '/content/drive/MyDrive/dissertation/audio_only_model.pth'\nlyrics_model_path = '/content/drive/MyDrive/dissertation/lyrics_only_model.pth'\n\n# Flag to control whether to train\nTRAIN_UNIMODAL = True  # Set to False if you want to load pre-trained weights instead\n\nif not TRAIN_UNIMODAL:\n    try:\n        audio_only_model.load_state_dict(torch.load(audio_model_path, map_location=device))\n        print(\"Loaded pre-trained audio-only model\")\n    except:\n        print(\"No pre-trained audio-only model found\")\n        TRAIN_UNIMODAL = True\n\n    try:\n        lyrics_only_model.load_state_dict(torch.load(lyrics_model_path, map_location=device))\n        print(\"Loaded pre-trained lyrics-only model\")\n    except:\n        print(\"No pre-trained lyrics-only model found\")\n        TRAIN_UNIMODAL = True\n\nif TRAIN_UNIMODAL:\n    print(\"\\n\" + \"=\"*70)\n    print(\"TRAINING UNIMODAL MODELS\")\n    print(\"=\"*70)\n\n    # Training configuration matching MODEL 4\n    NUM_EPOCHS = 50\n    optimizer_audio = optim.Adam(audio_only_model.parameters(), lr=0.001)\n    optimizer_lyrics = optim.Adam(lyrics_only_model.parameters(), lr=0.001)\n    loss_fn = nn.MSELoss()\n\n    scheduler_audio = optim.lr_scheduler.StepLR(optimizer_audio, step_size=15, gamma=0.5)\n    scheduler_lyrics = optim.lr_scheduler.StepLR(optimizer_lyrics, step_size=15, gamma=0.5)\n\n    # Initialize W&B for both models\n    wandb.init(project=\"dissertation-mer-unimodal\", name=\"audio-only-training\")\n\n    # Early stopping setup for audio model\n    best_val_loss_audio = float('inf')\n    patience = 10\n    patience_counter_audio = 0\n    best_audio_state = None\n    best_epoch_audio = 0\n\n    print(\"\\n--- Training Audio-Only Model ---\\n\")\n\n    for epoch in range(NUM_EPOCHS):\n        # Training\n        audio_only_model.train()\n        total_train_loss = 0\n\n        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(train_loader, desc=f\"Audio Training Epoch {epoch+1}\"):\n            spectrogram_batch = spectrogram_batch.to(device)\n            labels_batch = labels_batch.to(device)\n\n            optimizer_audio.zero_grad()\n            outputs = audio_only_model(spectrogram_batch)\n            loss = loss_fn(outputs, labels_batch)\n            total_train_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(audio_only_model.parameters(), max_norm=1.0)\n            optimizer_audio.step()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        print(f\"Audio Epoch {epoch+1}/{NUM_EPOCHS}, Training Loss: {avg_train_loss:.4f}\")\n        wandb.log({\"audio_epoch\": epoch+1, \"audio_train_loss\": avg_train_loss})\n\n        # Validation\n        audio_only_model.eval()\n        total_val_loss = 0\n\n        with torch.no_grad():\n            for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(val_loader, desc=f\"Audio Validation Epoch {epoch+1}\"):\n                spectrogram_batch = spectrogram_batch.to(device)\n                labels_batch = labels_batch.to(device)\n\n                outputs = audio_only_model(spectrogram_batch)\n                loss = loss_fn(outputs, labels_batch)\n                total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        print(f\"Audio Epoch {epoch+1}/{NUM_EPOCHS}, Validation Loss: {avg_val_loss:.4f}\")\n        wandb.log({\"audio_val_loss\": avg_val_loss})\n\n        # Early stopping\n        if avg_val_loss < best_val_loss_audio:\n            best_val_loss_audio = avg_val_loss\n            best_epoch_audio = epoch + 1\n            patience_counter_audio = 0\n            best_audio_state = audio_only_model.state_dict().copy()\n            print(f\"✓ New best audio validation loss: {best_val_loss_audio:.4f}\")\n        else:\n            patience_counter_audio += 1\n            print(f\"No improvement for {patience_counter_audio} epochs (patience: {patience})\")\n\n            if patience_counter_audio >= patience:\n                print(f\"Early stopping triggered! Best validation loss: {best_val_loss_audio:.4f}\")\n                audio_only_model.load_state_dict(best_audio_state)\n                break\n\n        scheduler_audio.step()\n        current_lr = scheduler_audio.get_last_lr()[0]\n        print(f\"Learning Rate: {current_lr:.6f}\")\n        wandb.log({\"audio_learning_rate\": current_lr})\n\n    print(f\"\\nAudio model training complete. Best val loss: {best_val_loss_audio:.4f} at epoch {best_epoch_audio}\")\n\n    # Finish audio W&B run\n    wandb.finish()\n\n    # Now train lyrics model\n    wandb.init(project=\"dissertation-mer-unimodal\", name=\"lyrics-only-training\")\n\n    best_val_loss_lyrics = float('inf')\n    patience_counter_lyrics = 0\n    best_lyrics_state = None\n    best_epoch_lyrics = 0\n\n    print(\"\\n--- Training Lyrics-Only Model ---\\n\")\n\n    for epoch in range(NUM_EPOCHS):\n        # Training\n        lyrics_only_model.train()\n        total_train_loss = 0\n\n        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(train_loader, desc=f\"Lyrics Training Epoch {epoch+1}\"):\n            input_ids_batch = input_ids_batch.to(device)\n            attention_mask_batch = attention_mask_batch.to(device)\n            labels_batch = labels_batch.to(device)\n\n            optimizer_lyrics.zero_grad()\n            outputs = lyrics_only_model(input_ids_batch, attention_mask_batch)\n            loss = loss_fn(outputs, labels_batch)\n            total_train_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(lyrics_only_model.parameters(), max_norm=1.0)\n            optimizer_lyrics.step()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        print(f\"Lyrics Epoch {epoch+1}/{NUM_EPOCHS}, Training Loss: {avg_train_loss:.4f}\")\n        wandb.log({\"lyrics_epoch\": epoch+1, \"lyrics_train_loss\": avg_train_loss})\n\n        # Validation\n        lyrics_only_model.eval()\n        total_val_loss = 0\n\n        with torch.no_grad():\n            for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(val_loader, desc=f\"Lyrics Validation Epoch {epoch+1}\"):\n                input_ids_batch = input_ids_batch.to(device)\n                attention_mask_batch = attention_mask_batch.to(device)\n                labels_batch = labels_batch.to(device)\n\n                outputs = lyrics_only_model(input_ids_batch, attention_mask_batch)\n                loss = loss_fn(outputs, labels_batch)\n                total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        print(f\"Lyrics Epoch {epoch+1}/{NUM_EPOCHS}, Validation Loss: {avg_val_loss:.4f}\")\n        wandb.log({\"lyrics_val_loss\": avg_val_loss})\n\n        # Early stopping\n        if avg_val_loss < best_val_loss_lyrics:\n            best_val_loss_lyrics = avg_val_loss\n            best_epoch_lyrics = epoch + 1\n            patience_counter_lyrics = 0\n            best_lyrics_state = lyrics_only_model.state_dict().copy()\n            print(f\"✓ New best lyrics validation loss: {best_val_loss_lyrics:.4f}\")\n        else:\n            patience_counter_lyrics += 1\n            print(f\"No improvement for {patience_counter_lyrics} epochs (patience: {patience})\")\n\n            if patience_counter_lyrics >= patience:\n                print(f\"Early stopping triggered! Best validation loss: {best_val_loss_lyrics:.4f}\")\n                lyrics_only_model.load_state_dict(best_lyrics_state)\n                break\n\n        scheduler_lyrics.step()\n        current_lr = scheduler_lyrics.get_last_lr()[0]\n        print(f\"Learning Rate: {current_lr:.6f}\")\n        wandb.log({\"lyrics_learning_rate\": current_lr})\n\n    print(f\"\\nLyrics model training complete. Best val loss: {best_val_loss_lyrics:.4f} at epoch {best_epoch_lyrics}\")\n\n    # Finish lyrics W&B run\n    wandb.finish()\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"TRAINING COMPLETE\")\n    print(\"=\"*70)\n    print(f\"Audio model  - Best val loss: {best_val_loss_audio:.4f} at epoch {best_epoch_audio}\")\n    print(f\"Lyrics model - Best val loss: {best_val_loss_lyrics:.4f} at epoch {best_epoch_lyrics}\")\n\n# Set to evaluation mode\naudio_only_model.eval()\nlyrics_only_model.eval()",
   "metadata": {
    "id": "1xCMyHbkZ5IA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_unimodal_predictions(audio_model, lyrics_model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Get predictions from audio-only and lyrics-only models.\n",
    "    \"\"\"\n",
    "\n",
    "    audio_preds_list = []\n",
    "    lyrics_preds_list = []\n",
    "    ground_truth_list = []\n",
    "\n",
    "    audio_model.eval()\n",
    "    lyrics_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(dataloader, desc=\"Extracting predictions\"):\n",
    "            # Move to device\n",
    "            spectrogram_batch = spectrogram_batch.to(device)\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "\n",
    "            # Get predictions from each modality\n",
    "            audio_preds = audio_model(spectrogram_batch)\n",
    "            lyrics_preds = lyrics_model(input_ids_batch, attention_mask_batch)\n",
    "\n",
    "            # Store\n",
    "            audio_preds_list.append(audio_preds.cpu().numpy())\n",
    "            lyrics_preds_list.append(lyrics_preds.cpu().numpy())\n",
    "            ground_truth_list.append(labels_batch.cpu().numpy())\n",
    "\n",
    "    # Concatenate\n",
    "    audio_predictions = np.concatenate(audio_preds_list, axis=0)\n",
    "    lyrics_predictions = np.concatenate(lyrics_preds_list, axis=0)\n",
    "    ground_truth = np.concatenate(ground_truth_list, axis=0)\n",
    "\n",
    "    print(f\"\\n✓ Extraction complete!\")\n",
    "    print(f\"  Audio predictions shape:  {audio_predictions.shape}\")\n",
    "    print(f\"  Lyrics predictions shape: {lyrics_predictions.shape}\")\n",
    "    print(f\"  Ground truth shape:       {ground_truth.shape}\")\n",
    "\n",
    "    return audio_predictions, lyrics_predictions, ground_truth\n",
    "\n",
    "# Extract predictions\n",
    "audio_predictions, lyrics_predictions, ground_truth = extract_unimodal_predictions(\n",
    "    audio_only_model,\n",
    "    lyrics_only_model,\n",
    "    test_loader,\n",
    "    device\n",
    ")\n"
   ],
   "metadata": {
    "id": "13cd4P7xaFAZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_unimodal_comparison(audio_preds, lyrics_preds, bimodal_preds, ground_truth):\n",
    "    \"\"\"\n",
    "    Compare predictions between modalities using Euclidean distance.\n",
    "    Also compute performance metrics for each modality.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNIMODAL VS BIMODAL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    n_samples = len(audio_preds)\n",
    "\n",
    "    # Compute Euclidean distances between predictions\n",
    "    audio_lyrics_distances = []\n",
    "    audio_bimodal_distances = []\n",
    "    lyrics_bimodal_distances = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Distance between audio and lyrics predictions\n",
    "        d1 = euclidean(audio_preds[i], lyrics_preds[i])\n",
    "        audio_lyrics_distances.append(d1)\n",
    "\n",
    "        # Distance between audio and bimodal\n",
    "        d2 = euclidean(audio_preds[i], bimodal_preds[i])\n",
    "        audio_bimodal_distances.append(d2)\n",
    "\n",
    "        # Distance between lyrics and bimodal\n",
    "        d3 = euclidean(lyrics_preds[i], bimodal_preds[i])\n",
    "        lyrics_bimodal_distances.append(d3)\n",
    "\n",
    "    audio_lyrics_distances = np.array(audio_lyrics_distances)\n",
    "    audio_bimodal_distances = np.array(audio_bimodal_distances)\n",
    "    lyrics_bimodal_distances = np.array(lyrics_bimodal_distances)\n",
    "\n",
    "    # Print distance statistics\n",
    "    print(f\"\\n1. EUCLIDEAN DISTANCES BETWEEN PREDICTIONS:\")\n",
    "    print(f\"   Audio ↔ Lyrics:   Mean={audio_lyrics_distances.mean():.4f}, Std={audio_lyrics_distances.std():.4f}\")\n",
    "    print(f\"   Audio ↔ Bimodal:  Mean={audio_bimodal_distances.mean():.4f}, Std={audio_bimodal_distances.std():.4f}\")\n",
    "    print(f\"   Lyrics ↔ Bimodal: Mean={lyrics_bimodal_distances.mean():.4f}, Std={lyrics_bimodal_distances.std():.4f}\")\n",
    "\n",
    "    # Compute performance metrics for each modality\n",
    "    print(f\"\\n2. PERFORMANCE METRICS:\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for name, preds in [('Audio-Only', audio_preds),\n",
    "                        ('Lyrics-Only', lyrics_preds),\n",
    "                        ('Bimodal', bimodal_preds)]:\n",
    "\n",
    "        # Overall metrics\n",
    "        mse = mean_squared_error(ground_truth, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(ground_truth, preds)\n",
    "        r2 = r2_score(ground_truth, preds)\n",
    "\n",
    "        # Per-dimension metrics\n",
    "        valence_rmse = np.sqrt(mean_squared_error(ground_truth[:, 0], preds[:, 0]))\n",
    "        arousal_rmse = np.sqrt(mean_squared_error(ground_truth[:, 1], preds[:, 1]))\n",
    "\n",
    "        valence_mae = mean_absolute_error(ground_truth[:, 0], preds[:, 0])\n",
    "        arousal_mae = mean_absolute_error(ground_truth[:, 1], preds[:, 1])\n",
    "\n",
    "        valence_r2 = r2_score(ground_truth[:, 0], preds[:, 0])\n",
    "        arousal_r2 = r2_score(ground_truth[:, 1], preds[:, 1])\n",
    "\n",
    "        metrics[name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2,\n",
    "            'Valence_RMSE': valence_rmse,\n",
    "            'Arousal_RMSE': arousal_rmse,\n",
    "            'Valence_MAE': valence_mae,\n",
    "            'Arousal_MAE': arousal_mae,\n",
    "            'Valence_R2': valence_r2,\n",
    "            'Arousal_R2': arousal_r2\n",
    "        }\n",
    "\n",
    "        print(f\"\\n   {name}:\")\n",
    "        print(f\"     Overall  - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "        print(f\"     Valence  - RMSE: {valence_rmse:.4f}, MAE: {valence_mae:.4f}, R²: {valence_r2:.4f}\")\n",
    "        print(f\"     Arousal  - RMSE: {arousal_rmse:.4f}, MAE: {arousal_mae:.4f}, R²: {arousal_r2:.4f}\")\n",
    "\n",
    "    return metrics, audio_lyrics_distances, audio_bimodal_distances, lyrics_bimodal_distances\n",
    "\n",
    "# Run comparison\n",
    "metrics, audio_lyrics_dist, audio_bimodal_dist, lyrics_bimodal_dist = compute_unimodal_comparison(\n",
    "    audio_predictions,\n",
    "    lyrics_predictions,\n",
    "    predictions,  # From bimodal model (Cell 14)\n",
    "    ground_truth\n",
    ")"
   ],
   "metadata": {
    "id": "Afhde4X6aL7W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Euclidean Distance Distributions\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.hist(audio_lyrics_dist, bins=30, alpha=0.6, label='Audio ↔ Lyrics', color='steelblue')\n",
    "ax1.hist(audio_bimodal_dist, bins=30, alpha=0.6, label='Audio ↔ Bimodal', color='orange')\n",
    "ax1.hist(lyrics_bimodal_dist, bins=30, alpha=0.6, label='Lyrics ↔ Bimodal', color='green')\n",
    "ax1.set_xlabel('Euclidean Distance', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of Prediction Distances', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "models = ['Audio-Only', 'Lyrics-Only', 'Bimodal']\n",
    "rmse_vals = [metrics[m]['RMSE'] for m in models]\n",
    "colors_rmse = ['steelblue', 'orange', 'green']\n",
    "ax2.bar(models, rmse_vals, color=colors_rmse, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('RMSE', fontsize=12)\n",
    "ax2.set_title('Overall RMSE Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(rmse_vals):\n",
    "    ax2.text(i, v + 0.005, f'{v:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "# 3. R² Comparison\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "r2_vals = [metrics[m]['R2'] for m in models]\n",
    "ax3.bar(models, r2_vals, color=colors_rmse, alpha=0.7, edgecolor='black')\n",
    "ax3.set_ylabel('R² Score', fontsize=12)\n",
    "ax3.set_title('Overall R² Comparison', fontsize=12, fontweight='bold')\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(r2_vals):\n",
    "    ax3.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# 4. MAE Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "mae_vals = [metrics[m]['MAE'] for m in models]\n",
    "ax4.bar(models, mae_vals, color=colors_rmse, alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylabel('MAE', fontsize=12)\n",
    "ax4.set_title('Overall MAE Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(mae_vals):\n",
    "    ax4.text(i, v + 0.003, f'{v:.4f}', ha='center', fontsize=10)\n",
    "\n",
    "# 5. Valence RMSE Comparison\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "val_rmse = [metrics[m]['Valence_RMSE'] for m in models]\n",
    "ax5.bar(models, val_rmse, color=colors_rmse, alpha=0.7, edgecolor='black')\n",
    "ax5.set_ylabel('RMSE', fontsize=12)\n",
    "ax5.set_title('Valence RMSE', fontsize=12, fontweight='bold')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Arousal RMSE Comparison\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "aro_rmse = [metrics[m]['Arousal_RMSE'] for m in models]\n",
    "ax6.bar(models, aro_rmse, color=colors_rmse, alpha=0.7, edgecolor='black')\n",
    "ax6.set_ylabel('RMSE', fontsize=12)\n",
    "ax6.set_title('Arousal RMSE', fontsize=12, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 7. Scatter: Audio vs Lyrics Predictions (Valence)\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.scatter(audio_predictions[:, 0], lyrics_predictions[:, 0], alpha=0.5, s=20, color='purple')\n",
    "ax7.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect agreement')\n",
    "ax7.set_xlabel('Audio Valence Prediction', fontsize=10)\n",
    "ax7.set_ylabel('Lyrics Valence Prediction', fontsize=10)\n",
    "ax7.set_title('Audio vs Lyrics: Valence', fontsize=12, fontweight='bold')\n",
    "ax7.legend()\n",
    "ax7.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Unimodal vs Bimodal Prediction Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "iczD_ZNZacv5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create comprehensive results DataFrame\n",
    "results_df = analysis_df[[id_column_name, 'valence', 'arousal']].copy()\n",
    "\n",
    "# Add predictions from all three models\n",
    "results_df['audio_valence_pred'] = audio_predictions[:, 0]\n",
    "results_df['audio_arousal_pred'] = audio_predictions[:, 1]\n",
    "results_df['lyrics_valence_pred'] = lyrics_predictions[:, 0]\n",
    "results_df['lyrics_arousal_pred'] = lyrics_predictions[:, 1]\n",
    "results_df['bimodal_valence_pred'] = predictions[:, 0]\n",
    "results_df['bimodal_arousal_pred'] = predictions[:, 1]\n",
    "\n",
    "# Add Euclidean distances\n",
    "results_df['audio_lyrics_distance'] = audio_lyrics_dist\n",
    "results_df['audio_bimodal_distance'] = audio_bimodal_dist\n",
    "results_df['lyrics_bimodal_distance'] = lyrics_bimodal_dist\n",
    "\n",
    "# Add individual errors\n",
    "results_df['audio_valence_error'] = np.abs(audio_predictions[:, 0] - ground_truth[:, 0])\n",
    "results_df['audio_arousal_error'] = np.abs(audio_predictions[:, 1] - ground_truth[:, 1])\n",
    "results_df['lyrics_valence_error'] = np.abs(lyrics_predictions[:, 0] - ground_truth[:, 0])\n",
    "results_df['lyrics_arousal_error'] = np.abs(lyrics_predictions[:, 1] - ground_truth[:, 1])\n",
    "results_df['bimodal_valence_error'] = np.abs(predictions[:, 0] - ground_truth[:, 0])\n",
    "results_df['bimodal_arousal_error'] = np.abs(predictions[:, 1] - ground_truth[:, 1])\n",
    "\n",
    "# Save to Excel\n",
    "excel_path = '/content/drive/MyDrive/dissertation/unimodal_comparison_results.xlsx'\n",
    "results_df.to_excel(excel_path, index=False, sheet_name='Predictions')\n",
    "\n",
    "print(f\"Results saved to Excel: {excel_path}\")\n",
    "print(f\"\\nColumns saved:\")\n",
    "print(f\"  - Song ID and ground truth (valence, arousal)\")\n",
    "print(f\"  - Audio predictions (valence, arousal)\")\n",
    "print(f\"  - Lyrics predictions (valence, arousal)\")\n",
    "print(f\"  - Bimodal predictions (valence, arousal)\")\n",
    "print(f\"  - Euclidean distances between predictions\")\n",
    "print(f\"  - Individual errors for each modality\")"
   ],
   "metadata": {
    "id": "sm0u6fThados"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save unimodal model weights with descriptive names\n",
    "output_dir = '/content/drive/MyDrive/dissertation/'\n",
    "\n",
    "# Save audio-only model\n",
    "audio_save_path = os.path.join(output_dir, 'audio_only_model_weights.pth')\n",
    "torch.save(audio_only_model.state_dict(), audio_save_path)\n",
    "print(f\"Audio-only model saved: {audio_save_path}\")\n",
    "\n",
    "# Save lyrics-only model\n",
    "lyrics_save_path = os.path.join(output_dir, 'lyrics_only_model_weights.pth')\n",
    "torch.save(lyrics_only_model.state_dict(), lyrics_save_path)\n",
    "print(f\"Lyrics-only model saved: {lyrics_save_path}\")\n",
    "\n",
    "# Also save a summary JSON with metrics\n",
    "metrics_summary = {\n",
    "    'audio_only': metrics['Audio-Only'],\n",
    "    'lyrics_only': metrics['Lyrics-Only'],\n",
    "    'bimodal': metrics['Bimodal'],\n",
    "    'distance_statistics': {\n",
    "        'audio_lyrics_mean': float(audio_lyrics_dist.mean()),\n",
    "        'audio_lyrics_std': float(audio_lyrics_dist.std()),\n",
    "        'audio_bimodal_mean': float(audio_bimodal_dist.mean()),\n",
    "        'audio_bimodal_std': float(audio_bimodal_dist.std()),\n",
    "        'lyrics_bimodal_mean': float(lyrics_bimodal_dist.mean()),\n",
    "        'lyrics_bimodal_std': float(lyrics_bimodal_dist.std())\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = os.path.join(output_dir, 'unimodal_comparison_metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "print(f\"Metrics summary saved: {metrics_path}\")"
   ],
   "metadata": {
    "id": "ALPPZT_OamHI"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}