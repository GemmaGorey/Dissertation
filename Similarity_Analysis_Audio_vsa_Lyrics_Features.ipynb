{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN9nYYrfw5ypiZxiS2Ibz5n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Similarity_Analysis_Audio_vsa_Lyrics_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB327CIPG0v4"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# install mamba to use instead of pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the config file and build the environment.\n",
        "yaml_content = \"\"\"\n",
        "name: dissertation\n",
        "channels:\n",
        "  - pytorch\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - pytorch=2.2.2\n",
        "  - torchvision=0.17.2\n",
        "  - torchaudio\n",
        "  - librosa\n",
        "  - numpy<2\n",
        "  - pandas\n",
        "  - jupyter\n",
        "  - wandb\n",
        "\"\"\"\n",
        "\n",
        "# Write the string content to a file -  'environment.yml'.\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"environment.yml file created successfully.\")\n",
        "\n",
        "# create the environment using mamba from the yml file.\n",
        "print(\"\\n Creating environment\")\n",
        "\n",
        "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
      ],
      "metadata": {
        "id": "NqkjmK3RHcYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and setting up of GitHub and W&B\n",
        "\n",
        "# clone project repository from GitHub\n",
        "print(\"â³ Cloning GitHub repository...\")\n",
        "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
        "print(\"Repository cloned.\")\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import subprocess\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cross_decomposition import CCA\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import types\n",
        "import json\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "metadata": {
        "id": "tG0a7AkQHf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MER_Dataset(Dataset):\n",
        "    \"\"\" Custom PyTorch Dataset for loading MER data. \"\"\"\n",
        "    def __init__(self, annotations_df, tokenizer):\n",
        "        \"\"\" Creation of the Dataset from the dataframe (predefined splits in MERGE dataset) \"\"\"\n",
        "        self.annotations = annotations_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Function to return the total number of songs in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Function to get a song from the dataset.\n",
        "        \"\"\"\n",
        "        song_info = self.annotations.iloc[index] #which song ID/row is picked from the dataset as per the index\n",
        "\n",
        "        spectrogram_path = song_info['spectrogram_path'] # columns from the df\n",
        "        lyrics_path = song_info['lyrics_path'] # columns from the df\n",
        "        valence = song_info['valence'] # columns from the df\n",
        "        arousal = song_info['arousal'] # columns from the df\n",
        "\n",
        "        #change spectorgram into a tensor\n",
        "        spectrogram = np.load(spectrogram_path) #loading spectorgram from path saved in df\n",
        "        spectrogram_tensor = torch.from_numpy(spectrogram).float() # changing the np array to tensor\n",
        "        spectrogram_tensor = spectrogram_tensor.unsqueeze(0) #Adding a \"channel\" dimension for CNN\n",
        "\n",
        "        #Load the lyric tokens\n",
        "        encoded_lyrics = torch.load(lyrics_path, weights_only=False)\n",
        "        input_ids = encoded_lyrics['input_ids'].squeeze(0) #remove the batch dimension from input ids so 1d array\n",
        "        attention_mask = encoded_lyrics['attention_mask'].squeeze(0) #remove the batch dimension from attention mask so 1d\n",
        "\n",
        "        labels = torch.tensor([valence, arousal], dtype=torch.float32) # extract labels\n",
        "\n",
        "        return spectrogram_tensor, input_ids, attention_mask, labels"
      ],
      "metadata": {
        "id": "vmJVKE2BAbTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModule(nn.Module): #Addition from V1\n",
        "    def __init__(self, feature_dim):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        '''\n",
        "        Attention mechanism to weight the importance of different features\n",
        "        '''\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim // 4),  # input is 64 will map to16\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feature_dim // 4, feature_dim),  #reverts back to 64 from 16\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, 64]\n",
        "        attention_weights = self.attention(x)  # [batch_size, 64]\n",
        "        weighted_features = x * attention_weights  # Element-wise multiplication\n",
        "        return weighted_features"
      ],
      "metadata": {
        "id": "uQy6xyLPJxAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGish_Audio_Model(nn.Module):\n",
        "    '''As previous vERSION but adding in the following\n",
        "      - Batch normalisation\n",
        "      - Attention mechanism\n",
        "      - Learning rate scheduling\n",
        "      - early stopping'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGish_Audio_Model, self).__init__()\n",
        "        '''\n",
        "        A VGG-style model for the audio tower for a starting model.\n",
        "        No longer trying to implement the method from MERGE paper as this had mistakes in the paper\n",
        "        V1.1 includes attention to see if this improves performance.\n",
        "        V1.2  implements true VGG-style blocks with multiple convolutions per block.\n",
        "        '''\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1 - 2 convolutions\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2 - 2 convolutions\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3 - 2 convolutions\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4 - 2 convolutions\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.attention = AttentionModule(256) #Add attention here from v2 (model 3)\n",
        "        self.fc2 = nn.Linear(256, 64) # Final feature vector size should be 64 - needs to match input of combined\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # Flatten the features for the classifier\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pM1B4zwQcdnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BimodalClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The final bimodal model. No longer using MERGE archtecture as\n",
        "    transformer would be better. Also due to mistakes in the paper it is\n",
        "    unclear what some of the parameters are.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BimodalClassifier, self).__init__()\n",
        "\n",
        "        #initiate audio tower\n",
        "        self.audio_tower = VGGish_Audio_Model()\n",
        "\n",
        "        #use transformer for lyrics (using bert base uncased for now, but may change)\n",
        "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        for param in self.lyrics_tower.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define feature sizes from the previous step and from bert\n",
        "        AUDIO_FEATURES_OUT = 64\n",
        "        LYRICS_FEATURES_OUT = 768\n",
        "        COMBINED_FEATURES = AUDIO_FEATURES_OUT + LYRICS_FEATURES_OUT\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=COMBINED_FEATURES, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=100, out_features=2) # 2 Outputs for Valence and Arousal\n",
        "        )\n",
        "\n",
        "    def forward(self, x_audio, input_ids, attention_mask):\n",
        "        #process audio input\n",
        "        audio_features = self.audio_tower(x_audio)\n",
        "\n",
        "        #get lyric features\n",
        "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        #use the embedding of the [CLS] token as the feature vector for whole lyrics\n",
        "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        #combine the features from both towers\n",
        "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
        "\n",
        "        #pass the combined features to the final classifier head\n",
        "        output = self.classifier_head(combined_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "r1llg53WvWDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(self, x_audio, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    Extract audio and lyrics features separately (before fusion).\n",
        "    Returns: (audio_features, lyrics_features, predictions)\n",
        "    \"\"\"\n",
        "\n",
        "    # Process audio input\n",
        "    audio_features = self.audio_tower(x_audio)  # [batch_size, 64]\n",
        "\n",
        "    # Get lyric features\n",
        "    lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
        "\n",
        "    # Combine features and get predictions\n",
        "    combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
        "    predictions = self.classifier_head(combined_features)\n",
        "\n",
        "    return audio_features, lyrics_features, predictions"
      ],
      "metadata": {
        "id": "hGyfADadkeTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data loading and prep\n",
        "\n",
        "#get the paths to dissertation folder and new folder on colab\n",
        "print(\"Starting data transfer from Google Drive to local Colab storage...\")\n",
        "\n",
        "#get paths for old file location and new colab one\n",
        "gdrive_zip_path = '/content/drive/MyDrive/dissertation/merge_dataset_zipped.zip'\n",
        "local_storage_path = '/content/local_dissertation_data/'\n",
        "local_zip_path = os.path.join(local_storage_path, 'merge_dataset_zipped.zip')\n",
        "os.makedirs(local_storage_path, exist_ok=True) # Ensure the destination directory exists\n",
        "\n",
        "#Copy zip file from Drive to Colab\n",
        "print(\"Copying single archive file from Google Drive...\")\n",
        "!rsync -ah --progress \"{gdrive_zip_path}\" \"{local_storage_path}\"\n",
        "\n",
        "#get total number of files for progress\n",
        "total_files = int(subprocess.check_output(f\"zipinfo -1 {local_zip_path} | wc -l\", shell=True))\n",
        "\n",
        "#unzip the file\n",
        "print(\"Extracting files locally\")\n",
        "!unzip -o \"{local_zip_path}\" -d \"{local_storage_path}\" | tqdm --unit=files --total={total_files} > /dev/null\n",
        "\n",
        "print(\"Data transfer and extraction complete.\")\n",
        "\n",
        "#load master data from new location\n",
        "local_output_path = os.path.join(local_storage_path, 'merge_dataset/output_from_code/')\n",
        "master_file_path = os.path.join(local_output_path, 'master_processed_file_list.csv')\n",
        "master_df = pd.read_csv(master_file_path)\n",
        "\n",
        "#checking the valence and arousal range in the dataset\n",
        "print(f\"\\nValence range in data: [{master_df['valence'].min()}, {master_df['valence'].max()}]\")\n",
        "print(f\"Arousal range in data: [{master_df['arousal'].min()}, {master_df['arousal'].max()}]\")\n",
        "print(f\"Valence mean: {master_df['valence'].mean():.4f}, std: {master_df['valence'].std():.4f}\")\n",
        "print(f\"Arousal mean: {master_df['arousal'].mean():.4f}, std: {master_df['arousal'].std():.4f}\")\n",
        "print(f\"Total samples in master_df: {len(master_df)}\")\n",
        "\n",
        "# Verify its the right column - not quadrants\n",
        "print(f\"\\nNumber of unique valence values: {master_df['valence'].nunique()}\")\n",
        "print(f\"Number of unique arousal values: {master_df['arousal'].nunique()}\")\n",
        "print(f\"Number of unique quadrant values: {master_df['quadrant'].nunique()}\")\n",
        "\n",
        "# Sample some actual values\n",
        "print(f\"\\nSample valence values: {master_df['valence'].sample(10).values}\")\n",
        "print(f\"Sample arousal values: {master_df['arousal'].sample(10).values}\")\n",
        "\n",
        "#update the paths in the csv\n",
        "print(\"\\nUpdating dataframe paths to use fast local storage...\")\n",
        "gdrive_output_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
        "master_df['spectrogram_path'] = master_df['spectrogram_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
        "master_df['lyrics_path'] = master_df['lyrics_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
        "print(\"Dataframe paths updated.\")\n",
        "\n",
        "#load the data splits from the new path in the predefined splits folder tvt\n",
        "local_split_folder_path = os.path.join(local_storage_path, 'merge_dataset/MERGE_Bimodal_Complete/tvt_dataframes/tvt_70_15_15/')\n",
        "train_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_train_bimodal_complete.csv'))\n",
        "val_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_validate_bimodal_complete.csv'))\n",
        "test_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_test_bimodal_complete.csv'))\n",
        "print(\"\\nSplit files loaded from local storage.\")\n",
        "\n",
        "#merge the files\n",
        "id_column_name = 'song_id'\n",
        "train_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "val_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "test_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "\n",
        "train_df = pd.merge(master_df, train_split_df, on=id_column_name)\n",
        "val_df = pd.merge(master_df, val_split_df, on=id_column_name)\n",
        "test_df = pd.merge(master_df, test_split_df, on=id_column_name)\n",
        "\n",
        "#checking no files are lost in merging - and checking length of the dataframes.\n",
        "print(\"\\nchecking data\")\n",
        "\n",
        "#check no data lost in merge\n",
        "if len(train_df) == len(train_split_df):\n",
        "    print(\"\\nTraining split: Merge successful. All songs accounted for.\")\n",
        "else:\n",
        "    print(f\"\\nWARNING: Training split lost {len(train_split_df) - len(train_df)} songs during merge.\")\n",
        "\n",
        "if len(val_df) == len(val_split_df):\n",
        "    print(\"Validation split: Merge successful. All songs accounted for.\")\n",
        "else:\n",
        "    print(f\"WARNING: Validation split lost {len(val_split_df) - len(val_df)} songs during merge.\")\n",
        "\n",
        "if len(test_df) == len(test_split_df):\n",
        "    print(\"Test split: Merge successful. All songs accounted for.\")\n",
        "else:\n",
        "    print(f\"WARNING: Test split lost {len(test_split_df) - len(test_df)} songs during merge.\")\n",
        "\n",
        "#check length\n",
        "expected_train_len = 1552\n",
        "expected_val_len = 332\n",
        "expected_test_len = 332\n",
        "\n",
        "assert len(train_df) == expected_train_len, f\"Expected {expected_train_len} training samples, but found {len(train_df)}\"\n",
        "assert len(val_df) == expected_val_len, f\"Expected {expected_val_len} validation samples, but found {len(val_df)}\"\n",
        "assert len(test_df) == expected_test_len, f\"Expected {expected_test_len} test samples, but found {len(test_df)}\"\n",
        "\n",
        "print(f\"\\nFinal dataset lengths are correct: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
        "print(\"Data Check Complete\")\n",
        "\n",
        "#createthe datasets and loaders\n",
        "train_dataset = MER_Dataset(annotations_df=train_df, tokenizer=tokenizer)\n",
        "val_dataset = MER_Dataset(annotations_df=val_df, tokenizer=tokenizer)\n",
        "test_dataset = MER_Dataset(annotations_df=test_df, tokenizer=tokenizer)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\nDataLoaders created successfully.\")"
      ],
      "metadata": {
        "id": "KUbPRcEUHrTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#select dataset for similarity analysis\n",
        "\n",
        "analysis_df = test_df.copy()  #can change to train_df or val_df\n",
        "\n",
        "print(f\"\\nâœ“ Selected dataset for similarity analysis: TEST SET\")\n",
        "print(f\"  Total songs to analyze: {len(analysis_df)}\")\n",
        "print(f\"  Song IDs: {analysis_df[id_column_name].head(10).tolist()}...\")"
      ],
      "metadata": {
        "id": "6ngnjHAHktgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a CUDA-enabled GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Using CUDA device.\")\n",
        "else:\n",
        "    # If no GPU is found, print an error and stop execution by raising an error.\n",
        "    raise RuntimeError(\"Error: No GPU found. This script requires a GPU to run.\")\n"
      ],
      "metadata": {
        "id": "sKtbxvlbtVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BimodalClassifier()\n",
        "model.to(device)\n",
        "#load model 4\n",
        "model_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "\n",
        "# Add the method wfor getting features\n",
        "model.get_features = types.MethodType(get_features, model)\n",
        "\n",
        "print(\"Feature extraction added to model.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x2r9lbd-SH7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_dataset(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Extract audio and lyrics features for all songs in the dataloader.\n",
        "    \"\"\"\n",
        "\n",
        "    #Create lists to store results\n",
        "    audio_features_list = []\n",
        "    lyrics_features_list = []\n",
        "    predictions_list = []\n",
        "    ground_truth_list = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Extract features without computing gradients\n",
        "    with torch.no_grad():\n",
        "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            # Move data to device\n",
        "            spectrogram_batch = spectrogram_batch.to(device)\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_mask_batch = attention_mask_batch.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            audio_feat, lyrics_feat, preds = model.get_features(\n",
        "                spectrogram_batch,\n",
        "                input_ids_batch,\n",
        "                attention_mask_batch\n",
        "            )\n",
        "\n",
        "            # Move to CPU and convert to numpy\n",
        "            audio_features_list.append(audio_feat.cpu().numpy())\n",
        "            lyrics_features_list.append(lyrics_feat.cpu().numpy())\n",
        "            predictions_list.append(preds.cpu().numpy())\n",
        "            ground_truth_list.append(labels_batch.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    audio_features = np.concatenate(audio_features_list, axis=0)      # [N, 64]\n",
        "    lyrics_features = np.concatenate(lyrics_features_list, axis=0)    # [N, 768]\n",
        "    predictions = np.concatenate(predictions_list, axis=0)            # [N, 2]\n",
        "    ground_truth = np.concatenate(ground_truth_list, axis=0)          # [N, 2]\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nâœ“ Feature extraction complete!\")\n",
        "    print(f\"  Total songs processed: {len(audio_features)}\")\n",
        "    print(f\"  Audio features shape:  {audio_features.shape}\")\n",
        "    print(f\"  Lyrics features shape: {lyrics_features.shape}\")\n",
        "    print(f\"  Predictions shape:     {predictions.shape}\")\n",
        "    print(f\"  Ground truth shape:    {ground_truth.shape}\")\n",
        "\n",
        "    return {\n",
        "        'audio_features': audio_features,\n",
        "        'lyrics_features': lyrics_features,\n",
        "        'predictions': predictions,\n",
        "        'ground_truth': ground_truth\n",
        "    }\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "BAcQ0UJ_mAHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_dict = extract_features_from_dataset(model, test_loader, device)\n",
        "\n",
        "# Store variable names that match MODEL 4\n",
        "audio_features = features_dict['audio_features']\n",
        "lyrics_features = features_dict['lyrics_features']\n",
        "predictions = features_dict['predictions']\n",
        "ground_truth = features_dict['ground_truth']\n",
        "\n",
        "print(\"\\nâœ“ Features stored in variables:\")\n",
        "print(\"  - audio_features\")\n",
        "print(\"  - lyrics_features\")\n",
        "print(\"  - predictions\")\n",
        "print(\"  - ground_truth\")"
      ],
      "metadata": {
        "id": "kE2--cCsmaTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity_analysis(audio_features, lyrics_features):\n",
        "    \"\"\"\n",
        "    Calculate pairwise cosine similarities within each modality.\n",
        "    \"\"\"\n",
        "\n",
        "    # Audio-to-audio similarity [N, N]\n",
        "    audio_sim = cosine_similarity(audio_features, audio_features)\n",
        "\n",
        "    # Lyrics-to-lyrics similarity [N, N]\n",
        "    lyrics_sim = cosine_similarity(lyrics_features, lyrics_features)\n",
        "\n",
        "    # Create mask for off-diagonal elements\n",
        "    mask = np.ones_like(audio_sim, dtype=bool)\n",
        "    np.fill_diagonal(mask, False)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"WITHIN-MODALITY COSINE SIMILARITY ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\n1. AUDIO-TO-AUDIO SIMILARITY:\")\n",
        "    print(f\"   Mean (off-diagonal):  {audio_sim[mask].mean():.4f}\")\n",
        "    print(f\"   Std (off-diagonal):   {audio_sim[mask].std():.4f}\")\n",
        "    print(f\"   Diagonal (self):      {np.diag(audio_sim).mean():.4f}\")\n",
        "\n",
        "    print(f\"\\n2. LYRICS-TO-LYRICS SIMILARITY:\")\n",
        "    print(f\"   Mean (off-diagonal):  {lyrics_sim[mask].mean():.4f}\")\n",
        "    print(f\"   Std (off-diagonal):   {lyrics_sim[mask].std():.4f}\")\n",
        "    print(f\"   Diagonal (self):      {np.diag(lyrics_sim).mean():.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\n3. INTERPRETATION:\")\n",
        "    audio_distinctiveness = 1 - audio_sim[mask].mean()\n",
        "    lyrics_distinctiveness = 1 - lyrics_sim[mask].mean()\n",
        "\n",
        "    print(f\"   Audio distinctiveness:  {audio_distinctiveness:.4f}\")\n",
        "    print(f\"   Lyrics distinctiveness: {lyrics_distinctiveness:.4f}\")\n",
        "    print(f\"\\n   Higher distinctiveness = songs are more unique from each other\")\n",
        "\n",
        "    print(f\"\\n4. CROSS-MODAL SIMILARITY:\")\n",
        "    print(f\"   âš  Cannot compute directly due to different dimensions (audio=64, lyrics=768)\")\n",
        "    print(f\"   â†’ Use CCA analysis (Cell 18) for cross-modal correlation\")\n",
        "    print(f\"   â†’ Use Retrieval analysis (Cell 21) for cross-modal matching\")\n",
        "\n",
        "    return audio_sim, lyrics_sim\n",
        "\n",
        "# Run analysis\n",
        "audio_sim, lyrics_sim = compute_cosine_similarity_analysis(\n",
        "    audio_features,\n",
        "    lyrics_features\n",
        ")\n",
        "\n",
        "# Create placeholder for cross_modal_sim and self_sim for visualization compatibility\n",
        "# These will be computed properly via CCA and retrieval\n",
        "cross_modal_sim = None\n",
        "self_sim = None"
      ],
      "metadata": {
        "id": "F9TbUxII2umU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot audio similarity\n",
        "im1 = axes[0].imshow(audio_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
        "axes[0].set_title('Audio-to-Audio Similarity', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Song Index')\n",
        "axes[0].set_ylabel('Song Index')\n",
        "plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
        "\n",
        "# Plot lyrics similarity\n",
        "im2 = axes[1].imshow(lyrics_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
        "axes[1].set_title('Lyrics-to-Lyrics Similarity', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Song Index')\n",
        "axes[1].set_ylabel('Song Index')\n",
        "plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NOTE: Cross-modal similarity visualization skipped\")\n",
        "print(\"Cross-modal relationships are analyzed via:\")\n",
        "print(\"  â€¢ CCA (Canonical Correlation Analysis) - see Cell 18\")\n",
        "print(\"  â€¢ Retrieval Analysis - see Cell 21\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "KRg5DJ6Mmsvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CCA"
      ],
      "metadata": {
        "id": "tR5tUaoQmzKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_cca_analysis(audio_features, lyrics_features, n_components=10):\n",
        "    \"\"\"\n",
        "    Canonical Correlation Analysis between audio and lyrics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise CCA\n",
        "    cca = CCA(n_components=n_components, max_iter=1000)\n",
        "\n",
        "    # Fit CCA to learn transformations\n",
        "    cca.fit(audio_features, lyrics_features)\n",
        "\n",
        "    # Transform features to canonical space\n",
        "    audio_canonical, lyrics_canonical = cca.transform(audio_features, lyrics_features)\n",
        "\n",
        "    # Compute correlation for each canonical component\n",
        "    correlations = []\n",
        "    for i in range(n_components):\n",
        "        corr, _ = pearsonr(audio_canonical[:, i], lyrics_canonical[:, i])\n",
        "        correlations.append(corr)\n",
        "\n",
        "    correlations = np.array(correlations)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nCanonical correlations (n={n_components}):\")\n",
        "    for i, corr in enumerate(correlations):\n",
        "        print(f\"  Component {i+1}: {corr:.4f}\")\n",
        "\n",
        "    print(f\"\\nSummary statistics:\")\n",
        "    print(f\"  Mean correlation: {correlations.mean():.4f}\")\n",
        "    print(f\"  Max correlation:  {correlations.max():.4f}\")\n",
        "    print(f\"  Std:              {correlations.std():.4f}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\nINTERPRETATION:\")\n",
        "    if correlations[0] > 0.7:\n",
        "        print(f\"  STRONG shared structure: First component correlation = {correlations[0]:.3f}\")\n",
        "    elif correlations[0] > 0.5:\n",
        "        print(f\"  MODERATE shared structure: Some shared latent dimensions\")\n",
        "    else:\n",
        "        print(f\"  LIMITED shared structure: Modalities may be complementary\")\n",
        "\n",
        "    return cca, correlations, audio_canonical, lyrics_canonical\n",
        "\n",
        "# Run CCA\n",
        "cca_model, cca_corrs, audio_can, lyrics_can = perform_cca_analysis(\n",
        "    audio_features,\n",
        "    lyrics_features,\n",
        "    n_components=10\n",
        ")"
      ],
      "metadata": {
        "id": "kH1EKlB0m1XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Bar chart of canonical correlations\n",
        "axes[0].bar(range(1, len(cca_corrs) + 1), cca_corrs, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "axes[0].axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Moderate (0.5)')\n",
        "axes[0].axhline(y=0.7, color='darkred', linestyle='--', linewidth=2, label='Strong (0.7)')\n",
        "axes[0].set_xlabel('Canonical Component', fontsize=12)\n",
        "axes[0].set_ylabel('Correlation Coefficient', fontsize=12)\n",
        "axes[0].set_title('Canonical Correlations', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Scatter plot of first canonical component\n",
        "axes[1].scatter(audio_can[:, 0], lyrics_can[:, 0], alpha=0.6, s=50, color='purple', edgecolors='black', linewidth=0.5)\n",
        "axes[1].set_xlabel('Audio Canonical Component 1', fontsize=12)\n",
        "axes[1].set_ylabel('Lyrics Canonical Component 1', fontsize=12)\n",
        "axes[1].set_title(f'First Canonical Component (r={cca_corrs[0]:.3f})', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Add correlation line\n",
        "z = np.polyfit(audio_can[:, 0], lyrics_can[:, 0], 1)\n",
        "p = np.poly1d(z)\n",
        "axes[1].plot(audio_can[:, 0], p(audio_can[:, 0]), \"r--\", linewidth=2, label='Linear fit')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "n06LLipknFei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CMR"
      ],
      "metadata": {
        "id": "TDWy3SkbnIac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_modal_retrieval_analysis(audio_features, lyrics_features, predictions, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform cross-modal retrieval task using predictions as the common space.\n",
        "    Since audio (64D) and lyrics (768D) have different dimensions, we use the\n",
        "    model's learned prediction space (valence/arousal) as a proxy for cross-modal similarity.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CROSS-MODAL RETRIEVAL ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nUsing predictions (valence/arousal) as common space for retrieval\")\n",
        "    print(f\"This represents what the model learned about audio-lyrics alignment\\n\")\n",
        "\n",
        "    # Use predictions as the common space (both modalities map to same valence/arousal)\n",
        "    # Compute similarity in prediction space\n",
        "    sim_matrix = cosine_similarity(predictions, predictions)\n",
        "    n_samples = len(predictions)\n",
        "\n",
        "    # Audio â†’ Lyrics retrieval\n",
        "    # For each song's audio, find most similar lyrics based on predicted valence/arousal\n",
        "    audio_to_lyrics_top_k = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :top_k]\n",
        "\n",
        "    # Check if correct match is in top-k\n",
        "    audio_to_lyrics_hits = []\n",
        "    for i in range(n_samples):\n",
        "        if i in audio_to_lyrics_top_k[i]:\n",
        "            audio_to_lyrics_hits.append(1)\n",
        "        else:\n",
        "            audio_to_lyrics_hits.append(0)\n",
        "\n",
        "    audio_to_lyrics_acc = np.mean(audio_to_lyrics_hits)\n",
        "\n",
        "    # Since we're using the same prediction space for both, audioâ†’lyrics and lyricsâ†’audio\n",
        "    # will give the same results (symmetric)\n",
        "    lyrics_to_audio_acc = audio_to_lyrics_acc\n",
        "\n",
        "    # Top-1 (exact match)\n",
        "    audio_to_lyrics_top1 = np.argmax(sim_matrix, axis=1)\n",
        "    top1_acc = np.mean(audio_to_lyrics_top1 == np.arange(n_samples))\n",
        "\n",
        "    # Top-10\n",
        "    if n_samples >= 10:\n",
        "        audio_to_lyrics_top_10 = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :10]\n",
        "        top10_hits = [i in audio_to_lyrics_top_10[i] for i in range(n_samples)]\n",
        "        top10_acc = np.mean(top10_hits)\n",
        "    else:\n",
        "        top10_acc = None\n",
        "\n",
        "    # Print results\n",
        "    print(f\"1. RETRIEVAL ACCURACY (via prediction space):\")\n",
        "    print(f\"   Top-{top_k} accuracy:  {audio_to_lyrics_acc:.2%}\")\n",
        "    print(f\"   Top-1 accuracy:        {top1_acc:.2%}\")\n",
        "    if top10_acc:\n",
        "        print(f\"   Top-10 accuracy:       {top10_acc:.2%}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\n2. INTERPRETATION:\")\n",
        "    print(f\"   Baseline (random): {1/n_samples:.2%}\")\n",
        "    if audio_to_lyrics_acc > 0.5:\n",
        "        print(f\"   âœ“ STRONG: Model learned good alignment between modalities\")\n",
        "    elif audio_to_lyrics_acc > 0.2:\n",
        "        print(f\"   âœ“ MODERATE: Some learned alignment\")\n",
        "    else:\n",
        "        print(f\"   ! WEAK: Limited cross-modal predictability\")\n",
        "\n",
        "    print(f\"\\n3. MEANING:\")\n",
        "    print(f\"   {audio_to_lyrics_acc:.1%} of songs can be matched to themselves\")\n",
        "    print(f\"   in the top-{top_k} when comparing via learned predictions\")\n",
        "\n",
        "    print(f\"\\n4. NOTE:\")\n",
        "    print(f\"   This measures how well the model maps both modalities\")\n",
        "    print(f\"   to the same emotional space (valence/arousal)\")\n",
        "\n",
        "    return {\n",
        "        'audio_to_lyrics_acc': audio_to_lyrics_acc,\n",
        "        'lyrics_to_audio_acc': lyrics_to_audio_acc,\n",
        "        'top1_acc': top1_acc,\n",
        "        'top10_acc': top10_acc,\n",
        "        'similarity_matrix': sim_matrix\n",
        "    }\n",
        "\n",
        "# Run retrieval analysis\n",
        "retrieval_results = cross_modal_retrieval_analysis(\n",
        "    audio_features,\n",
        "    lyrics_features,\n",
        "    predictions,  # Add predictions as parameter\n",
        "    top_k=5\n",
        ")"
      ],
      "metadata": {
        "id": "Mr1Lv-d7nJ2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_modal_retrieval_via_cca(audio_canonical, lyrics_canonical, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform cross-modal retrieval using CCA-transformed features.\n",
        "    Both modalities are now in the same canonical space, so direct comparison is valid.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CROSS-MODAL RETRIEVAL ANALYSIS (via CCA)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Compute cross-modal similarity in canonical space\n",
        "    sim_matrix = cosine_similarity(audio_canonical, lyrics_canonical)\n",
        "    n_samples = len(audio_canonical)\n",
        "\n",
        "    # Audio â†’ Lyrics retrieval\n",
        "    audio_to_lyrics_top_k = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :top_k]\n",
        "\n",
        "    audio_to_lyrics_hits = []\n",
        "    for i in range(n_samples):\n",
        "        if i in audio_to_lyrics_top_k[i]:\n",
        "            audio_to_lyrics_hits.append(1)\n",
        "        else:\n",
        "            audio_to_lyrics_hits.append(0)\n",
        "\n",
        "    audio_to_lyrics_acc = np.mean(audio_to_lyrics_hits)\n",
        "\n",
        "    # Lyrics â†’ Audio retrieval\n",
        "    lyrics_to_audio_top_k = np.argsort(sim_matrix.T, axis=1)[:, ::-1][:, :top_k]\n",
        "\n",
        "    lyrics_to_audio_hits = []\n",
        "    for i in range(n_samples):\n",
        "        if i in lyrics_to_audio_top_k[i]:\n",
        "            lyrics_to_audio_hits.append(1)\n",
        "        else:\n",
        "            lyrics_to_audio_hits.append(0)\n",
        "\n",
        "    lyrics_to_audio_acc = np.mean(lyrics_to_audio_hits)\n",
        "\n",
        "    # Top-1 (exact match)\n",
        "    audio_to_lyrics_top1 = np.argmax(sim_matrix, axis=1)\n",
        "    top1_acc = np.mean(audio_to_lyrics_top1 == np.arange(n_samples))\n",
        "\n",
        "    # Top-10\n",
        "    if n_samples >= 10:\n",
        "        audio_to_lyrics_top_10 = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :10]\n",
        "        top10_hits = [i in audio_to_lyrics_top_10[i] for i in range(n_samples)]\n",
        "        top10_acc = np.mean(top10_hits)\n",
        "    else:\n",
        "        top10_acc = None\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n1. RETRIEVAL ACCURACY:\")\n",
        "    print(f\"   Audio â†’ Lyrics (Top-{top_k}): {audio_to_lyrics_acc:.2%}\")\n",
        "    print(f\"   Lyrics â†’ Audio (Top-{top_k}): {lyrics_to_audio_acc:.2%}\")\n",
        "\n",
        "    print(f\"\\n2. ADDITIONAL METRICS:\")\n",
        "    print(f\"   Top-1 accuracy (exact match):  {top1_acc:.2%}\")\n",
        "    if top10_acc:\n",
        "        print(f\"   Top-10 accuracy:               {top10_acc:.2%}\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\n3. INTERPRETATION:\")\n",
        "    if audio_to_lyrics_acc > 0.5:\n",
        "        print(f\"   âœ“ GOOD alignment: CCA found strong cross-modal structure\")\n",
        "    elif audio_to_lyrics_acc > 0.2:\n",
        "        print(f\"   âœ“ MODERATE alignment: Some shared structure\")\n",
        "    else:\n",
        "        print(f\"   ! WEAK alignment: Limited cross-modal structure in CCA space\")\n",
        "\n",
        "    print(f\"\\n   Meaning: {audio_to_lyrics_acc:.1%} of the time, given audio features,\")\n",
        "    print(f\"   the correct lyrics are in the top-{top_k} most similar in CCA space.\")\n",
        "\n",
        "    return {\n",
        "        'audio_to_lyrics_acc': audio_to_lyrics_acc,\n",
        "        'lyrics_to_audio_acc': lyrics_to_audio_acc,\n",
        "        'top1_acc': top1_acc,\n",
        "        'top10_acc': top10_acc,\n",
        "        'similarity_matrix': sim_matrix\n",
        "    }\n",
        "\n",
        "# Run CCA-based retrieval analysis\n",
        "retrieval_results_cca = cross_modal_retrieval_via_cca(\n",
        "    audio_can,\n",
        "    lyrics_can,\n",
        "    top_k=5\n",
        ")"
      ],
      "metadata": {
        "id": "vKy5w_0X3hF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval accuracy using CCA canonical space\n",
        "k_values = [1, 2, 3, 5, 10, 20]\n",
        "accuracies_cca = []\n",
        "\n",
        "# Use CCA-transformed features for true cross-modal retrieval\n",
        "sim_matrix_cca = cosine_similarity(audio_can, lyrics_can)\n",
        "n_samples = len(audio_can)\n",
        "\n",
        "# Compute accuracy for different k values\n",
        "for k in k_values:\n",
        "    if k <= n_samples:\n",
        "        top_k_indices = np.argsort(sim_matrix_cca, axis=1)[:, ::-1][:, :k]\n",
        "        hits = [i in top_k_indices[i] for i in range(n_samples)]\n",
        "        accuracies_cca.append(np.mean(hits))\n",
        "    else:\n",
        "        accuracies_cca.append(None)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "valid_k = [k for k, acc in zip(k_values, accuracies) if acc is not None]\n",
        "valid_acc_pred = [acc for acc in accuracies if acc is not None]\n",
        "valid_acc_cca = [acc for acc in accuracies_cca if acc is not None]\n",
        "\n",
        "plt.plot(valid_k, valid_acc_pred, marker='o', linewidth=2, markersize=8,\n",
        "         color='steelblue', label='Prediction Space')\n",
        "plt.plot(valid_k, valid_acc_cca, marker='s', linewidth=2, markersize=8,\n",
        "         color='forestgreen', label='CCA Space')\n",
        "\n",
        "plt.xlabel('Top-K', fontsize=12)\n",
        "plt.ylabel('Retrieval Accuracy', fontsize=12)\n",
        "plt.title('Cross-Modal Retrieval Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim([0, 1])\n",
        "\n",
        "# Add value labels\n",
        "for k, acc_pred, acc_cca in zip(valid_k, valid_acc_pred, valid_acc_cca):\n",
        "    plt.text(k, acc_pred + 0.03, f'{acc_pred:.1%}', ha='center', fontsize=9, color='steelblue')\n",
        "    plt.text(k, acc_cca - 0.05, f'{acc_cca:.1%}', ha='center', fontsize=9, color='forestgreen')\n",
        "\n",
        "# Add baseline\n",
        "plt.axhline(y=1/n_samples, color='red', linestyle='--', linewidth=1,\n",
        "            label=f'Random baseline ({1/n_samples:.2%})')\n",
        "plt.legend(fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š COMPARISON:\")\n",
        "print(f\"  â€¢ Prediction Space: Measures task-based alignment (valence/arousal)\")\n",
        "print(f\"  â€¢ CCA Space: Measures statistical correlation between modalities\")\n",
        "print(f\"  â€¢ Both are valid metrics for different aspects of cross-modal learning\")"
      ],
      "metadata": {
        "id": "B9EiJcplnb67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory\n",
        "output_dir = '/content/drive/MyDrive/dissertation/similarity_analysis_results/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save similarity matrices\n",
        "np.save(os.path.join(output_dir, 'audio_similarity_matrix.npy'), audio_sim)\n",
        "np.save(os.path.join(output_dir, 'lyrics_similarity_matrix.npy'), lyrics_sim)\n",
        "np.save(os.path.join(output_dir, 'cross_modal_similarity_matrix.npy'), cross_modal_sim)\n",
        "\n",
        "# Save CCA results\n",
        "np.save(os.path.join(output_dir, 'cca_correlations.npy'), cca_corrs)\n",
        "np.save(os.path.join(output_dir, 'audio_canonical.npy'), audio_can)\n",
        "np.save(os.path.join(output_dir, 'lyrics_canonical.npy'), lyrics_can)\n",
        "\n",
        "# Save extracted features\n",
        "np.save(os.path.join(output_dir, 'audio_features.npy'), audio_features)\n",
        "np.save(os.path.join(output_dir, 'lyrics_features.npy'), lyrics_features)\n",
        "\n",
        "# Create summary CSV with per-song similarity scores\n",
        "results_df = analysis_df[[id_column_name, 'valence', 'arousal']].copy()\n",
        "results_df['self_similarity'] = self_sim\n",
        "results_df['valence_predicted'] = predictions[:, 0]\n",
        "results_df['arousal_predicted'] = predictions[:, 1]\n",
        "results_df.to_csv(os.path.join(output_dir, 'similarity_summary.csv'), index=False)\n",
        "\n",
        "# Save metrics summary as JSON\n",
        "metrics = {\n",
        "    'dataset': 'test_set',\n",
        "    'n_songs': len(analysis_df),\n",
        "    'mean_self_similarity': float(self_sim.mean()),\n",
        "    'std_self_similarity': float(self_sim.std()),\n",
        "    'cca_correlation_1': float(cca_corrs[0]),\n",
        "    'cca_mean_correlation': float(cca_corrs.mean()),\n",
        "    'retrieval_audio_to_lyrics': float(retrieval_results['audio_to_lyrics_acc']),\n",
        "    'retrieval_lyrics_to_audio': float(retrieval_results['lyrics_to_audio_acc']),\n",
        "    'retrieval_top1': float(retrieval_results['top1_acc'])\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'metrics_summary.json'), 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"âœ“ Results saved to: {output_dir}\")\n",
        "print(f\"\\nFiles created:\")\n",
        "print(f\"  - audio_similarity_matrix.npy\")\n",
        "print(f\"  - lyrics_similarity_matrix.npy\")\n",
        "print(f\"  - cross_modal_similarity_matrix.npy\")\n",
        "print(f\"  - cca_correlations.npy\")\n",
        "print(f\"  - audio_canonical.npy\")\n",
        "print(f\"  - lyrics_canonical.npy\")\n",
        "print(f\"  - audio_features.npy\")\n",
        "print(f\"  - lyrics_features.npy\")\n",
        "print(f\"  - similarity_summary.csv\")\n",
        "print(f\"  - metrics_summary.json\")\n",
        ""
      ],
      "metadata": {
        "id": "t3MBXJ9Cnrub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}