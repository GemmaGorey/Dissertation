{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMvtXMWFd0d2i5C27w06zvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Similarity_Analysis_Audio_vsa_Lyrics_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB327CIPG0v4"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# install mamba to use instead of pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the config file and build the environment.\n",
        "yaml_content = \"\"\"\n",
        "name: dissertation\n",
        "channels:\n",
        "  - pytorch\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - pytorch=2.2.2\n",
        "  - torchvision=0.17.2\n",
        "  - torchaudio\n",
        "  - librosa\n",
        "  - numpy<2\n",
        "  - pandas\n",
        "  - jupyter\n",
        "  - wandb\n",
        "\"\"\"\n",
        "\n",
        "# Write the string content to a file -  'environment.yml'.\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"environment.yml file created successfully.\")\n",
        "\n",
        "# create the environment using mamba from the yml file.\n",
        "print(\"\\n Creating environment\")\n",
        "\n",
        "!mamba env create -f environment.yml --quiet && echo -e \"\\n 'dissertation' environment is ready to use.\""
      ],
      "metadata": {
        "id": "NqkjmK3RHcYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and setting up of GitHub and W&B\n",
        "\n",
        "# clone project repository from GitHub\n",
        "print(\"⏳ Cloning GitHub repository...\")\n",
        "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
        "print(\"Repository cloned.\")\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#imports\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import subprocess\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #loading the tokenizer for lyrics processing\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cross_decomposition import CCA\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import types\n",
        "import json\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ],
      "metadata": {
        "id": "tG0a7AkQHf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MER_Dataset(Dataset):\n",
        "    \"\"\" Custom PyTorch Dataset for loading MER data. \"\"\"\n",
        "    def __init__(self, annotations_df, tokenizer):\n",
        "        \"\"\" Creation of the Dataset from the dataframe (predefined splits in MERGE dataset) \"\"\"\n",
        "        self.annotations = annotations_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Function to return the total number of songs in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Function to get a song from the dataset.\n",
        "        \"\"\"\n",
        "        song_info = self.annotations.iloc[index] #which song ID/row is picked from the dataset as per the index\n",
        "\n",
        "        spectrogram_path = song_info['spectrogram_path'] # columns from the df\n",
        "        lyrics_path = song_info['lyrics_path'] # columns from the df\n",
        "        valence = song_info['valence'] # columns from the df\n",
        "        arousal = song_info['arousal'] # columns from the df\n",
        "\n",
        "        #change spectorgram into a tensor\n",
        "        spectrogram = np.load(spectrogram_path) #loading spectorgram from path saved in df\n",
        "        spectrogram_tensor = torch.from_numpy(spectrogram).float() # changing the np array to tensor\n",
        "        spectrogram_tensor = spectrogram_tensor.unsqueeze(0) #Adding a \"channel\" dimension for CNN\n",
        "\n",
        "        #Load the lyric tokens\n",
        "        encoded_lyrics = torch.load(lyrics_path, weights_only=False)\n",
        "        input_ids = encoded_lyrics['input_ids'].squeeze(0) #remove the batch dimension from input ids so 1d array\n",
        "        attention_mask = encoded_lyrics['attention_mask'].squeeze(0) #remove the batch dimension from attention mask so 1d\n",
        "\n",
        "        labels = torch.tensor([valence, arousal], dtype=torch.float32) # extract labels\n",
        "\n",
        "        return spectrogram_tensor, input_ids, attention_mask, labels"
      ],
      "metadata": {
        "id": "vmJVKE2BAbTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModule(nn.Module): #Addition from V1\n",
        "    def __init__(self, feature_dim):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        '''\n",
        "        Attention mechanism to weight the importance of different features\n",
        "        '''\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim // 4),  # input is 64 will map to16\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feature_dim // 4, feature_dim),  #reverts back to 64 from 16\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, 64]\n",
        "        attention_weights = self.attention(x)  # [batch_size, 64]\n",
        "        weighted_features = x * attention_weights  # Element-wise multiplication\n",
        "        return weighted_features"
      ],
      "metadata": {
        "id": "uQy6xyLPJxAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGish_Audio_Model(nn.Module):\n",
        "    '''As previous vERSION but adding in the following\n",
        "      - Batch normalisation\n",
        "      - Attention mechanism\n",
        "      - Learning rate scheduling\n",
        "      - early stopping'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGish_Audio_Model, self).__init__()\n",
        "        '''\n",
        "        A VGG-style model for the audio tower for a starting model.\n",
        "        No longer trying to implement the method from MERGE paper as this had mistakes in the paper\n",
        "        V1.1 includes attention to see if this improves performance.\n",
        "        V1.2  implements true VGG-style blocks with multiple convolutions per block.\n",
        "        '''\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1 - 2 convolutions\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2 - 2 convolutions\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3 - 2 convolutions\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4 - 2 convolutions\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.attention = AttentionModule(256) #Add attention here from v2 (model 3)\n",
        "        self.fc2 = nn.Linear(256, 64) # Final feature vector size should be 64 - needs to match input of combined\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # Flatten the features for the classifier\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pM1B4zwQcdnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BimodalClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The final bimodal model. No longer using MERGE archtecture as\n",
        "    transformer would be better. Also due to mistakes in the paper it is\n",
        "    unclear what some of the parameters are.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BimodalClassifier, self).__init__()\n",
        "\n",
        "        #initiate audio tower\n",
        "        self.audio_tower = VGGish_Audio_Model()\n",
        "\n",
        "        #use transformer for lyrics (using bert base uncased for now, but may change)\n",
        "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        for param in self.lyrics_tower.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define feature sizes from the previous step and from bert\n",
        "        AUDIO_FEATURES_OUT = 64\n",
        "        LYRICS_FEATURES_OUT = 768\n",
        "        COMBINED_FEATURES = AUDIO_FEATURES_OUT + LYRICS_FEATURES_OUT\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=COMBINED_FEATURES, out_features=100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=100, out_features=2) # 2 Outputs for Valence and Arousal\n",
        "        )\n",
        "\n",
        "    def forward(self, x_audio, input_ids, attention_mask):\n",
        "        #process audio input\n",
        "        audio_features = self.audio_tower(x_audio)\n",
        "\n",
        "        #get lyric features\n",
        "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        #use the embedding of the [CLS] token as the feature vector for whole lyrics\n",
        "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        #combine the features from both towers\n",
        "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
        "\n",
        "        #pass the combined features to the final classifier head\n",
        "        output = self.classifier_head(combined_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "r1llg53WvWDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(self, x_audio, input_ids, attention_mask):\n",
        "    \"\"\"\n",
        "    Extract audio and lyrics features separately (before fusion).\n",
        "    Returns: (audio_features, lyrics_features, predictions)\n",
        "    \"\"\"\n",
        "\n",
        "    # Process audio input\n",
        "    audio_features = self.audio_tower(x_audio)  # [batch_size, 64]\n",
        "\n",
        "    # Get lyric features\n",
        "    lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
        "\n",
        "    # Combine features and get predictions\n",
        "    combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
        "    predictions = self.classifier_head(combined_features)\n",
        "\n",
        "    return audio_features, lyrics_features, predictions"
      ],
      "metadata": {
        "id": "hGyfADadkeTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data loading and prep\n",
        "\n",
        "#get the paths to dissertation folder and new folder on colab\n",
        "print(\"Starting data transfer from Google Drive to local Colab storage...\")\n",
        "\n",
        "#get paths for old file location and new colab one\n",
        "gdrive_zip_path = '/content/drive/MyDrive/dissertation/merge_dataset_zipped.zip'\n",
        "local_storage_path = '/content/local_dissertation_data/'\n",
        "local_zip_path = os.path.join(local_storage_path, 'merge_dataset_zipped.zip')\n",
        "os.makedirs(local_storage_path, exist_ok=True) # Ensure the destination directory exists\n",
        "\n",
        "#Copy zip file from Drive to Colab\n",
        "print(\"Copying single archive file from Google Drive...\")\n",
        "!rsync -ah --progress \"{gdrive_zip_path}\" \"{local_storage_path}\"\n",
        "\n",
        "#get total number of files for progress\n",
        "total_files = int(subprocess.check_output(f\"zipinfo -1 {local_zip_path} | wc -l\", shell=True))\n",
        "\n",
        "#unzip the file\n",
        "print(\"Extracting files locally\")\n",
        "!unzip -o \"{local_zip_path}\" -d \"{local_storage_path}\" | tqdm --unit=files --total={total_files} > /dev/null\n",
        "\n",
        "print(\"Data transfer and extraction complete.\")\n",
        "\n",
        "#load master data from new location\n",
        "local_output_path = os.path.join(local_storage_path, 'merge_dataset/output_from_code/')\n",
        "master_file_path = os.path.join(local_output_path, 'master_processed_file_list.csv')\n",
        "master_df = pd.read_csv(master_file_path)\n",
        "\n",
        "#checking the valence and arousal range in the dataset\n",
        "print(f\"\\nValence range in data: [{master_df['valence'].min()}, {master_df['valence'].max()}]\")\n",
        "print(f\"Arousal range in data: [{master_df['arousal'].min()}, {master_df['arousal'].max()}]\")\n",
        "print(f\"Valence mean: {master_df['valence'].mean():.4f}, std: {master_df['valence'].std():.4f}\")\n",
        "print(f\"Arousal mean: {master_df['arousal'].mean():.4f}, std: {master_df['arousal'].std():.4f}\")\n",
        "print(f\"Total samples in master_df: {len(master_df)}\")\n",
        "\n",
        "# Verify its the right column - not quadrants\n",
        "print(f\"\\nNumber of unique valence values: {master_df['valence'].nunique()}\")\n",
        "print(f\"Number of unique arousal values: {master_df['arousal'].nunique()}\")\n",
        "print(f\"Number of unique quadrant values: {master_df['quadrant'].nunique()}\")\n",
        "\n",
        "# Sample some actual values\n",
        "print(f\"\\nSample valence values: {master_df['valence'].sample(10).values}\")\n",
        "print(f\"Sample arousal values: {master_df['arousal'].sample(10).values}\")\n",
        "\n",
        "#update the paths in the csv\n",
        "print(\"\\nUpdating dataframe paths to use fast local storage...\")\n",
        "gdrive_output_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
        "master_df['spectrogram_path'] = master_df['spectrogram_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
        "master_df['lyrics_path'] = master_df['lyrics_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
        "print(\"Dataframe paths updated.\")\n",
        "\n",
        "#load the data splits from the new path in the predefined splits folder tvt\n",
        "local_split_folder_path = os.path.join(local_storage_path, 'merge_dataset/MERGE_Bimodal_Complete/tvt_dataframes/tvt_70_15_15/')\n",
        "train_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_train_bimodal_complete.csv'))\n",
        "val_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_validate_bimodal_complete.csv'))\n",
        "test_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_test_bimodal_complete.csv'))\n",
        "print(\"\\nSplit files loaded from local storage.\")\n",
        "\n",
        "#merge the files\n",
        "id_column_name = 'song_id'\n",
        "train_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "val_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "test_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
        "\n",
        "train_df = pd.merge(master_df, train_split_df, on=id_column_name)\n",
        "val_df = pd.merge(master_df, val_split_df, on=id_column_name)\n",
        "test_df = pd.merge(master_df, test_split_df, on=id_column_name)\n",
        "\n",
        "#checking no files are lost in merging - and checking length of the dataframes.\n",
        "print(\"\\nchecking data\")\n",
        "\n",
        "#check no data lost in merge\n",
        "if len(train_df) == len(train_split_df):\n",
        "    print(\"\\nTraining split: Merge successful. All songs accounted for.\")\n",
        "else:\n",
        "    print(f\"\\nWARNING: Training split lost {len(train_split_df) - len(train_df)} songs during merge.\")\n",
        "\n",
        "if len(val_df) == len(val_split_df):\n",
        "    print(\"Validation split: Merge successful. All songs accounted for.\")\n",
        "else:\n",
        "    print(f\"WARNING: Validation split lost {len(val_split_df) - len(val_df)} songs during merge.\")\n",
        "\n",
        "if len(test_df) == len(test_split_df):\n",
        "    print(\"Test split: Merge successful. All songs accounted for.\")\n",
        "else:\n",
        "    print(f\"WARNING: Test split lost {len(test_split_df) - len(test_df)} songs during merge.\")\n",
        "\n",
        "#check length\n",
        "expected_train_len = 1552\n",
        "expected_val_len = 332\n",
        "expected_test_len = 332\n",
        "\n",
        "assert len(train_df) == expected_train_len, f\"Expected {expected_train_len} training samples, but found {len(train_df)}\"\n",
        "assert len(val_df) == expected_val_len, f\"Expected {expected_val_len} validation samples, but found {len(val_df)}\"\n",
        "assert len(test_df) == expected_test_len, f\"Expected {expected_test_len} test samples, but found {len(test_df)}\"\n",
        "\n",
        "print(f\"\\nFinal dataset lengths are correct: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
        "print(\"Data Check Complete\")\n",
        "\n",
        "#createthe datasets and loaders\n",
        "train_dataset = MER_Dataset(annotations_df=train_df, tokenizer=tokenizer)\n",
        "val_dataset = MER_Dataset(annotations_df=val_df, tokenizer=tokenizer)\n",
        "test_dataset = MER_Dataset(annotations_df=test_df, tokenizer=tokenizer)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\nDataLoaders created successfully.\")"
      ],
      "metadata": {
        "id": "KUbPRcEUHrTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select dataset for similarity analysis\n",
        "\n",
        "analysis_df = test_df.copy()  #can change to train_df or val_df\n",
        "\n",
        "print(f\"\\n✓ Selected dataset for similarity analysis: TEST SET\")\n",
        "print(f\"  Total songs to analyze: {len(analysis_df)}\")\n",
        "print(f\"  Song IDs: {analysis_df[id_column_name].head(10).tolist()}...\")"
      ],
      "metadata": {
        "id": "6ngnjHAHktgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a CUDA-enabled GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Using CUDA device.\")\n",
        "else:\n",
        "    # If no GPU is found, print an error and stop execution by raising an error.\n",
        "    raise RuntimeError(\"Error: No GPU found. This script requires a GPU to run.\")\n"
      ],
      "metadata": {
        "id": "sKtbxvlbtVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BimodalClassifier()\n",
        "model.to(device)\n",
        "#load model 4\n",
        "model_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "\n",
        "# Add the method wfor getting features\n",
        "model.get_features = types.MethodType(get_features, model)\n",
        "\n",
        "print(\"Feature extraction added to model.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x2r9lbd-SH7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_dataset(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Extract audio and lyrics features for all songs in the dataloader.\n",
        "    \"\"\"\n",
        "\n",
        "    #Create lists to store results\n",
        "    audio_features_list = []\n",
        "    lyrics_features_list = []\n",
        "    predictions_list = []\n",
        "    ground_truth_list = []\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Extract features without computing gradients\n",
        "    with torch.no_grad():\n",
        "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            # Move data to device\n",
        "            spectrogram_batch = spectrogram_batch.to(device)\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_mask_batch = attention_mask_batch.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            audio_feat, lyrics_feat, preds = model.get_features(\n",
        "                spectrogram_batch,\n",
        "                input_ids_batch,\n",
        "                attention_mask_batch\n",
        "            )\n",
        "\n",
        "            # Move to CPU and convert to numpy\n",
        "            audio_features_list.append(audio_feat.cpu().numpy())\n",
        "            lyrics_features_list.append(lyrics_feat.cpu().numpy())\n",
        "            predictions_list.append(preds.cpu().numpy())\n",
        "            ground_truth_list.append(labels_batch.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    audio_features = np.concatenate(audio_features_list, axis=0)      # [N, 64]\n",
        "    lyrics_features = np.concatenate(lyrics_features_list, axis=0)    # [N, 768]\n",
        "    predictions = np.concatenate(predictions_list, axis=0)            # [N, 2]\n",
        "    ground_truth = np.concatenate(ground_truth_list, axis=0)          # [N, 2]\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n✓ Feature extraction complete!\")\n",
        "    print(f\"  Total songs processed: {len(audio_features)}\")\n",
        "    print(f\"  Audio features shape:  {audio_features.shape}\")\n",
        "    print(f\"  Lyrics features shape: {lyrics_features.shape}\")\n",
        "    print(f\"  Predictions shape:     {predictions.shape}\")\n",
        "    print(f\"  Ground truth shape:    {ground_truth.shape}\")\n",
        "\n",
        "    return {\n",
        "        'audio_features': audio_features,\n",
        "        'lyrics_features': lyrics_features,\n",
        "        'predictions': predictions,\n",
        "        'ground_truth': ground_truth\n",
        "    }\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "BAcQ0UJ_mAHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}