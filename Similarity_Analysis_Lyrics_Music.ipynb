{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis: Lyrics vs Music Outputs\n",
    "\n",
    "This notebook performs comprehensive similarity analysis between:\n",
    "- **Audio features** (64-dim from VGGish)\n",
    "- **Lyrics features** (768-dim from BERT)\n",
    "- **Emotion predictions** (valence & arousal)\n",
    "\n",
    "## Methods Implemented:\n",
    "1. **Cosine Similarity** - Direct feature comparison\n",
    "2. **Canonical Correlation Analysis** - Find correlated components\n",
    "3. **Cross-Modal Retrieval** - Retrieval accuracy between modalities\n",
    "4. **Prediction Correlation** - Compare emotion predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Your Trained Model and Extract Features\n",
    "\n",
    "We need to extract intermediate features (before fusion) from both modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained bimodal model\n",
    "# REPLACE THIS PATH with your actual model path\n",
    "model_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
    "\n",
    "# Define your model architecture (copy from your training notebook)\n",
    "# This is a placeholder - you need to use your actual BimodalClassifier class\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class VGGish_Audio_Model(nn.Module):\n",
    "    \"\"\"Your audio model architecture\"\"\"\n",
    "    def __init__(self, num_classes=64):\n",
    "        super(VGGish_Audio_Model, self).__init__()\n",
    "        # Block 1: Input channels=1 (grayscale spectrogram) → 64 feature maps\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 2: 64 → 128 feature maps\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 3: 128 → 256 feature maps\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 4: 256 → 512 feature maps (no pooling)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Global pooling to fixed size output\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier layers: 512 → 256 → 64\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, 128, 1292] - mel-spectrogram\n",
    "        \n",
    "        # Block 1: Convolution → BatchNorm → ReLU → MaxPool\n",
    "        x = self.conv1(x)           # [B, 64, 128, 1292]\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)           # [B, 64, 64, 646]\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)           # [B, 128, 64, 646]\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)           # [B, 128, 32, 323]\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)           # [B, 256, 32, 323]\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool3(x)           # [B, 256, 16, 161]\n",
    "        \n",
    "        # Block 4 (no pooling)\n",
    "        x = self.conv4(x)           # [B, 512, 16, 161]\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Global average pooling: collapse spatial dimensions\n",
    "        x = self.adaptive_pool(x)   # [B, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1)   # Flatten to [B, 512]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)             # [B, 256]\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)             # [B, 64] - audio feature vector\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BimodalClassifier(nn.Module):\n",
    "    \"\"\"Combined audio + lyrics model\"\"\"\n",
    "    def __init__(self, audio_feature_dim=64, text_feature_dim=768, num_emotions=2):\n",
    "        super(BimodalClassifier, self).__init__()\n",
    "        \n",
    "        # Audio tower: VGGish CNN\n",
    "        self.audio_model = VGGish_Audio_Model(num_classes=audio_feature_dim)\n",
    "        \n",
    "        # Lyrics tower: BERT (frozen)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  # Freeze BERT weights\n",
    "        \n",
    "        # Combined dimension: 64 (audio) + 768 (lyrics) = 832\n",
    "        combined_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "        # Fusion classifier: 832 → 100 → 2 (valence, arousal)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(100, num_emotions)  # Outputs: [valence, arousal]\n",
    "        )\n",
    "        \n",
    "    def forward(self, spectrogram, input_ids, attention_mask):\n",
    "        # Extract audio features\n",
    "        # spectrogram: [batch_size, 1, 128, 1292]\n",
    "        audio_features = self.audio_model(spectrogram)  # [B, 64]\n",
    "        \n",
    "        # Extract lyrics features from BERT [CLS] token\n",
    "        # input_ids: [batch_size, 512], attention_mask: [batch_size, 512]\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        lyrics_features = bert_output.last_hidden_state[:, 0, :]  # [B, 768] - [CLS] token\n",
    "        \n",
    "        # Concatenate audio and lyrics features\n",
    "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)  # [B, 832]\n",
    "        \n",
    "        # Predict emotions\n",
    "        emotions = self.classifier(combined_features)  # [B, 2]\n",
    "        \n",
    "        return emotions, audio_features, lyrics_features\n",
    "    \n",
    "    def get_audio_features(self, spectrogram):\n",
    "        \"\"\"Extract only audio features (for similarity analysis)\"\"\"\n",
    "        return self.audio_model(spectrogram)\n",
    "    \n",
    "    def get_lyrics_features(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract only lyrics features (for similarity analysis)\"\"\"\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return bert_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Load the trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = BimodalClassifier(audio_feature_dim=64, text_feature_dim=768, num_emotions=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset and Extract Features\n",
    "\n",
    "We'll extract features for all songs in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (use the same dataloader from training)\n",
    "# REPLACE THESE PATHS with your actual paths\n",
    "csv_path = '/content/drive/MyDrive/dissertation/merge_dataset/output_from_code/master_processed_file_list.csv'\n",
    "tvt_path = '/content/drive/MyDrive/dissertation/merge_dataset/tvt_70_15_15_*.csv'  # Use appropriate split file\n",
    "\n",
    "# Load metadata\n",
    "# Line 1: Read the CSV file containing all processed song information\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Line 2: Display first few rows to verify data structure\n",
    "print(f\"Total songs: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "# For demonstration, let's use a subset (e.g., first 100 songs)\n",
    "# Line 3: Select subset for faster analysis (remove .head(100) to use all data)\n",
    "df_subset = df.head(100)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "# Line 4: Create tokenizer object - converts text to BERT-compatible tokens\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dataset(model, df, device, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract audio and lyrics features for all songs in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BimodalClassifier\n",
    "        df: DataFrame with spectrogram paths, lyrics paths, and labels\n",
    "        device: torch device (cuda or cpu)\n",
    "        tokenizer: BERT tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - audio_features: [N, 64] numpy array\n",
    "        - lyrics_features: [N, 768] numpy array\n",
    "        - predictions: [N, 2] numpy array (valence, arousal)\n",
    "        - ground_truth: [N, 2] numpy array (true valence, arousal)\n",
    "        - song_ids: list of song IDs\n",
    "    \"\"\"\n",
    "    # Line 1: Initialize empty lists to store extracted features\n",
    "    audio_features_list = []\n",
    "    lyrics_features_list = []\n",
    "    predictions_list = []\n",
    "    ground_truth_list = []\n",
    "    song_ids = []\n",
    "    \n",
    "    # Line 2: Disable gradient computation for faster inference (we're not training)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Line 3: Loop through each song in the dataframe with progress bar\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n",
    "            try:\n",
    "                # Line 4: Load pre-processed spectrogram from .npy file\n",
    "                # Shape: [128, 1292] - mel-spectrogram\n",
    "                spectrogram = np.load(row['spectrogram_path'])\n",
    "                \n",
    "                # Line 5: Convert numpy array to PyTorch tensor and add batch dimension\n",
    "                # [128, 1292] → [1, 1, 128, 1292] (batch=1, channels=1)\n",
    "                spectrogram = torch.FloatTensor(spectrogram).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Line 6: Load pre-tokenized lyrics from .pt file\n",
    "                lyrics_tokens = torch.load(row['lyrics_path'])\n",
    "                \n",
    "                # Line 7: Extract input_ids and attention_mask, move to device\n",
    "                # input_ids: token IDs [1, 512]\n",
    "                # attention_mask: indicates which tokens are real vs padding [1, 512]\n",
    "                input_ids = lyrics_tokens['input_ids'].to(device)\n",
    "                attention_mask = lyrics_tokens['attention_mask'].to(device)\n",
    "                \n",
    "                # Line 8: Forward pass through the model\n",
    "                # Returns: predictions [1, 2], audio_feat [1, 64], lyrics_feat [1, 768]\n",
    "                preds, audio_feat, lyrics_feat = model(spectrogram, input_ids, attention_mask)\n",
    "                \n",
    "                # Line 9: Move tensors to CPU and convert to numpy arrays\n",
    "                # Remove batch dimension with squeeze() and detach from computation graph\n",
    "                audio_features_list.append(audio_feat.squeeze().cpu().numpy())    # [64]\n",
    "                lyrics_features_list.append(lyrics_feat.squeeze().cpu().numpy())  # [768]\n",
    "                predictions_list.append(preds.squeeze().cpu().numpy())            # [2]\n",
    "                \n",
    "                # Line 10: Store ground truth valence and arousal values\n",
    "                ground_truth_list.append([row['valence'], row['arousal']])        # [2]\n",
    "                \n",
    "                # Line 11: Store song identifier for reference\n",
    "                song_ids.append(row['song_id'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Line 12: Print error message if processing fails for a song\n",
    "                print(f\"Error processing {row['song_id']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Line 13: Convert lists of arrays to single numpy arrays\n",
    "    # Stack along axis 0 to create [N, feature_dim] arrays\n",
    "    audio_features = np.stack(audio_features_list)      # [N, 64]\n",
    "    lyrics_features = np.stack(lyrics_features_list)    # [N, 768]\n",
    "    predictions = np.stack(predictions_list)            # [N, 2]\n",
    "    ground_truth = np.stack(ground_truth_list)          # [N, 2]\n",
    "    \n",
    "    # Line 14: Print summary statistics\n",
    "    print(f\"\\nExtracted features for {len(song_ids)} songs\")\n",
    "    print(f\"Audio features shape: {audio_features.shape}\")\n",
    "    print(f\"Lyrics features shape: {lyrics_features.shape}\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Line 15: Return dictionary with all extracted information\n",
    "    return {\n",
    "        'audio_features': audio_features,\n",
    "        'lyrics_features': lyrics_features,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'song_ids': song_ids\n",
    "    }\n",
    "\n",
    "# Extract features\n",
    "features_dict = extract_features_from_dataset(model, df_subset, device, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Cosine Similarity Analysis\n",
    "\n",
    "**What it does**: Measures the angle between feature vectors. Values range from -1 (opposite) to 1 (identical).\n",
    "\n",
    "**When to use**: To see if songs with similar audio have similar lyrics encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_similarity(audio_features, lyrics_features):\n",
    "    \"\"\"\n",
    "    Compute pairwise cosine similarity between audio and lyrics.\n",
    "    \n",
    "    Returns:\n",
    "        - audio_similarity: [N, N] matrix - similarity between audio vectors\n",
    "        - lyrics_similarity: [N, N] matrix - similarity between lyrics vectors\n",
    "        - cross_modal_similarity: [N, N] matrix - similarity between audio[i] and lyrics[j]\n",
    "    \"\"\"\n",
    "    # Line 1: Compute cosine similarity between all pairs of audio feature vectors\n",
    "    # For N songs, creates NxN matrix where entry [i,j] = similarity(audio_i, audio_j)\n",
    "    # Diagonal elements are 1.0 (song compared to itself)\n",
    "    audio_similarity = cosine_similarity(audio_features, audio_features)\n",
    "    \n",
    "    # Line 2: Same for lyrics features\n",
    "    # Entry [i,j] = similarity(lyrics_i, lyrics_j)\n",
    "    lyrics_similarity = cosine_similarity(lyrics_features, lyrics_features)\n",
    "    \n",
    "    # Line 3: CROSS-MODAL similarity - compare audio to lyrics\n",
    "    # Entry [i,j] = similarity(audio_i, lyrics_j)\n",
    "    # This tells us: for song i, how similar is its audio to song j's lyrics?\n",
    "    cross_modal_similarity = cosine_similarity(audio_features, lyrics_features)\n",
    "    \n",
    "    # Line 4: Print statistics about the similarity distributions\n",
    "    print(f\"Audio similarity - Mean: {audio_similarity.mean():.3f}, Std: {audio_similarity.std():.3f}\")\n",
    "    print(f\"Lyrics similarity - Mean: {lyrics_similarity.mean():.3f}, Std: {lyrics_similarity.std():.3f}\")\n",
    "    print(f\"Cross-modal similarity - Mean: {cross_modal_similarity.mean():.3f}, Std: {cross_modal_similarity.std():.3f}\")\n",
    "    \n",
    "    # Line 5: Return all three similarity matrices\n",
    "    return audio_similarity, lyrics_similarity, cross_modal_similarity\n",
    "\n",
    "# Compute similarities\n",
    "audio_sim, lyrics_sim, cross_modal_sim = compute_pairwise_similarity(\n",
    "    features_dict['audio_features'], \n",
    "    features_dict['lyrics_features']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrices\n",
    "def plot_similarity_matrices(audio_sim, lyrics_sim, cross_modal_sim):\n",
    "    \"\"\"\n",
    "    Create heatmaps to visualize similarity patterns.\n",
    "    \"\"\"\n",
    "    # Line 1: Create a figure with 3 subplots side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Line 2: Plot audio similarity matrix\n",
    "    # vmin=0, vmax=1 sets color scale from 0 (blue) to 1 (red)\n",
    "    # cmap='coolwarm' uses blue for low, red for high values\n",
    "    im1 = axes[0].imshow(audio_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "    axes[0].set_title('Audio-to-Audio Similarity', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Song Index')\n",
    "    axes[0].set_ylabel('Song Index')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Line 3: Plot lyrics similarity matrix\n",
    "    im2 = axes[1].imshow(lyrics_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "    axes[1].set_title('Lyrics-to-Lyrics Similarity', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Song Index')\n",
    "    axes[1].set_ylabel('Song Index')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Line 4: Plot cross-modal similarity matrix\n",
    "    # This is the KEY matrix: shows how audio features relate to lyrics features\n",
    "    im3 = axes[2].imshow(cross_modal_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "    axes[2].set_title('Audio-to-Lyrics Cross-Modal Similarity', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Lyrics Index')\n",
    "    axes[2].set_ylabel('Audio Index')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    # Line 5: Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Line 6: Analyze the diagonal of cross-modal similarity\n",
    "    # Diagonal = similarity between song's OWN audio and lyrics\n",
    "    # High diagonal values = audio and lyrics are aligned\n",
    "    diagonal_similarity = np.diag(cross_modal_sim)\n",
    "    print(f\"\\nSelf-similarity (audio vs own lyrics):\")\n",
    "    print(f\"  Mean: {diagonal_similarity.mean():.3f}\")\n",
    "    print(f\"  Std: {diagonal_similarity.std():.3f}\")\n",
    "    print(f\"  Min: {diagonal_similarity.min():.3f}\")\n",
    "    print(f\"  Max: {diagonal_similarity.max():.3f}\")\n",
    "\n",
    "plot_similarity_matrices(audio_sim, lyrics_sim, cross_modal_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Canonical Correlation Analysis (CCA)\n",
    "\n",
    "**What it does**: Finds linear transformations of audio and lyrics features that maximize correlation.\n",
    "\n",
    "**When to use**: To discover if there are latent shared dimensions between modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cca_analysis(audio_features, lyrics_features, n_components=10):\n",
    "    \"\"\"\n",
    "    Perform Canonical Correlation Analysis.\n",
    "    \n",
    "    Args:\n",
    "        audio_features: [N, 64] audio feature matrix\n",
    "        lyrics_features: [N, 768] lyrics feature matrix\n",
    "        n_components: number of canonical components to extract\n",
    "    \n",
    "    Returns:\n",
    "        cca: fitted CCA model\n",
    "        correlations: correlation coefficient for each component\n",
    "    \"\"\"\n",
    "    # Line 1: Initialize CCA with specified number of components\n",
    "    # n_components: how many correlated dimensions to find\n",
    "    # max_iter: maximum iterations for optimization\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    \n",
    "    # Line 2: Fit CCA to find transformations that maximize correlation\n",
    "    # Learns weights W_audio and W_lyrics such that:\n",
    "    #   audio_features @ W_audio and lyrics_features @ W_lyrics are maximally correlated\n",
    "    cca.fit(audio_features, lyrics_features)\n",
    "    \n",
    "    # Line 3: Transform features into canonical space\n",
    "    # audio_canonical: [N, n_components] - audio projected onto canonical dimensions\n",
    "    # lyrics_canonical: [N, n_components] - lyrics projected onto canonical dimensions\n",
    "    audio_canonical, lyrics_canonical = cca.transform(audio_features, lyrics_features)\n",
    "    \n",
    "    # Line 4: Compute correlation for each canonical component\n",
    "    # For each of the n_components dimensions, calculate how correlated they are\n",
    "    correlations = []\n",
    "    for i in range(n_components):\n",
    "        # Line 5: Pearson correlation between i-th canonical dimension\n",
    "        # pearsonr returns (correlation_coefficient, p_value)\n",
    "        corr, _ = pearsonr(audio_canonical[:, i], lyrics_canonical[:, i])\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    # Line 6: Convert to numpy array for easier handling\n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Line 7: Print results\n",
    "    print(f\"\\nCCA Results (n_components={n_components}):\")\n",
    "    for i, corr in enumerate(correlations):\n",
    "        print(f\"  Component {i+1}: {corr:.4f}\")\n",
    "    print(f\"\\nMean correlation: {correlations.mean():.4f}\")\n",
    "    print(f\"Max correlation: {correlations.max():.4f}\")\n",
    "    \n",
    "    # Line 8: Return CCA model and correlations\n",
    "    return cca, correlations, audio_canonical, lyrics_canonical\n",
    "\n",
    "# Perform CCA\n",
    "cca_model, cca_correlations, audio_can, lyrics_can = perform_cca_analysis(\n",
    "    features_dict['audio_features'],\n",
    "    features_dict['lyrics_features'],\n",
    "    n_components=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CCA correlations\n",
    "def plot_cca_results(correlations):\n",
    "    \"\"\"\n",
    "    Plot canonical correlations as a bar chart.\n",
    "    \"\"\"\n",
    "    # Line 1: Create bar chart of canonical correlations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Line 2: Create bars for each component\n",
    "    # x-axis: component index, y-axis: correlation value\n",
    "    plt.bar(range(1, len(correlations) + 1), correlations, color='steelblue', alpha=0.7)\n",
    "    \n",
    "    # Line 3: Add horizontal line at 0.5 for reference\n",
    "    # Correlations above 0.5 indicate moderate to strong relationship\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', label='Moderate correlation (0.5)')\n",
    "    \n",
    "    # Line 4: Labels and formatting\n",
    "    plt.xlabel('Canonical Component', fontsize=12)\n",
    "    plt.ylabel('Correlation Coefficient', fontsize=12)\n",
    "    plt.title('Canonical Correlations Between Audio and Lyrics Features', fontsize=14, fontweight='bold')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Line 5: Interpretation guidance\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- High correlations (>0.7): Strong shared latent dimensions between audio and lyrics\")\n",
    "    print(\"- Moderate correlations (0.5-0.7): Some shared structure\")\n",
    "    print(\"- Low correlations (<0.5): Modalities encode different information\")\n",
    "\n",
    "plot_cca_results(cca_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Cross-Modal Retrieval Analysis\n",
    "\n",
    "**What it does**: For each song's audio, find the most similar lyrics (and vice versa). Measures retrieval accuracy.\n",
    "\n",
    "**When to use**: To test if one modality can predict the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_modal_retrieval(audio_features, lyrics_features, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform cross-modal retrieval: given audio, find matching lyrics and vice versa.\n",
    "    \n",
    "    Args:\n",
    "        audio_features: [N, 64]\n",
    "        lyrics_features: [N, 768]\n",
    "        top_k: how many top matches to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with retrieval results\n",
    "    \"\"\"\n",
    "    # Line 1: Compute cross-modal similarity matrix\n",
    "    # sim[i, j] = similarity between audio_i and lyrics_j\n",
    "    similarity_matrix = cosine_similarity(audio_features, lyrics_features)\n",
    "    \n",
    "    n_samples = len(audio_features)\n",
    "    \n",
    "    # === AUDIO-TO-LYRICS RETRIEVAL ===\n",
    "    # Line 2: For each audio, find top-k most similar lyrics\n",
    "    # np.argsort returns indices that would sort array (lowest to highest)\n",
    "    # [:, ::-1] reverses to get highest to lowest\n",
    "    # [:, :top_k] keeps only top-k matches\n",
    "    audio_to_lyrics_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    # Line 3: Check if correct match (diagonal) is in top-k\n",
    "    # For song i, correct match is lyrics i (when i == j)\n",
    "    audio_to_lyrics_accuracy = []\n",
    "    for i in range(n_samples):\n",
    "        # Line 4: Check if i (correct lyrics index) is in the top-k retrieved indices\n",
    "        if i in audio_to_lyrics_indices[i]:\n",
    "            audio_to_lyrics_accuracy.append(1)  # Correct\n",
    "        else:\n",
    "            audio_to_lyrics_accuracy.append(0)  # Incorrect\n",
    "    \n",
    "    # Line 5: Calculate accuracy (proportion of correct retrievals)\n",
    "    audio_to_lyrics_acc = np.mean(audio_to_lyrics_accuracy)\n",
    "    \n",
    "    # === LYRICS-TO-AUDIO RETRIEVAL ===\n",
    "    # Line 6: Transpose similarity matrix for lyrics-to-audio direction\n",
    "    # Now rows = lyrics, columns = audio\n",
    "    lyrics_to_audio_indices = np.argsort(similarity_matrix.T, axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    # Line 7: Same accuracy check for lyrics-to-audio\n",
    "    lyrics_to_audio_accuracy = []\n",
    "    for i in range(n_samples):\n",
    "        if i in lyrics_to_audio_indices[i]:\n",
    "            lyrics_to_audio_accuracy.append(1)\n",
    "        else:\n",
    "            lyrics_to_audio_accuracy.append(0)\n",
    "    \n",
    "    # Line 8: Calculate accuracy\n",
    "    lyrics_to_audio_acc = np.mean(lyrics_to_audio_accuracy)\n",
    "    \n",
    "    # Line 9: Print results\n",
    "    print(f\"\\nCross-Modal Retrieval Results (Top-{top_k}):\")\n",
    "    print(f\"  Audio → Lyrics accuracy: {audio_to_lyrics_acc:.2%}\")\n",
    "    print(f\"  Lyrics → Audio accuracy: {lyrics_to_audio_acc:.2%}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  {audio_to_lyrics_acc:.1%} of times, given a song's audio, the correct lyrics\")\n",
    "    print(f\"  are in the top-{top_k} most similar lyrics based on feature similarity.\")\n",
    "    \n",
    "    # Line 10: Return detailed results\n",
    "    return {\n",
    "        'audio_to_lyrics_accuracy': audio_to_lyrics_acc,\n",
    "        'lyrics_to_audio_accuracy': lyrics_to_audio_acc,\n",
    "        'audio_to_lyrics_indices': audio_to_lyrics_indices,\n",
    "        'lyrics_to_audio_indices': lyrics_to_audio_indices\n",
    "    }\n",
    "\n",
    "# Perform retrieval analysis\n",
    "retrieval_results = cross_modal_retrieval(\n",
    "    features_dict['audio_features'],\n",
    "    features_dict['lyrics_features'],\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Prediction-Level Correlation\n",
    "\n",
    "**What it does**: Compares emotion predictions when using only audio vs only lyrics vs both.\n",
    "\n",
    "**When to use**: To see if audio and lyrics agree on the predicted emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_agreement(features_dict, model, device):\n",
    "    \"\"\"\n",
    "    Analyze how audio-only and lyrics-only predictions correlate.\n",
    "    \n",
    "    NOTE: This requires training separate audio-only and lyrics-only models,\n",
    "    or using zero vectors for the other modality.\n",
    "    \"\"\"\n",
    "    # Line 1: Extract features\n",
    "    audio_features = features_dict['audio_features']\n",
    "    lyrics_features = features_dict['lyrics_features']\n",
    "    combined_predictions = features_dict['predictions']  # [N, 2] - from bimodal model\n",
    "    ground_truth = features_dict['ground_truth']\n",
    "    \n",
    "    # Line 2: Compute correlation between audio features and predictions\n",
    "    # For each audio feature dimension, correlate with valence and arousal predictions\n",
    "    audio_valence_corr = []\n",
    "    audio_arousal_corr = []\n",
    "    \n",
    "    for dim in range(audio_features.shape[1]):  # 64 dimensions\n",
    "        # Line 3: Correlate this audio dimension with valence predictions\n",
    "        corr_val, _ = pearsonr(audio_features[:, dim], combined_predictions[:, 0])\n",
    "        audio_valence_corr.append(corr_val)\n",
    "        \n",
    "        # Line 4: Correlate this audio dimension with arousal predictions\n",
    "        corr_ar, _ = pearsonr(audio_features[:, dim], combined_predictions[:, 1])\n",
    "        audio_arousal_corr.append(corr_ar)\n",
    "    \n",
    "    # Line 5: Same for lyrics features (768 dimensions)\n",
    "    lyrics_valence_corr = []\n",
    "    lyrics_arousal_corr = []\n",
    "    \n",
    "    for dim in range(lyrics_features.shape[1]):\n",
    "        corr_val, _ = pearsonr(lyrics_features[:, dim], combined_predictions[:, 0])\n",
    "        lyrics_valence_corr.append(corr_val)\n",
    "        \n",
    "        corr_ar, _ = pearsonr(lyrics_features[:, dim], combined_predictions[:, 1])\n",
    "        lyrics_arousal_corr.append(corr_ar)\n",
    "    \n",
    "    # Line 6: Convert to arrays\n",
    "    audio_valence_corr = np.abs(audio_valence_corr)  # Use absolute value\n",
    "    audio_arousal_corr = np.abs(audio_arousal_corr)\n",
    "    lyrics_valence_corr = np.abs(lyrics_valence_corr)\n",
    "    lyrics_arousal_corr = np.abs(lyrics_arousal_corr)\n",
    "    \n",
    "    # Line 7: Print summary statistics\n",
    "    print(\"\\nFeature-Prediction Correlation Analysis:\")\n",
    "    print(f\"\\nAudio features correlation with predictions:\")\n",
    "    print(f\"  Valence - Mean: {audio_valence_corr.mean():.3f}, Max: {audio_valence_corr.max():.3f}\")\n",
    "    print(f\"  Arousal - Mean: {audio_arousal_corr.mean():.3f}, Max: {audio_arousal_corr.max():.3f}\")\n",
    "    \n",
    "    print(f\"\\nLyrics features correlation with predictions:\")\n",
    "    print(f\"  Valence - Mean: {lyrics_valence_corr.mean():.3f}, Max: {lyrics_valence_corr.max():.3f}\")\n",
    "    print(f\"  Arousal - Mean: {lyrics_arousal_corr.mean():.3f}, Max: {lyrics_arousal_corr.max():.3f}\")\n",
    "    \n",
    "    # Line 8: Compare which modality has stronger predictive features\n",
    "    if audio_valence_corr.mean() > lyrics_valence_corr.mean():\n",
    "        print(f\"\\n→ Audio features have stronger correlation with valence predictions\")\n",
    "    else:\n",
    "        print(f\"\\n→ Lyrics features have stronger correlation with valence predictions\")\n",
    "    \n",
    "    if audio_arousal_corr.mean() > lyrics_arousal_corr.mean():\n",
    "        print(f\"→ Audio features have stronger correlation with arousal predictions\")\n",
    "    else:\n",
    "        print(f\"→ Lyrics features have stronger correlation with arousal predictions\")\n",
    "    \n",
    "    # Line 9: Return results for plotting\n",
    "    return {\n",
    "        'audio_valence_corr': audio_valence_corr,\n",
    "        'audio_arousal_corr': audio_arousal_corr,\n",
    "        'lyrics_valence_corr': lyrics_valence_corr,\n",
    "        'lyrics_arousal_corr': lyrics_arousal_corr\n",
    "    }\n",
    "\n",
    "# Analyze prediction agreement\n",
    "prediction_analysis = analyze_prediction_agreement(features_dict, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "def plot_feature_importance(prediction_analysis):\n",
    "    \"\"\"\n",
    "    Plot which features correlate most with emotion predictions.\n",
    "    \"\"\"\n",
    "    # Line 1: Create subplots for audio and lyrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Line 2: Plot audio feature correlations with valence\n",
    "    axes[0, 0].bar(range(len(prediction_analysis['audio_valence_corr'])), \n",
    "                    prediction_analysis['audio_valence_corr'], \n",
    "                    color='steelblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Audio Feature Correlation with Valence', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Audio Feature Dimension (0-63)')\n",
    "    axes[0, 0].set_ylabel('Absolute Correlation')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Line 3: Plot audio feature correlations with arousal\n",
    "    axes[0, 1].bar(range(len(prediction_analysis['audio_arousal_corr'])), \n",
    "                    prediction_analysis['audio_arousal_corr'], \n",
    "                    color='coral', alpha=0.7)\n",
    "    axes[0, 1].set_title('Audio Feature Correlation with Arousal', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Audio Feature Dimension (0-63)')\n",
    "    axes[0, 1].set_ylabel('Absolute Correlation')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Line 4: Plot lyrics feature correlations with valence (sample every 10th for visibility)\n",
    "    sampled_lyrics_val = prediction_analysis['lyrics_valence_corr'][::10]\n",
    "    axes[1, 0].bar(range(len(sampled_lyrics_val)), sampled_lyrics_val, \n",
    "                    color='green', alpha=0.7)\n",
    "    axes[1, 0].set_title('Lyrics Feature Correlation with Valence (sampled)', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Lyrics Feature Dimension (sampled every 10th)')\n",
    "    axes[1, 0].set_ylabel('Absolute Correlation')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Line 5: Plot lyrics feature correlations with arousal\n",
    "    sampled_lyrics_ar = prediction_analysis['lyrics_arousal_corr'][::10]\n",
    "    axes[1, 1].bar(range(len(sampled_lyrics_ar)), sampled_lyrics_ar, \n",
    "                    color='purple', alpha=0.7)\n",
    "    axes[1, 1].set_title('Lyrics Feature Correlation with Arousal (sampled)', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Lyrics Feature Dimension (sampled every 10th)')\n",
    "    axes[1, 1].set_ylabel('Absolute Correlation')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Line 6: Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(prediction_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "Generate a comprehensive summary of all similarity analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity_report(audio_sim, lyrics_sim, cross_modal_sim, \n",
    "                                 cca_correlations, retrieval_results, \n",
    "                                 features_dict):\n",
    "    \"\"\"\n",
    "    Generate comprehensive similarity analysis report.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\" \"*20 + \"SIMILARITY ANALYSIS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Line 1: Dataset summary\n",
    "    print(f\"\\n1. DATASET SUMMARY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Total songs analyzed: {len(features_dict['song_ids'])}\")\n",
    "    print(f\"   Audio feature dimension: {features_dict['audio_features'].shape[1]}\")\n",
    "    print(f\"   Lyrics feature dimension: {features_dict['lyrics_features'].shape[1]}\")\n",
    "    \n",
    "    # Line 2: Cosine similarity results\n",
    "    print(f\"\\n2. COSINE SIMILARITY ANALYSIS\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Line 3: Self-similarity (diagonal of cross-modal matrix)\n",
    "    self_similarity = np.diag(cross_modal_sim)\n",
    "    print(f\"   Within-song similarity (audio vs own lyrics):\")\n",
    "    print(f\"     Mean: {self_similarity.mean():.4f}\")\n",
    "    print(f\"     Std:  {self_similarity.std():.4f}\")\n",
    "    print(f\"     Range: [{self_similarity.min():.4f}, {self_similarity.max():.4f}]\")\n",
    "    \n",
    "    # Line 4: Off-diagonal similarity (cross-song)\n",
    "    mask = np.ones_like(cross_modal_sim, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    off_diagonal = cross_modal_sim[mask]\n",
    "    print(f\"\\n   Cross-song similarity (audio_i vs lyrics_j, i≠j):\")\n",
    "    print(f\"     Mean: {off_diagonal.mean():.4f}\")\n",
    "    print(f\"     Std:  {off_diagonal.std():.4f}\")\n",
    "    \n",
    "    # Line 5: CCA results\n",
    "    print(f\"\\n3. CANONICAL CORRELATION ANALYSIS\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Top 3 canonical correlations:\")\n",
    "    for i in range(min(3, len(cca_correlations))):\n",
    "        print(f\"     Component {i+1}: {cca_correlations[i]:.4f}\")\n",
    "    print(f\"   Mean correlation: {cca_correlations.mean():.4f}\")\n",
    "    \n",
    "    # Line 6: Retrieval results\n",
    "    print(f\"\\n4. CROSS-MODAL RETRIEVAL\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Audio → Lyrics (Top-5 accuracy): {retrieval_results['audio_to_lyrics_accuracy']:.2%}\")\n",
    "    print(f\"   Lyrics → Audio (Top-5 accuracy): {retrieval_results['lyrics_to_audio_accuracy']:.2%}\")\n",
    "    \n",
    "    # Line 7: Interpretation\n",
    "    print(f\"\\n5. INTERPRETATION\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Line 8: Assess agreement level\n",
    "    if self_similarity.mean() > 0.7:\n",
    "        agreement = \"STRONG\"\n",
    "    elif self_similarity.mean() > 0.5:\n",
    "        agreement = \"MODERATE\"\n",
    "    else:\n",
    "        agreement = \"WEAK\"\n",
    "    \n",
    "    print(f\"   Agreement between audio and lyrics: {agreement}\")\n",
    "    print(f\"   (Based on mean self-similarity of {self_similarity.mean():.3f})\")\n",
    "    \n",
    "    # Line 9: CCA interpretation\n",
    "    if cca_correlations[0] > 0.7:\n",
    "        print(f\"\\n   CCA reveals STRONG shared latent structure between modalities.\")\n",
    "        print(f\"   The first canonical component has correlation {cca_correlations[0]:.3f}.\")\n",
    "    elif cca_correlations[0] > 0.5:\n",
    "        print(f\"\\n   CCA reveals MODERATE shared latent structure between modalities.\")\n",
    "    else:\n",
    "        print(f\"\\n   CCA reveals LIMITED shared latent structure between modalities.\")\n",
    "        print(f\"   Audio and lyrics may encode complementary rather than redundant information.\")\n",
    "    \n",
    "    # Line 10: Retrieval interpretation\n",
    "    if retrieval_results['audio_to_lyrics_accuracy'] > 0.5:\n",
    "        print(f\"\\n   Cross-modal retrieval shows GOOD alignment:\")\n",
    "        print(f\"   Given audio, we can identify matching lyrics {retrieval_results['audio_to_lyrics_accuracy']:.1%} of the time.\")\n",
    "    else:\n",
    "        print(f\"\\n   Cross-modal retrieval shows LIMITED alignment:\")\n",
    "        print(f\"   Audio features alone are insufficient to reliably identify matching lyrics.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate report\n",
    "generate_similarity_report(audio_sim, lyrics_sim, cross_modal_sim, \n",
    "                            cca_correlations, retrieval_results, \n",
    "                            features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save similarity matrices and metrics for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "output_dir = '/content/drive/MyDrive/dissertation/similarity_analysis_results/'\n",
    "\n",
    "# Line 1: Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Line 2: Save similarity matrices as numpy arrays\n",
    "np.save(os.path.join(output_dir, 'audio_similarity_matrix.npy'), audio_sim)\n",
    "np.save(os.path.join(output_dir, 'lyrics_similarity_matrix.npy'), lyrics_sim)\n",
    "np.save(os.path.join(output_dir, 'cross_modal_similarity_matrix.npy'), cross_modal_sim)\n",
    "\n",
    "# Line 3: Save CCA results\n",
    "np.save(os.path.join(output_dir, 'cca_correlations.npy'), cca_correlations)\n",
    "\n",
    "# Line 4: Save features for future use\n",
    "np.save(os.path.join(output_dir, 'audio_features.npy'), features_dict['audio_features'])\n",
    "np.save(os.path.join(output_dir, 'lyrics_features.npy'), features_dict['lyrics_features'])\n",
    "\n",
    "# Line 5: Create summary CSV with per-song similarity scores\n",
    "results_df = pd.DataFrame({\n",
    "    'song_id': features_dict['song_ids'],\n",
    "    'self_similarity': np.diag(cross_modal_sim),\n",
    "    'true_valence': features_dict['ground_truth'][:, 0],\n",
    "    'true_arousal': features_dict['ground_truth'][:, 1],\n",
    "    'pred_valence': features_dict['predictions'][:, 0],\n",
    "    'pred_arousal': features_dict['predictions'][:, 1]\n",
    "})\n",
    "\n",
    "# Line 6: Save CSV\n",
    "results_df.to_csv(os.path.join(output_dir, 'similarity_analysis_summary.csv'), index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - audio_similarity_matrix.npy\")\n",
    "print(f\"  - lyrics_similarity_matrix.npy\")\n",
    "print(f\"  - cross_modal_similarity_matrix.npy\")\n",
    "print(f\"  - cca_correlations.npy\")\n",
    "print(f\"  - audio_features.npy\")\n",
    "print(f\"  - lyrics_features.npy\")\n",
    "print(f\"  - similarity_analysis_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
