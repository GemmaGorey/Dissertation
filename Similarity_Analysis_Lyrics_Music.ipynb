{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis: Audio vs Lyrics Features\n",
    "\n",
    "This notebook analyzes how similar audio and lyrics features are using multiple complementary metrics.\n",
    "\n",
    "## Part A: Similarity Metrics (Current Analysis)\n",
    "1. **Cosine Similarity** - Direct angular similarity between features\n",
    "2. **Canonical Correlation Analysis (CCA)** - Find shared latent structure\n",
    "3. **Cross-Modal Retrieval** - Test if one modality predicts the other\n",
    "\n",
    "## Part B: Siamese Networks (Future Work)\n",
    "- Explanation of Siamese networks for semantic similarity\n",
    "- Implementation guidance\n",
    "- Complexity assessment for your codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART A: SIMILARITY METRICS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Your Trained Model\n",
    "\n",
    "We'll load your BimodalClassifier to extract audio and lyrics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 1: Import transformers for BERT\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class VGGish_Audio_Model(nn.Module):\n",
    "    \"\"\"Audio feature extractor - VGGish architecture\"\"\"\n",
    "    def __init__(self, num_classes=64):\n",
    "        super(VGGish_Audio_Model, self).__init__()\n",
    "        # Block 1: 1 channel (grayscale) â†’ 64 feature maps\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 2: 64 â†’ 128 feature maps\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 3: 128 â†’ 256 feature maps\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Block 4: 256 â†’ 512 feature maps\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Global pooling\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier: 512 â†’ 256 â†’ 64\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input: [batch, 1, 128, 1292] mel-spectrogram\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output: [batch, 64]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BimodalClassifier(nn.Module):\n",
    "    \"\"\"Bimodal model: Audio + Lyrics â†’ Emotion prediction\"\"\"\n",
    "    def __init__(self, audio_feature_dim=64, text_feature_dim=768, num_emotions=2):\n",
    "        super(BimodalClassifier, self).__init__()\n",
    "        \n",
    "        # Audio tower\n",
    "        self.audio_model = VGGish_Audio_Model(num_classes=audio_feature_dim)\n",
    "        \n",
    "        # Lyrics tower (BERT frozen)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Fusion classifier\n",
    "        combined_dim = audio_feature_dim + text_feature_dim  # 64 + 768 = 832\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(100, num_emotions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, spectrogram, input_ids, attention_mask):\n",
    "        # Line 1: Extract audio features [batch, 64]\n",
    "        audio_features = self.audio_model(spectrogram)\n",
    "        \n",
    "        # Line 2: Extract lyrics features from BERT [CLS] token [batch, 768]\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        lyrics_features = bert_output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Line 3: Concatenate [batch, 832]\n",
    "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
    "        \n",
    "        # Line 4: Predict emotions [batch, 2] (valence, arousal)\n",
    "        emotions = self.classifier(combined_features)\n",
    "        \n",
    "        return emotions, audio_features, lyrics_features\n",
    "\n",
    "# Line 5: Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Line 6: Initialize and load model\n",
    "# UPDATE THIS PATH to your trained model\n",
    "model_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
    "\n",
    "model = BimodalClassifier(audio_feature_dim=64, text_feature_dim=768, num_emotions=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Features from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 1: Load your dataset CSV\n",
    "# UPDATE THIS PATH to your master CSV file\n",
    "csv_path = '/content/drive/MyDrive/dissertation/merge_dataset/output_from_code/master_processed_file_list.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total songs available: {len(df)}\")\n",
    "print(f\"\\nDataFrame columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Line 2: Select subset or full dataset\n",
    "# For quick testing, use df.head(100)\n",
    "# For full analysis, use df (all songs)\n",
    "df_subset = df.head(100)  # Change this as needed\n",
    "print(f\"\\nâœ“ Using {len(df_subset)} songs for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dataset(model, df, device):\n",
    "    \"\"\"\n",
    "    Extract audio and lyrics features for all songs.\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "        - audio_features: [N, 64] numpy array\n",
    "        - lyrics_features: [N, 768] numpy array\n",
    "        - predictions: [N, 2] numpy array (valence, arousal)\n",
    "        - ground_truth: [N, 2] numpy array\n",
    "        - song_ids: list of song IDs\n",
    "    \"\"\"\n",
    "    # Line 1: Initialize storage lists\n",
    "    audio_features_list = []\n",
    "    lyrics_features_list = []\n",
    "    predictions_list = []\n",
    "    ground_truth_list = []\n",
    "    song_ids = []\n",
    "    \n",
    "    # Line 2: Extract features without computing gradients\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n",
    "            try:\n",
    "                # Line 3: Load spectrogram [128, 1292]\n",
    "                spectrogram = np.load(row['spectrogram_path'])\n",
    "                # Line 4: Convert to tensor [1, 1, 128, 1292]\n",
    "                spectrogram = torch.FloatTensor(spectrogram).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Line 5: Load tokenized lyrics\n",
    "                lyrics_tokens = torch.load(row['lyrics_path'])\n",
    "                input_ids = lyrics_tokens['input_ids'].to(device)\n",
    "                attention_mask = lyrics_tokens['attention_mask'].to(device)\n",
    "                \n",
    "                # Line 6: Forward pass\n",
    "                preds, audio_feat, lyrics_feat = model(spectrogram, input_ids, attention_mask)\n",
    "                \n",
    "                # Line 7: Store results (move to CPU and convert to numpy)\n",
    "                audio_features_list.append(audio_feat.squeeze().cpu().numpy())\n",
    "                lyrics_features_list.append(lyrics_feat.squeeze().cpu().numpy())\n",
    "                predictions_list.append(preds.squeeze().cpu().numpy())\n",
    "                ground_truth_list.append([row['valence'], row['arousal']])\n",
    "                song_ids.append(row['song_id'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row['song_id']}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Line 8: Convert lists to arrays\n",
    "    audio_features = np.stack(audio_features_list)      # [N, 64]\n",
    "    lyrics_features = np.stack(lyrics_features_list)    # [N, 768]\n",
    "    predictions = np.stack(predictions_list)            # [N, 2]\n",
    "    ground_truth = np.stack(ground_truth_list)          # [N, 2]\n",
    "    \n",
    "    print(f\"\\nâœ“ Extracted features for {len(song_ids)} songs\")\n",
    "    print(f\"  Audio features shape: {audio_features.shape}\")\n",
    "    print(f\"  Lyrics features shape: {lyrics_features.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'audio_features': audio_features,\n",
    "        'lyrics_features': lyrics_features,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'song_ids': song_ids\n",
    "    }\n",
    "\n",
    "# Extract features\n",
    "features_dict = extract_features_from_dataset(model, df_subset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 1: Cosine Similarity\n",
    "\n",
    "**What it measures**: Angular similarity between feature vectors (ranges from -1 to 1)\n",
    "\n",
    "**Key insight**: Diagonal of cross-modal matrix = similarity between each song's audio and its OWN lyrics\n",
    "\n",
    "**Interpretation**:\n",
    "- High diagonal values (>0.7): Audio and lyrics are strongly aligned\n",
    "- Moderate (0.5-0.7): Some alignment\n",
    "- Low (<0.5): Audio and lyrics encode different aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity_analysis(audio_features, lyrics_features):\n",
    "    \"\"\"\n",
    "    Compute all pairwise cosine similarities.\n",
    "    \"\"\"\n",
    "    # Line 1: Audio-to-audio similarity [N, N]\n",
    "    # Entry [i,j] = similarity between audio_i and audio_j\n",
    "    audio_sim = cosine_similarity(audio_features, audio_features)\n",
    "    \n",
    "    # Line 2: Lyrics-to-lyrics similarity [N, N]\n",
    "    lyrics_sim = cosine_similarity(lyrics_features, lyrics_features)\n",
    "    \n",
    "    # Line 3: CROSS-MODAL similarity [N, N]\n",
    "    # Entry [i,j] = similarity between audio_i and lyrics_j\n",
    "    # KEY METRIC: Diagonal elements = audio_i vs lyrics_i (same song)\n",
    "    cross_modal_sim = cosine_similarity(audio_features, lyrics_features)\n",
    "    \n",
    "    # Line 4: Extract diagonal (self-similarity)\n",
    "    self_similarity = np.diag(cross_modal_sim)\n",
    "    \n",
    "    # Line 5: Extract off-diagonal (cross-song similarity)\n",
    "    mask = np.ones_like(cross_modal_sim, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    cross_song_sim = cross_modal_sim[mask]\n",
    "    \n",
    "    # Line 6: Print results\n",
    "    print(\"=\"*70)\n",
    "    print(\"COSINE SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n1. SELF-SIMILARITY (audio vs own lyrics):\")\n",
    "    print(f\"   Mean:  {self_similarity.mean():.4f}\")\n",
    "    print(f\"   Std:   {self_similarity.std():.4f}\")\n",
    "    print(f\"   Range: [{self_similarity.min():.4f}, {self_similarity.max():.4f}]\")\n",
    "    \n",
    "    print(f\"\\n2. CROSS-SONG SIMILARITY (audio_i vs lyrics_j, iâ‰ j):\")\n",
    "    print(f\"   Mean:  {cross_song_sim.mean():.4f}\")\n",
    "    print(f\"   Std:   {cross_song_sim.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n3. WITHIN-MODALITY SIMILARITY:\")\n",
    "    print(f\"   Audio-to-audio mean:   {audio_sim[mask].mean():.4f}\")\n",
    "    print(f\"   Lyrics-to-lyrics mean: {lyrics_sim[mask].mean():.4f}\")\n",
    "    \n",
    "    # Line 7: Interpretation\n",
    "    if self_similarity.mean() > 0.7:\n",
    "        print(f\"\\nâœ“ STRONG alignment: Audio and lyrics are highly similar\")\n",
    "    elif self_similarity.mean() > 0.5:\n",
    "        print(f\"\\nâœ“ MODERATE alignment: Some similarity between audio and lyrics\")\n",
    "    else:\n",
    "        print(f\"\\n! WEAK alignment: Audio and lyrics encode different information\")\n",
    "    \n",
    "    return audio_sim, lyrics_sim, cross_modal_sim, self_similarity\n",
    "\n",
    "# Run analysis\n",
    "audio_sim, lyrics_sim, cross_modal_sim, self_sim = compute_cosine_similarity_analysis(\n",
    "    features_dict['audio_features'],\n",
    "    features_dict['lyrics_features']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Line 1: Plot audio similarity\n",
    "im1 = axes[0].imshow(audio_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "axes[0].set_title('Audio-to-Audio Similarity', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Song Index')\n",
    "axes[0].set_ylabel('Song Index')\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Line 2: Plot lyrics similarity\n",
    "im2 = axes[1].imshow(lyrics_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "axes[1].set_title('Lyrics-to-Lyrics Similarity', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Song Index')\n",
    "axes[1].set_ylabel('Song Index')\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Line 3: Plot cross-modal similarity (KEY PLOT)\n",
    "im3 = axes[2].imshow(cross_modal_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "axes[2].set_title('Audio-to-Lyrics Cross-Modal Similarity', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Lyrics Index')\n",
    "axes[2].set_ylabel('Audio Index')\n",
    "plt.colorbar(im3, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Line 4: Plot histogram of self-similarity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(self_sim, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(self_sim.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {self_sim.mean():.3f}')\n",
    "plt.xlabel('Cosine Similarity (audio vs own lyrics)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Self-Similarity Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 2: Canonical Correlation Analysis (CCA)\n",
    "\n",
    "**What it does**: Finds linear transformations that maximize correlation between audio and lyrics\n",
    "\n",
    "**Key insight**: Discovers if there are shared latent dimensions (e.g., \"energy\" encoded in both modalities)\n",
    "\n",
    "**Interpretation**:\n",
    "- High canonical correlations (>0.7): Strong shared structure\n",
    "- Moderate (0.5-0.7): Some shared dimensions\n",
    "- Low (<0.5): Modalities are complementary, not redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cca_analysis(audio_features, lyrics_features, n_components=10):\n",
    "    \"\"\"\n",
    "    Canonical Correlation Analysis between audio and lyrics.\n",
    "    \"\"\"\n",
    "    # Line 1: Initialize CCA\n",
    "    # n_components: how many shared dimensions to find\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    \n",
    "    # Line 2: Fit CCA to learn transformations\n",
    "    # Finds W_audio and W_lyrics that maximize correlation\n",
    "    cca.fit(audio_features, lyrics_features)\n",
    "    \n",
    "    # Line 3: Transform features to canonical space\n",
    "    audio_canonical, lyrics_canonical = cca.transform(audio_features, lyrics_features)\n",
    "    \n",
    "    # Line 4: Compute correlation for each canonical component\n",
    "    correlations = []\n",
    "    for i in range(n_components):\n",
    "        # Pearson correlation between i-th canonical dimension\n",
    "        corr, p_value = pearsonr(audio_canonical[:, i], lyrics_canonical[:, i])\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Line 5: Print results\n",
    "    print(\"=\"*70)\n",
    "    print(\"CANONICAL CORRELATION ANALYSIS (CCA)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nCanonical correlations (n={n_components}):\")\n",
    "    for i, corr in enumerate(correlations):\n",
    "        print(f\"  Component {i+1}: {corr:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSummary statistics:\")\n",
    "    print(f\"  Mean correlation: {correlations.mean():.4f}\")\n",
    "    print(f\"  Max correlation:  {correlations.max():.4f}\")\n",
    "    print(f\"  Std:              {correlations.std():.4f}\")\n",
    "    \n",
    "    # Line 6: Interpretation\n",
    "    if correlations[0] > 0.7:\n",
    "        print(f\"\\nâœ“ STRONG shared structure: First component correlation = {correlations[0]:.3f}\")\n",
    "    elif correlations[0] > 0.5:\n",
    "        print(f\"\\nâœ“ MODERATE shared structure: Some shared latent dimensions\")\n",
    "    else:\n",
    "        print(f\"\\n! LIMITED shared structure: Modalities may be complementary\")\n",
    "    \n",
    "    return cca, correlations, audio_canonical, lyrics_canonical\n",
    "\n",
    "# Run CCA\n",
    "cca_model, cca_corrs, audio_can, lyrics_can = perform_cca_analysis(\n",
    "    features_dict['audio_features'],\n",
    "    features_dict['lyrics_features'],\n",
    "    n_components=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CCA results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Line 1: Bar chart of canonical correlations\n",
    "axes[0].bar(range(1, len(cca_corrs) + 1), cca_corrs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Moderate threshold (0.5)')\n",
    "axes[0].axhline(y=0.7, color='darkred', linestyle='--', linewidth=2, label='Strong threshold (0.7)')\n",
    "axes[0].set_xlabel('Canonical Component', fontsize=12)\n",
    "axes[0].set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "axes[0].set_title('Canonical Correlations', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Line 2: Scatter plot of first canonical component\n",
    "axes[1].scatter(audio_can[:, 0], lyrics_can[:, 0], alpha=0.6, s=50, color='purple', edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Audio Canonical Component 1', fontsize=12)\n",
    "axes[1].set_ylabel('Lyrics Canonical Component 1', fontsize=12)\n",
    "axes[1].set_title(f'First Canonical Component (r={cca_corrs[0]:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation line\n",
    "z = np.polyfit(audio_can[:, 0], lyrics_can[:, 0], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(audio_can[:, 0], p(audio_can[:, 0]), \"r--\", linewidth=2, label='Linear fit')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 3: Cross-Modal Retrieval\n",
    "\n",
    "**What it does**: Tests if audio features can retrieve matching lyrics (and vice versa)\n",
    "\n",
    "**Key insight**: Measures how well one modality predicts the other\n",
    "\n",
    "**Interpretation**:\n",
    "- High Top-5 accuracy (>50%): Strong predictive relationship\n",
    "- Moderate (20-50%): Some predictability\n",
    "- Low (<20%): One modality doesn't predict the other well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_modal_retrieval_analysis(audio_features, lyrics_features, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform cross-modal retrieval task.\n",
    "    \"\"\"\n",
    "    # Line 1: Compute cross-modal similarity matrix\n",
    "    sim_matrix = cosine_similarity(audio_features, lyrics_features)\n",
    "    n_samples = len(audio_features)\n",
    "    \n",
    "    # Line 2: Audio â†’ Lyrics retrieval\n",
    "    # For each audio, find top-k most similar lyrics\n",
    "    audio_to_lyrics_top_k = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    # Line 3: Check if correct match is in top-k\n",
    "    audio_to_lyrics_hits = []\n",
    "    for i in range(n_samples):\n",
    "        if i in audio_to_lyrics_top_k[i]:\n",
    "            audio_to_lyrics_hits.append(1)\n",
    "        else:\n",
    "            audio_to_lyrics_hits.append(0)\n",
    "    \n",
    "    audio_to_lyrics_acc = np.mean(audio_to_lyrics_hits)\n",
    "    \n",
    "    # Line 4: Lyrics â†’ Audio retrieval\n",
    "    lyrics_to_audio_top_k = np.argsort(sim_matrix.T, axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    lyrics_to_audio_hits = []\n",
    "    for i in range(n_samples):\n",
    "        if i in lyrics_to_audio_top_k[i]:\n",
    "            lyrics_to_audio_hits.append(1)\n",
    "        else:\n",
    "            lyrics_to_audio_hits.append(0)\n",
    "    \n",
    "    lyrics_to_audio_acc = np.mean(lyrics_to_audio_hits)\n",
    "    \n",
    "    # Line 5: Also compute Top-1 and Top-10 accuracy\n",
    "    # Top-1 (exact match)\n",
    "    audio_to_lyrics_top1 = np.argmax(sim_matrix, axis=1)\n",
    "    top1_acc = np.mean(audio_to_lyrics_top1 == np.arange(n_samples))\n",
    "    \n",
    "    # Top-10\n",
    "    if n_samples >= 10:\n",
    "        audio_to_lyrics_top_10 = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :10]\n",
    "        top10_hits = [i in audio_to_lyrics_top_10[i] for i in range(n_samples)]\n",
    "        top10_acc = np.mean(top10_hits)\n",
    "    else:\n",
    "        top10_acc = None\n",
    "    \n",
    "    # Line 6: Print results\n",
    "    print(\"=\"*70)\n",
    "    print(\"CROSS-MODAL RETRIEVAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n1. RETRIEVAL ACCURACY:\")\n",
    "    print(f\"   Audio â†’ Lyrics (Top-{top_k}): {audio_to_lyrics_acc:.2%}\")\n",
    "    print(f\"   Lyrics â†’ Audio (Top-{top_k}): {lyrics_to_audio_acc:.2%}\")\n",
    "    \n",
    "    print(f\"\\n2. ADDITIONAL METRICS:\")\n",
    "    print(f\"   Top-1 accuracy (exact match):  {top1_acc:.2%}\")\n",
    "    if top10_acc:\n",
    "        print(f\"   Top-10 accuracy:               {top10_acc:.2%}\")\n",
    "    \n",
    "    # Line 7: Interpretation\n",
    "    print(f\"\\n3. INTERPRETATION:\")\n",
    "    if audio_to_lyrics_acc > 0.5:\n",
    "        print(f\"   âœ“ GOOD alignment: Audio features predict matching lyrics well\")\n",
    "    elif audio_to_lyrics_acc > 0.2:\n",
    "        print(f\"   âœ“ MODERATE alignment: Some predictive power\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK alignment: Limited cross-modal predictability\")\n",
    "    \n",
    "    print(f\"\\n   Meaning: {audio_to_lyrics_acc:.1%} of the time, given a song's audio,\")\n",
    "    print(f\"   the correct lyrics are in the top-{top_k} most similar lyrics.\")\n",
    "    \n",
    "    return {\n",
    "        'audio_to_lyrics_acc': audio_to_lyrics_acc,\n",
    "        'lyrics_to_audio_acc': lyrics_to_audio_acc,\n",
    "        'top1_acc': top1_acc,\n",
    "        'top10_acc': top10_acc\n",
    "    }\n",
    "\n",
    "# Run retrieval analysis\n",
    "retrieval_results = cross_modal_retrieval_analysis(\n",
    "    features_dict['audio_features'],\n",
    "    features_dict['lyrics_features'],\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize retrieval accuracy at different k values\n",
    "k_values = [1, 2, 3, 5, 10, 20]\n",
    "accuracies = []\n",
    "\n",
    "sim_matrix = cosine_similarity(features_dict['audio_features'], features_dict['lyrics_features'])\n",
    "n_samples = len(features_dict['audio_features'])\n",
    "\n",
    "# Line 1: Compute accuracy for different k values\n",
    "for k in k_values:\n",
    "    if k <= n_samples:\n",
    "        top_k_indices = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :k]\n",
    "        hits = [i in top_k_indices[i] for i in range(n_samples)]\n",
    "        accuracies.append(np.mean(hits))\n",
    "    else:\n",
    "        accuracies.append(None)\n",
    "\n",
    "# Line 2: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "valid_k = [k for k, acc in zip(k_values, accuracies) if acc is not None]\n",
    "valid_acc = [acc for acc in accuracies if acc is not None]\n",
    "\n",
    "plt.plot(valid_k, valid_acc, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.xlabel('Top-K', fontsize=12)\n",
    "plt.ylabel('Retrieval Accuracy', fontsize=12)\n",
    "plt.title('Cross-Modal Retrieval Accuracy (Audio â†’ Lyrics)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for k, acc in zip(valid_k, valid_acc):\n",
    "    plt.text(k, acc + 0.03, f'{acc:.2%}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Report\n",
    "\n",
    "Comprehensive summary of all three similarity analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(features_dict, self_sim, cca_corrs, retrieval_results):\n",
    "    \"\"\"\n",
    "    Generate final summary report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\" \"*25 + \"COMPREHENSIVE SIMILARITY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š DATASET SUMMARY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Total songs analyzed: {len(features_dict['song_ids'])}\")\n",
    "    print(f\"   Audio feature dim:    {features_dict['audio_features'].shape[1]}\")\n",
    "    print(f\"   Lyrics feature dim:   {features_dict['lyrics_features'].shape[1]}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ METHOD 1: COSINE SIMILARITY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Self-similarity (audio vs own lyrics):\")\n",
    "    print(f\"     Mean: {self_sim.mean():.4f}\")\n",
    "    print(f\"     Std:  {self_sim.std():.4f}\")\n",
    "    \n",
    "    if self_sim.mean() > 0.7:\n",
    "        print(f\"   âœ“ STRONG alignment between audio and lyrics\")\n",
    "    elif self_sim.mean() > 0.5:\n",
    "        print(f\"   âœ“ MODERATE alignment\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK alignment - modalities encode different aspects\")\n",
    "    \n",
    "    print(f\"\\nðŸ”— METHOD 2: CANONICAL CORRELATION ANALYSIS\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Top 3 canonical correlations:\")\n",
    "    for i in range(min(3, len(cca_corrs))):\n",
    "        print(f\"     Component {i+1}: {cca_corrs[i]:.4f}\")\n",
    "    \n",
    "    if cca_corrs[0] > 0.7:\n",
    "        print(f\"   âœ“ STRONG shared latent structure\")\n",
    "    elif cca_corrs[0] > 0.5:\n",
    "        print(f\"   âœ“ MODERATE shared structure\")\n",
    "    else:\n",
    "        print(f\"   ! LIMITED shared structure - complementary modalities\")\n",
    "    \n",
    "    print(f\"\\nðŸ” METHOD 3: CROSS-MODAL RETRIEVAL\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Audio â†’ Lyrics (Top-5): {retrieval_results['audio_to_lyrics_acc']:.2%}\")\n",
    "    print(f\"   Lyrics â†’ Audio (Top-5): {retrieval_results['lyrics_to_audio_acc']:.2%}\")\n",
    "    print(f\"   Top-1 exact match:      {retrieval_results['top1_acc']:.2%}\")\n",
    "    \n",
    "    if retrieval_results['audio_to_lyrics_acc'] > 0.5:\n",
    "        print(f\"   âœ“ GOOD cross-modal predictability\")\n",
    "    elif retrieval_results['audio_to_lyrics_acc'] > 0.2:\n",
    "        print(f\"   âœ“ MODERATE predictability\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK predictability\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ OVERALL CONCLUSION\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    scores = [\n",
    "        self_sim.mean(),\n",
    "        cca_corrs[0],\n",
    "        retrieval_results['audio_to_lyrics_acc']\n",
    "    ]\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    if avg_score > 0.6:\n",
    "        print(f\"   The audio and lyrics features show STRONG similarity and alignment.\")\n",
    "        print(f\"   They encode related semantic information and have predictive power.\")\n",
    "    elif avg_score > 0.4:\n",
    "        print(f\"   The audio and lyrics features show MODERATE similarity.\")\n",
    "        print(f\"   They share some structure but also encode unique information.\")\n",
    "    else:\n",
    "        print(f\"   The audio and lyrics features show LIMITED similarity.\")\n",
    "        print(f\"   They appear to encode COMPLEMENTARY rather than redundant information.\")\n",
    "        print(f\"   This suggests both modalities contribute unique value to emotion prediction.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate report\n",
    "generate_comprehensive_report(features_dict, self_sim, cca_corrs, retrieval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "import os\n",
    "\n",
    "# Line 1: Create output directory\n",
    "output_dir = '/content/drive/MyDrive/dissertation/similarity_analysis_results/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Line 2: Save similarity matrices\n",
    "np.save(os.path.join(output_dir, 'audio_similarity_matrix.npy'), audio_sim)\n",
    "np.save(os.path.join(output_dir, 'lyrics_similarity_matrix.npy'), lyrics_sim)\n",
    "np.save(os.path.join(output_dir, 'cross_modal_similarity_matrix.npy'), cross_modal_sim)\n",
    "\n",
    "# Line 3: Save CCA results\n",
    "np.save(os.path.join(output_dir, 'cca_correlations.npy'), cca_corrs)\n",
    "np.save(os.path.join(output_dir, 'audio_canonical.npy'), audio_can)\n",
    "np.save(os.path.join(output_dir, 'lyrics_canonical.npy'), lyrics_can)\n",
    "\n",
    "# Line 4: Save extracted features\n",
    "np.save(os.path.join(output_dir, 'audio_features.npy'), features_dict['audio_features'])\n",
    "np.save(os.path.join(output_dir, 'lyrics_features.npy'), features_dict['lyrics_features'])\n",
    "\n",
    "# Line 5: Create summary CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'song_id': features_dict['song_ids'],\n",
    "    'self_similarity': self_sim,\n",
    "    'true_valence': features_dict['ground_truth'][:, 0],\n",
    "    'true_arousal': features_dict['ground_truth'][:, 1],\n",
    "    'pred_valence': features_dict['predictions'][:, 0],\n",
    "    'pred_arousal': features_dict['predictions'][:, 1]\n",
    "})\n",
    "results_df.to_csv(os.path.join(output_dir, 'similarity_summary.csv'), index=False)\n",
    "\n",
    "# Line 6: Save metrics summary\n",
    "metrics = {\n",
    "    'mean_self_similarity': float(self_sim.mean()),\n",
    "    'std_self_similarity': float(self_sim.std()),\n",
    "    'cca_correlation_1': float(cca_corrs[0]),\n",
    "    'cca_mean_correlation': float(cca_corrs.mean()),\n",
    "    'retrieval_audio_to_lyrics': float(retrieval_results['audio_to_lyrics_acc']),\n",
    "    'retrieval_lyrics_to_audio': float(retrieval_results['lyrics_to_audio_acc']),\n",
    "    'retrieval_top1': float(retrieval_results['top1_acc'])\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(output_dir, 'metrics_summary.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Results saved to: {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - audio_similarity_matrix.npy\")\n",
    "print(f\"  - lyrics_similarity_matrix.npy\")\n",
    "print(f\"  - cross_modal_similarity_matrix.npy\")\n",
    "print(f\"  - cca_correlations.npy\")\n",
    "print(f\"  - audio_canonical.npy\")\n",
    "print(f\"  - lyrics_canonical.npy\")\n",
    "print(f\"  - audio_features.npy\")\n",
    "print(f\"  - lyrics_features.npy\")\n",
    "print(f\"  - similarity_summary.csv\")\n",
    "print(f\"  - metrics_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# PART B: SIAMESE NETWORKS\n",
    "---\n",
    "---\n",
    "\n",
    "## What is a Siamese Network?\n",
    "\n",
    "A **Siamese network** is a neural network architecture designed to learn semantic similarity between inputs. It consists of:\n",
    "\n",
    "1. **Two identical subnetworks** (shared weights) that process each modality\n",
    "2. **A similarity metric** (e.g., distance or dot product) between the outputs\n",
    "3. **Contrastive or triplet loss** that trains the network to:\n",
    "   - Pull together matching pairs (audio_i, lyrics_i)\n",
    "   - Push apart non-matching pairs (audio_i, lyrics_j where iâ‰ j)\n",
    "\n",
    "### Architecture Diagram:\n",
    "\n",
    "```\n",
    "Audio Input                    Lyrics Input\n",
    "     â†“                              â†“\n",
    "[Spectrogram]                [Tokenized Text]\n",
    "     â†“                              â†“\n",
    "[Audio Encoder]              [Lyrics Encoder]\n",
    "(VGGish/CNN)                 (BERT/Transformer)\n",
    "     â†“                              â†“\n",
    "[Audio Embedding]            [Lyrics Embedding]\n",
    "(Shared dim: e.g., 256)      (Shared dim: e.g., 256)\n",
    "     â†“                              â†“\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€ Distance/Similarity â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "              [Loss Function]\n",
    "          (Contrastive or Triplet)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## A) What Will a Siamese Network Tell You?\n",
    "\n",
    "### Compared to your current similarity analysis:\n",
    "\n",
    "| Method | What it tells you |\n",
    "|--------|------------------|\n",
    "| **Cosine Similarity** | Uses features from models trained for **emotion prediction** | \n",
    "| **CCA** | Finds linear correlations in **existing** feature spaces |\n",
    "| **Siamese Network** | Learns a **new embedding space** explicitly optimized for cross-modal similarity |\n",
    "\n",
    "### Specific insights from Siamese networks:\n",
    "\n",
    "1. **Learned semantic similarity**: Instead of using emotion-prediction features, the network learns what \"similar\" means for audio-lyrics pairs\n",
    "\n",
    "2. **Improved retrieval**: After training, audioâ†’lyrics retrieval should be much better because the model is explicitly trained for this task\n",
    "\n",
    "3. **Embedding quality**: You can visualize the learned embedding space (using t-SNE or UMAP) to see if matching audio-lyrics pairs cluster together\n",
    "\n",
    "4. **Transferability**: The learned embeddings can be used for:\n",
    "   - Song recommendation (find songs with similar audio-lyrics characteristics)\n",
    "   - Anomaly detection (find songs where audio and lyrics don't match)\n",
    "   - Zero-shot learning (predict properties of new songs)\n",
    "\n",
    "5. **What's different from current approach**:\n",
    "   - **Current**: Your features are optimized for valence/arousal prediction\n",
    "   - **Siamese**: Features are optimized for audio-lyrics similarity\n",
    "   - These may capture different aspects of the songs!\n",
    "\n",
    "---\n",
    "\n",
    "## B) How to Implement a Siamese Network\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "#### Step 1: Design the Architecture\n",
    "\n",
    "You need:\n",
    "1. **Audio encoder**: Your VGGish model (reuse existing architecture)\n",
    "2. **Lyrics encoder**: Your BERT model (reuse existing architecture)\n",
    "3. **Projection heads**: Map both to same dimension (e.g., 256)\n",
    "4. **Loss function**: Contrastive loss or triplet loss\n",
    "\n",
    "```python\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, audio_encoder, lyrics_encoder, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Line 1: Reuse your existing encoders\n",
    "        self.audio_encoder = audio_encoder  # VGGish â†’ [batch, 64]\n",
    "        self.lyrics_encoder = lyrics_encoder  # BERT â†’ [batch, 768]\n",
    "        \n",
    "        # Line 2: Projection heads to shared embedding space\n",
    "        self.audio_projection = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.lyrics_projection = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, audio, lyrics_ids, lyrics_mask):\n",
    "        # Line 3: Extract features\n",
    "        audio_feat = self.audio_encoder(audio)  # [B, 64]\n",
    "        lyrics_feat = self.lyrics_encoder(lyrics_ids, lyrics_mask)  # [B, 768]\n",
    "        \n",
    "        # Line 4: Project to shared embedding space\n",
    "        audio_emb = self.audio_projection(audio_feat)  # [B, 256]\n",
    "        lyrics_emb = self.lyrics_projection(lyrics_feat)  # [B, 256]\n",
    "        \n",
    "        # Line 5: L2 normalize for cosine similarity\n",
    "        audio_emb = F.normalize(audio_emb, p=2, dim=1)\n",
    "        lyrics_emb = F.normalize(lyrics_emb, p=2, dim=1)\n",
    "        \n",
    "        return audio_emb, lyrics_emb\n",
    "```\n",
    "\n",
    "#### Step 2: Choose a Loss Function\n",
    "\n",
    "**Option 1: Contrastive Loss (Simpler)**\n",
    "\n",
    "```python\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, audio_emb, lyrics_emb, labels):\n",
    "        # Line 1: Compute pairwise distances\n",
    "        # labels: 1 if matching pair, 0 if not\n",
    "        distances = F.pairwise_distance(audio_emb, lyrics_emb)\n",
    "        \n",
    "        # Line 2: Contrastive loss\n",
    "        # For matching pairs: minimize distance\n",
    "        # For non-matching pairs: maximize distance (up to margin)\n",
    "        loss = labels * distances.pow(2) + \\\n",
    "               (1 - labels) * F.relu(self.margin - distances).pow(2)\n",
    "        \n",
    "        return loss.mean()\n",
    "```\n",
    "\n",
    "**Option 2: Triplet Loss (Better performance)**\n",
    "\n",
    "```python\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Line 1: Compute distances\n",
    "        # anchor: audio_i\n",
    "        # positive: lyrics_i (matching)\n",
    "        # negative: lyrics_j (non-matching)\n",
    "        pos_dist = F.pairwise_distance(anchor, positive)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        # Line 2: Triplet loss\n",
    "        # Want: distance(anchor, positive) + margin < distance(anchor, negative)\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        \n",
    "        return loss.mean()\n",
    "```\n",
    "\n",
    "**Option 3: InfoNCE Loss (State-of-the-art, used in CLIP)**\n",
    "\n",
    "```python\n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, audio_emb, lyrics_emb):\n",
    "        # Line 1: Compute similarity matrix\n",
    "        # [batch_size, batch_size]\n",
    "        logits = torch.matmul(audio_emb, lyrics_emb.T) / self.temperature\n",
    "        \n",
    "        # Line 2: Create labels (diagonal = positive pairs)\n",
    "        batch_size = audio_emb.size(0)\n",
    "        labels = torch.arange(batch_size, device=audio_emb.device)\n",
    "        \n",
    "        # Line 3: Cross-entropy loss (both directions)\n",
    "        loss_audio = F.cross_entropy(logits, labels)\n",
    "        loss_lyrics = F.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        return (loss_audio + loss_lyrics) / 2\n",
    "```\n",
    "\n",
    "**RECOMMENDATION**: Use **InfoNCE loss** (Option 3) - it's the most effective and easiest to implement.\n",
    "\n",
    "#### Step 3: Training Loop\n",
    "\n",
    "```python\n",
    "# Line 1: Initialize model\n",
    "siamese_model = SiameseNetwork(audio_encoder, lyrics_encoder, embedding_dim=256)\n",
    "criterion = InfoNCELoss(temperature=0.07)\n",
    "optimizer = torch.optim.Adam(siamese_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Line 2: Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        audio, lyrics_ids, lyrics_mask = batch\n",
    "        \n",
    "        # Line 3: Forward pass\n",
    "        audio_emb, lyrics_emb = siamese_model(audio, lyrics_ids, lyrics_mask)\n",
    "        \n",
    "        # Line 4: Compute loss\n",
    "        loss = criterion(audio_emb, lyrics_emb)\n",
    "        \n",
    "        # Line 5: Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "#### Step 4: Evaluation\n",
    "\n",
    "```python\n",
    "# Line 1: Extract embeddings for test set\n",
    "siamese_model.eval()\n",
    "with torch.no_grad():\n",
    "    audio_embeddings = []\n",
    "    lyrics_embeddings = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        audio_emb, lyrics_emb = siamese_model(audio, lyrics_ids, lyrics_mask)\n",
    "        audio_embeddings.append(audio_emb.cpu())\n",
    "        lyrics_embeddings.append(lyrics_emb.cpu())\n",
    "\n",
    "# Line 2: Compute retrieval accuracy\n",
    "similarity_matrix = cosine_similarity(audio_embeddings, lyrics_embeddings)\n",
    "top_k_acc = compute_retrieval_accuracy(similarity_matrix, k=5)\n",
    "\n",
    "print(f\"Retrieval accuracy: {top_k_acc:.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## C) Implementation Complexity Assessment\n",
    "\n",
    "### Effort Estimation: **MODERATE** (2-3 days of work)\n",
    "\n",
    "#### What you can REUSE from your current code:\n",
    "\n",
    "âœ… **Easy to reuse**:\n",
    "1. **Data loading**: Your existing dataloader (spectrograms + lyrics)\n",
    "2. **Audio encoder**: Your VGGish_Audio_Model architecture\n",
    "3. **Lyrics encoder**: Your BERT model\n",
    "4. **Preprocessing**: All your spectrogram and tokenization code\n",
    "5. **Training infrastructure**: Device management, saving/loading models\n",
    "\n",
    "#### What you need to ADD:\n",
    "\n",
    "ðŸ“ **New code required** (~300-400 lines):\n",
    "1. **SiameseNetwork class**: ~50 lines\n",
    "   - Wrapper around your existing encoders\n",
    "   - Add projection heads (2 small MLPs)\n",
    "\n",
    "2. **Loss function**: ~20 lines\n",
    "   - InfoNCE loss is simplest and best\n",
    "\n",
    "3. **Modified training loop**: ~100 lines\n",
    "   - Different loss computation\n",
    "   - No emotion prediction head needed\n",
    "\n",
    "4. **Evaluation code**: ~50 lines\n",
    "   - Retrieval accuracy computation\n",
    "   - Embedding visualization (t-SNE/UMAP)\n",
    "\n",
    "5. **Data augmentation** (optional but recommended): ~50 lines\n",
    "   - Audio: time masking, frequency masking\n",
    "   - Lyrics: random word masking\n",
    "\n",
    "#### Detailed Complexity Breakdown:\n",
    "\n",
    "| Task | Difficulty | Time | Notes |\n",
    "|------|-----------|------|-------|\n",
    "| **1. Architecture setup** | Easy | 2-3 hours | Mostly copy-paste from current model |\n",
    "| **2. Loss function** | Easy | 1 hour | Use InfoNCE (simple implementation) |\n",
    "| **3. Modify dataloader** | Easy | 1-2 hours | Ensure batches have positive pairs |\n",
    "| **4. Training loop** | Medium | 3-4 hours | Different from classification loop |\n",
    "| **5. Evaluation** | Medium | 2-3 hours | Retrieval metrics + visualization |\n",
    "| **6. Debugging** | Medium | 3-5 hours | Checking convergence, tuning hyperparams |\n",
    "| **7. Experiments** | Low | 4-6 hours | Training on full dataset |\n",
    "| **Total** | **MODERATE** | **16-24 hours** | ~2-3 days of focused work |\n",
    "\n",
    "#### Key Decisions:\n",
    "\n",
    "1. **Freeze encoders or fine-tune?**\n",
    "   - **Freeze** (easier, faster): Use pretrained features, only train projection heads\n",
    "   - **Fine-tune** (better, slower): Update entire network\n",
    "   - **Recommendation**: Start frozen, then try fine-tuning\n",
    "\n",
    "2. **Embedding dimension?**\n",
    "   - Typical: 128, 256, or 512\n",
    "   - **Recommendation**: 256 (good balance)\n",
    "\n",
    "3. **Batch size?**\n",
    "   - Larger is better for contrastive learning (more negative pairs)\n",
    "   - **Recommendation**: 64 or 128 if GPU memory allows\n",
    "\n",
    "4. **Temperature (for InfoNCE)?**\n",
    "   - Range: 0.05 - 0.1\n",
    "   - **Recommendation**: 0.07 (from CLIP paper)\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Implementation Code Structure\n",
    "\n",
    "Here's the full file structure you'd need:\n",
    "\n",
    "```\n",
    "siamese_network/\n",
    "â”œâ”€â”€ model.py              # SiameseNetwork class (~50 lines)\n",
    "â”œâ”€â”€ loss.py               # InfoNCE loss (~20 lines)\n",
    "â”œâ”€â”€ train.py              # Training loop (~100 lines)\n",
    "â”œâ”€â”€ evaluate.py           # Evaluation metrics (~50 lines)\n",
    "â”œâ”€â”€ dataset.py            # REUSE your existing dataloader\n",
    "â””â”€â”€ main.py               # Entry point (~30 lines)\n",
    "```\n",
    "\n",
    "**Total new code**: ~250 lines (not counting reused components)\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: Current Approach vs Siamese Network\n",
    "\n",
    "| Aspect | Current (Similarity Metrics) | Siamese Network |\n",
    "|--------|------------------------------|------------------|\n",
    "| **Purpose** | Analyze existing features | Learn similarity-optimized features |\n",
    "| **Training** | No additional training needed | Requires training (~4-6 hours) |\n",
    "| **Retrieval accuracy** | Baseline (e.g., 20-40%) | Improved (e.g., 60-80%) |\n",
    "| **Interpretability** | High (using known features) | Medium (learned representations) |\n",
    "| **Computation** | Fast (just inference) | Moderate (needs training) |\n",
    "| **For dissertation** | Good for analysis | Good for novel contribution |\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "### Do BOTH:\n",
    "\n",
    "1. **First**: Run the similarity analysis (Part A of this notebook)\n",
    "   - Quick to execute\n",
    "   - Establishes baseline\n",
    "   - Good for dissertation analysis section\n",
    "\n",
    "2. **Then**: Implement Siamese network\n",
    "   - ~2-3 days of work\n",
    "   - Shows improved performance\n",
    "   - Strong contribution for dissertation\n",
    "   - Demonstrates ability to design novel architectures\n",
    "\n",
    "### For your dissertation:\n",
    "\n",
    "**Chapter structure**:\n",
    "- **Section 1**: \"Similarity Analysis of Learned Features\" (Part A)\n",
    "- **Section 2**: \"Learning Cross-Modal Embeddings with Siamese Networks\" (Part B)\n",
    "- **Section 3**: \"Comparative Analysis\" (Which approach works better?)\n",
    "\n",
    "This gives you a strong narrative: analyze existing features â†’ design better approach â†’ demonstrate improvement.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. âœ… Run Part A (similarity analysis) first\n",
    "2. Review results and interpret findings\n",
    "3. If interested in Siamese networks, I can provide:\n",
    "   - Complete implementation code\n",
    "   - Training script adapted to your data\n",
    "   - Evaluation code for retrieval metrics\n",
    "\n",
    "Would you like me to implement the Siamese network code?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
