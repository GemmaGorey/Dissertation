{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/Similarity_Analysis_Step_By_Step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis: Audio vs Lyrics Features\n",
    "\n",
    "This notebook performs similarity analysis between audio and lyrics features using your trained MODEL 4.\n",
    "\n",
    "## Methodology\n",
    "- Uses the same environment setup as MODEL 4\n",
    "- Uses the same variable names as MODEL 4\n",
    "- References preprocessed data from your dissertation folder\n",
    "\n",
    "## Code Markings\n",
    "- ‚úÖ **EXISTING CODE** = Same as MODEL 4 (copy-paste directly)\n",
    "- ‚≠ê **NEW CODE** = Similarity analysis code (you need to add this)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Environment Setup\n",
    "\n",
    "## ‚úÖ EXISTING CODE (Same as MODEL 4)\n",
    "\n",
    "This code is identical to MODEL 4 - just copy-paste it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Install condacolab\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Create environment.yml and build environment\n",
    "yaml_content = \"\"\"\n",
    "name: dissertation\n",
    "channels:\n",
    "  - pytorch\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pytorch=2.2.2\n",
    "  - torchvision=0.17.2\n",
    "  - torchaudio\n",
    "  - librosa\n",
    "  - numpy<2\n",
    "  - pandas\n",
    "  - jupyter\n",
    "  - wandb\n",
    "\"\"\"\n",
    "\n",
    "with open('environment.yml', 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"environment.yml file created successfully.\")\n",
    "print(\"\\nCreating environment\")\n",
    "\n",
    "!mamba env create -f environment.yml --quiet && echo -e \"\\n'dissertation' environment is ready to use.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 2: Import Libraries and Setup\n",
    "\n",
    "## ‚úÖ EXISTING CODE (Same as MODEL 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Clone GitHub repo and mount Google Drive\n",
    "print(\"‚è≥ Cloning GitHub repository...\")\n",
    "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
    "print(\"Repository cloned.\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Import standard libraries (from MODEL 4)\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "import torch.optim as optim\n",
    "import subprocess\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê NEW CODE - Additional imports for similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Additional imports for similarity analysis\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì Similarity analysis libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 3: Define Model Classes\n",
    "\n",
    "## ‚úÖ EXISTING CODE (Same as MODEL 4)\n",
    "\n",
    "These are the exact same classes from MODEL 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - MER_Dataset class (from MODEL 4)\n",
    "class MER_Dataset(Dataset):\n",
    "    \"\"\" Custom PyTorch Dataset for loading MER data. \"\"\"\n",
    "    def __init__(self, annotations_df, tokenizer):\n",
    "        \"\"\" Creation of the Dataset from the dataframe (predefined splits in MERGE dataset) \"\"\"\n",
    "        self.annotations = annotations_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function to return the total number of songs in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Function to get a song from the dataset.\n",
    "        \"\"\"\n",
    "        song_info = self.annotations.iloc[index]\n",
    "\n",
    "        spectrogram_path = song_info['spectrogram_path']\n",
    "        lyrics_path = song_info['lyrics_path']\n",
    "        valence = song_info['valence']\n",
    "        arousal = song_info['arousal']\n",
    "\n",
    "        # Change spectrogram into a tensor\n",
    "        spectrogram = np.load(spectrogram_path)\n",
    "        spectrogram_tensor = torch.from_numpy(spectrogram).float()\n",
    "        spectrogram_tensor = spectrogram_tensor.unsqueeze(0)  # Adding a \"channel\" dimension for CNN\n",
    "\n",
    "        # Load the lyric tokens\n",
    "        encoded_lyrics = torch.load(lyrics_path, weights_only=False)\n",
    "        input_ids = encoded_lyrics['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded_lyrics['attention_mask'].squeeze(0)\n",
    "\n",
    "        labels = torch.tensor([valence, arousal], dtype=torch.float32)\n",
    "\n",
    "        return spectrogram_tensor, input_ids, attention_mask, labels\n",
    "\n",
    "print(\"‚úì MER_Dataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - AttentionModule class (from MODEL 4)\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        '''\n",
    "        Attention mechanism to weight the importance of different features\n",
    "        '''\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 4),  # input is 64 will map to 16\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim // 4, feature_dim),  # reverts back to 64 from 16\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 64]\n",
    "        attention_weights = self.attention(x)  # [batch_size, 64]\n",
    "        weighted_features = x * attention_weights  # Element-wise multiplication\n",
    "        return weighted_features\n",
    "\n",
    "print(\"‚úì AttentionModule class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - VGGish_Audio_Model class (from MODEL 4)\n",
    "class VGGish_Audio_Model(nn.Module):\n",
    "    '''\n",
    "    A VGG-style model for the audio tower.\n",
    "    V1.2 implements true VGG-style blocks with multiple convolutions per block.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(VGGish_Audio_Model, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1 - 2 convolutions\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2 - 2 convolutions\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3 - 2 convolutions\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4 - 2 convolutions\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.attention = AttentionModule(256)\n",
    "        self.fc2 = nn.Linear(256, 64)  # Final feature vector size should be 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the features for the classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úì VGGish_Audio_Model class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - BimodalClassifier class (from MODEL 4)\n",
    "class BimodalClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The final bimodal model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BimodalClassifier, self).__init__()\n",
    "\n",
    "        # Initiate audio tower\n",
    "        self.audio_tower = VGGish_Audio_Model()\n",
    "\n",
    "        # Use transformer for lyrics (using bert base uncased)\n",
    "        self.lyrics_tower = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.lyrics_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define feature sizes\n",
    "        AUDIO_FEATURES_OUT = 64\n",
    "        LYRICS_FEATURES_OUT = 768\n",
    "        COMBINED_FEATURES = AUDIO_FEATURES_OUT + LYRICS_FEATURES_OUT\n",
    "\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=COMBINED_FEATURES, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=100, out_features=2)  # 2 Outputs for Valence and Arousal\n",
    "        )\n",
    "\n",
    "    def forward(self, x_audio, input_ids, attention_mask):\n",
    "        # Process audio input\n",
    "        audio_features = self.audio_tower(x_audio)\n",
    "\n",
    "        # Get lyric features\n",
    "        lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use the embedding of the [CLS] token as the feature vector for whole lyrics\n",
    "        lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Combine the features from both towers\n",
    "        combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
    "\n",
    "        # Pass the combined features to the final classifier head\n",
    "        output = self.classifier_head(combined_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"‚úì BimodalClassifier class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê NEW CODE - Modified BimodalClassifier to return intermediate features\n",
    "\n",
    "We need to extract audio and lyrics features BEFORE they're combined. This requires a small modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Add a method to extract features (modification to BimodalClassifier)\n",
    "# We'll add this method to the existing model after loading\n",
    "\n",
    "def get_features(self, x_audio, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Extract audio and lyrics features separately (before fusion).\n",
    "    Returns: (audio_features, lyrics_features, predictions)\n",
    "    \"\"\"\n",
    "    # Process audio input\n",
    "    audio_features = self.audio_tower(x_audio)  # [batch_size, 64]\n",
    "\n",
    "    # Get lyric features\n",
    "    lyrics_outputs = self.lyrics_tower(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    lyrics_features = lyrics_outputs.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
    "\n",
    "    # Combine features and get predictions\n",
    "    combined_features = torch.cat((audio_features, lyrics_features), dim=1)\n",
    "    predictions = self.classifier_head(combined_features)\n",
    "\n",
    "    return audio_features, lyrics_features, predictions\n",
    "\n",
    "# We'll add this method to the model after loading\n",
    "print(\"‚úì Feature extraction method defined (will be added to model later).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 4: Load Data\n",
    "\n",
    "## ‚úÖ EXISTING CODE (Same as MODEL 4)\n",
    "\n",
    "This loads data exactly as in MODEL 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Data loading (from MODEL 4)\n",
    "print(\"Starting data transfer from Google Drive to local Colab storage...\")\n",
    "\n",
    "# Get paths for old file location and new colab one\n",
    "gdrive_zip_path = '/content/drive/MyDrive/dissertation/merge_dataset_zipped.zip'\n",
    "local_storage_path = '/content/local_dissertation_data/'\n",
    "local_zip_path = os.path.join(local_storage_path, 'merge_dataset_zipped.zip')\n",
    "os.makedirs(local_storage_path, exist_ok=True)\n",
    "\n",
    "# Copy zip file from Drive to Colab\n",
    "print(\"Copying single archive file from Google Drive...\")\n",
    "!rsync -ah --progress \"{gdrive_zip_path}\" \"{local_storage_path}\"\n",
    "\n",
    "# Get total number of files for progress\n",
    "total_files = int(subprocess.check_output(f\"zipinfo -1 {local_zip_path} | wc -l\", shell=True))\n",
    "\n",
    "# Unzip the file\n",
    "print(\"Extracting files locally\")\n",
    "!unzip -o \"{local_zip_path}\" -d \"{local_storage_path}\" | tqdm --unit=files --total={total_files} > /dev/null\n",
    "\n",
    "print(\"Data transfer and extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Load master data and update paths (from MODEL 4)\n",
    "local_output_path = os.path.join(local_storage_path, 'merge_dataset/output_from_code/')\n",
    "master_file_path = os.path.join(local_output_path, 'master_processed_file_list.csv')\n",
    "master_df = pd.read_csv(master_file_path)\n",
    "\n",
    "# Checking the valence and arousal range in the dataset\n",
    "print(f\"\\nValence range in data: [{master_df['valence'].min()}, {master_df['valence'].max()}]\")\n",
    "print(f\"Arousal range in data: [{master_df['arousal'].min()}, {master_df['arousal'].max()}]\")\n",
    "print(f\"Valence mean: {master_df['valence'].mean():.4f}, std: {master_df['valence'].std():.4f}\")\n",
    "print(f\"Arousal mean: {master_df['arousal'].mean():.4f}, std: {master_df['arousal'].std():.4f}\")\n",
    "print(f\"Total samples in master_df: {len(master_df)}\")\n",
    "\n",
    "# Update the paths in the csv\n",
    "print(\"\\nUpdating dataframe paths to use fast local storage...\")\n",
    "gdrive_output_path = '/content/drive/MyDrive/dissertation/output_from_code/'\n",
    "master_df['spectrogram_path'] = master_df['spectrogram_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
    "master_df['lyrics_path'] = master_df['lyrics_path'].str.replace(gdrive_output_path, local_output_path, regex=False)\n",
    "print(\"Dataframe paths updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Load train/val/test splits (from MODEL 4)\n",
    "local_split_folder_path = os.path.join(local_storage_path, 'merge_dataset/MERGE_Bimodal_Complete/tvt_dataframes/tvt_70_15_15/')\n",
    "train_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_train_bimodal_complete.csv'))\n",
    "val_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_validate_bimodal_complete.csv'))\n",
    "test_split_df = pd.read_csv(os.path.join(local_split_folder_path, 'tvt_70_15_15_test_bimodal_complete.csv'))\n",
    "print(\"\\nSplit files loaded from local storage.\")\n",
    "\n",
    "# Merge the files\n",
    "id_column_name = 'song_id'\n",
    "train_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "val_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "test_split_df.rename(columns={'Song': id_column_name}, inplace=True)\n",
    "\n",
    "train_df = pd.merge(master_df, train_split_df, on=id_column_name)\n",
    "val_df = pd.merge(master_df, val_split_df, on=id_column_name)\n",
    "test_df = pd.merge(master_df, test_split_df, on=id_column_name)\n",
    "\n",
    "# Checking no files are lost in merging\n",
    "print(\"\\nChecking data\")\n",
    "\n",
    "if len(train_df) == len(train_split_df):\n",
    "    print(\"\\nTraining split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Training split lost {len(train_split_df) - len(train_df)} songs during merge.\")\n",
    "\n",
    "if len(val_df) == len(val_split_df):\n",
    "    print(\"Validation split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"WARNING: Validation split lost {len(val_split_df) - len(val_df)} songs during merge.\")\n",
    "\n",
    "if len(test_df) == len(test_split_df):\n",
    "    print(\"Test split: Merge successful. All songs accounted for.\")\n",
    "else:\n",
    "    print(f\"WARNING: Test split lost {len(test_split_df) - len(test_df)} songs during merge.\")\n",
    "\n",
    "# Check length\n",
    "expected_train_len = 1552\n",
    "expected_val_len = 332\n",
    "expected_test_len = 332\n",
    "\n",
    "assert len(train_df) == expected_train_len, f\"Expected {expected_train_len} training samples, but found {len(train_df)}\"\n",
    "assert len(val_df) == expected_val_len, f\"Expected {expected_val_len} validation samples, but found {len(val_df)}\"\n",
    "assert len(test_df) == expected_test_len, f\"Expected {expected_test_len} test samples, but found {len(test_df)}\"\n",
    "\n",
    "print(f\"\\nFinal dataset lengths are correct: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
    "print(\"Data Check Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê NEW CODE - Choose which dataset to analyze\n",
    "\n",
    "You can analyze train, validation, or test set. We'll use test set by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Select dataset for similarity analysis\n",
    "# Choose which split to analyze: train_df, val_df, or test_df\n",
    "# We use test_df by default (the same set used for MODEL 4 evaluation)\n",
    "\n",
    "analysis_df = test_df.copy()  # Change this to train_df or val_df if needed\n",
    "\n",
    "print(f\"\\n‚úì Selected dataset for similarity analysis: TEST SET\")\n",
    "print(f\"  Total songs to analyze: {len(analysis_df)}\")\n",
    "print(f\"  Song IDs: {analysis_df[id_column_name].head(10).tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Create datasets and dataloaders (from MODEL 4)\n",
    "train_dataset = MER_Dataset(annotations_df=train_df, tokenizer=tokenizer)\n",
    "val_dataset = MER_Dataset(annotations_df=val_df, tokenizer=tokenizer)\n",
    "test_dataset = MER_Dataset(annotations_df=test_df, tokenizer=tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 5: Load Trained Model\n",
    "\n",
    "## ‚úÖ EXISTING CODE (Similar to MODEL 4, but loading saved model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Check GPU availability (from MODEL 4)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA device.\")\n",
    "else:\n",
    "    raise RuntimeError(\"Error: No GPU found. This script requires a GPU to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Initialize model (from MODEL 4)\n",
    "model = BimodalClassifier()\n",
    "model.to(device)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ EXISTING CODE - Load trained model weights\n",
    "# This loads the model you saved in MODEL 4\n",
    "\n",
    "model_path = '/content/drive/MyDrive/dissertation/bimodal_regression_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úì Model loaded successfully from: {model_path}\")\n",
    "print(\"‚úì Model set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê NEW CODE - Add feature extraction method to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Add the get_features method to the loaded model\n",
    "import types\n",
    "\n",
    "# Add the method we defined earlier to the model instance\n",
    "model.get_features = types.MethodType(get_features, model)\n",
    "\n",
    "print(\"‚úì Feature extraction method added to model.\")\n",
    "print(\"‚úì Model is ready for similarity analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 6: Extract Features from Dataset\n",
    "\n",
    "## ‚≠ê NEW CODE - Extract audio and lyrics features\n",
    "\n",
    "This extracts features for all songs in the selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Function to extract features from the dataset\n",
    "\n",
    "def extract_features_from_dataset(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extract audio and lyrics features for all songs in the dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BimodalClassifier with get_features method\n",
    "        dataloader: DataLoader (test_loader, train_loader, or val_loader)\n",
    "        device: torch device (cuda or cpu)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - audio_features: [N, 64] numpy array\n",
    "        - lyrics_features: [N, 768] numpy array\n",
    "        - predictions: [N, 2] numpy array (valence, arousal)\n",
    "        - ground_truth: [N, 2] numpy array (true valence, arousal)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXTRACTING FEATURES FROM DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    audio_features_list = []\n",
    "    lyrics_features_list = []\n",
    "    predictions_list = []\n",
    "    ground_truth_list = []\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract features without computing gradients\n",
    "    with torch.no_grad():\n",
    "        for spectrogram_batch, input_ids_batch, attention_mask_batch, labels_batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            # Move data to device\n",
    "            spectrogram_batch = spectrogram_batch.to(device)\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "            \n",
    "            # Extract features using our new method\n",
    "            audio_feat, lyrics_feat, preds = model.get_features(\n",
    "                spectrogram_batch, \n",
    "                input_ids_batch, \n",
    "                attention_mask_batch\n",
    "            )\n",
    "            \n",
    "            # Move to CPU and convert to numpy\n",
    "            audio_features_list.append(audio_feat.cpu().numpy())\n",
    "            lyrics_features_list.append(lyrics_feat.cpu().numpy())\n",
    "            predictions_list.append(preds.cpu().numpy())\n",
    "            ground_truth_list.append(labels_batch.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    audio_features = np.concatenate(audio_features_list, axis=0)      # [N, 64]\n",
    "    lyrics_features = np.concatenate(lyrics_features_list, axis=0)    # [N, 768]\n",
    "    predictions = np.concatenate(predictions_list, axis=0)            # [N, 2]\n",
    "    ground_truth = np.concatenate(ground_truth_list, axis=0)          # [N, 2]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n‚úì Feature extraction complete!\")\n",
    "    print(f\"  Total songs processed: {len(audio_features)}\")\n",
    "    print(f\"  Audio features shape:  {audio_features.shape}\")\n",
    "    print(f\"  Lyrics features shape: {lyrics_features.shape}\")\n",
    "    print(f\"  Predictions shape:     {predictions.shape}\")\n",
    "    print(f\"  Ground truth shape:    {ground_truth.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'audio_features': audio_features,\n",
    "        'lyrics_features': lyrics_features,\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "\n",
    "print(\"‚úì Feature extraction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Extract features from test set\n",
    "# This will take a few minutes depending on dataset size\n",
    "\n",
    "features_dict = extract_features_from_dataset(model, test_loader, device)\n",
    "\n",
    "# Store variable names that match MODEL 4\n",
    "audio_features = features_dict['audio_features']\n",
    "lyrics_features = features_dict['lyrics_features']\n",
    "predictions = features_dict['predictions']\n",
    "ground_truth = features_dict['ground_truth']\n",
    "\n",
    "print(\"\\n‚úì Features stored in variables:\")\n",
    "print(\"  - audio_features\")\n",
    "print(\"  - lyrics_features\")\n",
    "print(\"  - predictions\")\n",
    "print(\"  - ground_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# SIMILARITY ANALYSIS SECTION\n",
    "---\n",
    "---\n",
    "\n",
    "All code below is ‚≠ê **NEW CODE** for similarity analysis.\n",
    "\n",
    "You now have:\n",
    "- `audio_features`: [332, 64] - Audio features for each song\n",
    "- `lyrics_features`: [332, 768] - Lyrics features for each song\n",
    "- `predictions`: [332, 2] - Predicted valence and arousal\n",
    "- `ground_truth`: [332, 2] - True valence and arousal\n",
    "\n",
    "We'll perform 3 similarity analyses:\n",
    "1. **Cosine Similarity**\n",
    "2. **Canonical Correlation Analysis (CCA)**\n",
    "3. **Cross-Modal Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# METHOD 1: Cosine Similarity\n",
    "\n",
    "## ‚≠ê NEW CODE\n",
    "\n",
    "**What it measures**: Angular similarity between feature vectors (-1 to 1)\n",
    "\n",
    "**Key metric**: Diagonal of cross-modal matrix = how similar is each song's audio to its OWN lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Compute cosine similarity\n",
    "\n",
    "def compute_cosine_similarity_analysis(audio_features, lyrics_features):\n",
    "    \"\"\"\n",
    "    Compute pairwise cosine similarities.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 1: COSINE SIMILARITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Audio-to-audio similarity [N, N]\n",
    "    # Entry [i,j] = similarity between audio_i and audio_j\n",
    "    audio_sim = cosine_similarity(audio_features, audio_features)\n",
    "    \n",
    "    # Lyrics-to-lyrics similarity [N, N]\n",
    "    lyrics_sim = cosine_similarity(lyrics_features, lyrics_features)\n",
    "    \n",
    "    # CROSS-MODAL similarity [N, N]\n",
    "    # Entry [i,j] = similarity between audio_i and lyrics_j\n",
    "    # KEY METRIC: Diagonal = audio_i vs lyrics_i (same song)\n",
    "    cross_modal_sim = cosine_similarity(audio_features, lyrics_features)\n",
    "    \n",
    "    # Extract diagonal (self-similarity)\n",
    "    self_similarity = np.diag(cross_modal_sim)\n",
    "    \n",
    "    # Extract off-diagonal (cross-song similarity)\n",
    "    mask = np.ones_like(cross_modal_sim, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    cross_song_sim = cross_modal_sim[mask]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n1. SELF-SIMILARITY (audio vs own lyrics):\")\n",
    "    print(f\"   Mean:  {self_similarity.mean():.4f}\")\n",
    "    print(f\"   Std:   {self_similarity.std():.4f}\")\n",
    "    print(f\"   Range: [{self_similarity.min():.4f}, {self_similarity.max():.4f}]\")\n",
    "    \n",
    "    print(f\"\\n2. CROSS-SONG SIMILARITY (audio_i vs lyrics_j, i‚â†j):\")\n",
    "    print(f\"   Mean:  {cross_song_sim.mean():.4f}\")\n",
    "    print(f\"   Std:   {cross_song_sim.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n3. WITHIN-MODALITY SIMILARITY:\")\n",
    "    print(f\"   Audio-to-audio mean:   {audio_sim[mask].mean():.4f}\")\n",
    "    print(f\"   Lyrics-to-lyrics mean: {lyrics_sim[mask].mean():.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n4. INTERPRETATION:\")\n",
    "    if self_similarity.mean() > 0.7:\n",
    "        print(f\"   ‚úì STRONG alignment: Audio and lyrics are highly similar\")\n",
    "    elif self_similarity.mean() > 0.5:\n",
    "        print(f\"   ‚úì MODERATE alignment: Some similarity between audio and lyrics\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK alignment: Audio and lyrics encode different information\")\n",
    "    \n",
    "    return audio_sim, lyrics_sim, cross_modal_sim, self_similarity\n",
    "\n",
    "# Run analysis\n",
    "audio_sim, lyrics_sim, cross_modal_sim, self_sim = compute_cosine_similarity_analysis(\n",
    "    audio_features,\n",
    "    lyrics_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Visualize similarity matrices\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot audio similarity\n",
    "im1 = axes[0].imshow(audio_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "axes[0].set_title('Audio-to-Audio Similarity', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Song Index')\n",
    "axes[0].set_ylabel('Song Index')\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Plot lyrics similarity\n",
    "im2 = axes[1].imshow(lyrics_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "axes[1].set_title('Lyrics-to-Lyrics Similarity', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Song Index')\n",
    "axes[1].set_ylabel('Song Index')\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Plot cross-modal similarity (KEY PLOT)\n",
    "im3 = axes[2].imshow(cross_modal_sim, cmap='coolwarm', vmin=0, vmax=1, aspect='auto')\n",
    "axes[2].set_title('Audio-to-Lyrics Cross-Modal Similarity', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Lyrics Index')\n",
    "axes[2].set_ylabel('Audio Index')\n",
    "plt.colorbar(im3, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of self-similarity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(self_sim, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(self_sim.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {self_sim.mean():.3f}')\n",
    "plt.xlabel('Cosine Similarity (audio vs own lyrics)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Self-Similarity Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# METHOD 2: Canonical Correlation Analysis (CCA)\n",
    "\n",
    "## ‚≠ê NEW CODE\n",
    "\n",
    "**What it does**: Finds linear transformations that maximize correlation\n",
    "\n",
    "**Key insight**: Discovers shared latent dimensions (e.g., \"energy\" in both modalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Perform CCA analysis\n",
    "\n",
    "def perform_cca_analysis(audio_features, lyrics_features, n_components=10):\n",
    "    \"\"\"\n",
    "    Canonical Correlation Analysis between audio and lyrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 2: CANONICAL CORRELATION ANALYSIS (CCA)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize CCA\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    \n",
    "    # Fit CCA to learn transformations\n",
    "    cca.fit(audio_features, lyrics_features)\n",
    "    \n",
    "    # Transform features to canonical space\n",
    "    audio_canonical, lyrics_canonical = cca.transform(audio_features, lyrics_features)\n",
    "    \n",
    "    # Compute correlation for each canonical component\n",
    "    correlations = []\n",
    "    for i in range(n_components):\n",
    "        corr, _ = pearsonr(audio_canonical[:, i], lyrics_canonical[:, i])\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nCanonical correlations (n={n_components}):\")\n",
    "    for i, corr in enumerate(correlations):\n",
    "        print(f\"  Component {i+1}: {corr:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSummary statistics:\")\n",
    "    print(f\"  Mean correlation: {correlations.mean():.4f}\")\n",
    "    print(f\"  Max correlation:  {correlations.max():.4f}\")\n",
    "    print(f\"  Std:              {correlations.std():.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\nINTERPRETATION:\")\n",
    "    if correlations[0] > 0.7:\n",
    "        print(f\"  ‚úì STRONG shared structure: First component correlation = {correlations[0]:.3f}\")\n",
    "    elif correlations[0] > 0.5:\n",
    "        print(f\"  ‚úì MODERATE shared structure: Some shared latent dimensions\")\n",
    "    else:\n",
    "        print(f\"  ! LIMITED shared structure: Modalities may be complementary\")\n",
    "    \n",
    "    return cca, correlations, audio_canonical, lyrics_canonical\n",
    "\n",
    "# Run CCA\n",
    "cca_model, cca_corrs, audio_can, lyrics_can = perform_cca_analysis(\n",
    "    audio_features,\n",
    "    lyrics_features,\n",
    "    n_components=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Visualize CCA results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of canonical correlations\n",
    "axes[0].bar(range(1, len(cca_corrs) + 1), cca_corrs, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Moderate (0.5)')\n",
    "axes[0].axhline(y=0.7, color='darkred', linestyle='--', linewidth=2, label='Strong (0.7)')\n",
    "axes[0].set_xlabel('Canonical Component', fontsize=12)\n",
    "axes[0].set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "axes[0].set_title('Canonical Correlations', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scatter plot of first canonical component\n",
    "axes[1].scatter(audio_can[:, 0], lyrics_can[:, 0], alpha=0.6, s=50, color='purple', edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Audio Canonical Component 1', fontsize=12)\n",
    "axes[1].set_ylabel('Lyrics Canonical Component 1', fontsize=12)\n",
    "axes[1].set_title(f'First Canonical Component (r={cca_corrs[0]:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation line\n",
    "z = np.polyfit(audio_can[:, 0], lyrics_can[:, 0], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(audio_can[:, 0], p(audio_can[:, 0]), \"r--\", linewidth=2, label='Linear fit')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# METHOD 3: Cross-Modal Retrieval\n",
    "\n",
    "## ‚≠ê NEW CODE\n",
    "\n",
    "**What it does**: Tests if audio features can retrieve matching lyrics\n",
    "\n",
    "**Key metric**: Top-K accuracy (% of times correct match is in top K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Cross-modal retrieval analysis\n",
    "\n",
    "def cross_modal_retrieval_analysis(audio_features, lyrics_features, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform cross-modal retrieval task.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"METHOD 3: CROSS-MODAL RETRIEVAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Compute cross-modal similarity matrix\n",
    "    sim_matrix = cosine_similarity(audio_features, lyrics_features)\n",
    "    n_samples = len(audio_features)\n",
    "    \n",
    "    # Audio ‚Üí Lyrics retrieval\n",
    "    audio_to_lyrics_top_k = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    # Check if correct match is in top-k\n",
    "    audio_to_lyrics_hits = []\n",
    "    for i in range(n_samples):\n",
    "        if i in audio_to_lyrics_top_k[i]:\n",
    "            audio_to_lyrics_hits.append(1)\n",
    "        else:\n",
    "            audio_to_lyrics_hits.append(0)\n",
    "    \n",
    "    audio_to_lyrics_acc = np.mean(audio_to_lyrics_hits)\n",
    "    \n",
    "    # Lyrics ‚Üí Audio retrieval\n",
    "    lyrics_to_audio_top_k = np.argsort(sim_matrix.T, axis=1)[:, ::-1][:, :top_k]\n",
    "    \n",
    "    lyrics_to_audio_hits = []\n",
    "    for i in range(n_samples):\n",
    "        if i in lyrics_to_audio_top_k[i]:\n",
    "            lyrics_to_audio_hits.append(1)\n",
    "        else:\n",
    "            lyrics_to_audio_hits.append(0)\n",
    "    \n",
    "    lyrics_to_audio_acc = np.mean(lyrics_to_audio_hits)\n",
    "    \n",
    "    # Top-1 (exact match)\n",
    "    audio_to_lyrics_top1 = np.argmax(sim_matrix, axis=1)\n",
    "    top1_acc = np.mean(audio_to_lyrics_top1 == np.arange(n_samples))\n",
    "    \n",
    "    # Top-10\n",
    "    if n_samples >= 10:\n",
    "        audio_to_lyrics_top_10 = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :10]\n",
    "        top10_hits = [i in audio_to_lyrics_top_10[i] for i in range(n_samples)]\n",
    "        top10_acc = np.mean(top10_hits)\n",
    "    else:\n",
    "        top10_acc = None\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n1. RETRIEVAL ACCURACY:\")\n",
    "    print(f\"   Audio ‚Üí Lyrics (Top-{top_k}): {audio_to_lyrics_acc:.2%}\")\n",
    "    print(f\"   Lyrics ‚Üí Audio (Top-{top_k}): {lyrics_to_audio_acc:.2%}\")\n",
    "    \n",
    "    print(f\"\\n2. ADDITIONAL METRICS:\")\n",
    "    print(f\"   Top-1 accuracy (exact match):  {top1_acc:.2%}\")\n",
    "    if top10_acc:\n",
    "        print(f\"   Top-10 accuracy:               {top10_acc:.2%}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n3. INTERPRETATION:\")\n",
    "    if audio_to_lyrics_acc > 0.5:\n",
    "        print(f\"   ‚úì GOOD alignment: Audio features predict matching lyrics well\")\n",
    "    elif audio_to_lyrics_acc > 0.2:\n",
    "        print(f\"   ‚úì MODERATE alignment: Some predictive power\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK alignment: Limited cross-modal predictability\")\n",
    "    \n",
    "    print(f\"\\n   Meaning: {audio_to_lyrics_acc:.1%} of the time, given a song's audio,\")\n",
    "    print(f\"   the correct lyrics are in the top-{top_k} most similar lyrics.\")\n",
    "    \n",
    "    return {\n",
    "        'audio_to_lyrics_acc': audio_to_lyrics_acc,\n",
    "        'lyrics_to_audio_acc': lyrics_to_audio_acc,\n",
    "        'top1_acc': top1_acc,\n",
    "        'top10_acc': top10_acc\n",
    "    }\n",
    "\n",
    "# Run retrieval analysis\n",
    "retrieval_results = cross_modal_retrieval_analysis(\n",
    "    audio_features,\n",
    "    lyrics_features,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Visualize retrieval accuracy at different k values\n",
    "\n",
    "k_values = [1, 2, 3, 5, 10, 20]\n",
    "accuracies = []\n",
    "\n",
    "sim_matrix = cosine_similarity(audio_features, lyrics_features)\n",
    "n_samples = len(audio_features)\n",
    "\n",
    "# Compute accuracy for different k values\n",
    "for k in k_values:\n",
    "    if k <= n_samples:\n",
    "        top_k_indices = np.argsort(sim_matrix, axis=1)[:, ::-1][:, :k]\n",
    "        hits = [i in top_k_indices[i] for i in range(n_samples)]\n",
    "        accuracies.append(np.mean(hits))\n",
    "    else:\n",
    "        accuracies.append(None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "valid_k = [k for k, acc in zip(k_values, accuracies) if acc is not None]\n",
    "valid_acc = [acc for acc in accuracies if acc is not None]\n",
    "\n",
    "plt.plot(valid_k, valid_acc, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "plt.xlabel('Top-K', fontsize=12)\n",
    "plt.ylabel('Retrieval Accuracy', fontsize=12)\n",
    "plt.title('Cross-Modal Retrieval Accuracy (Audio ‚Üí Lyrics)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for k, acc in zip(valid_k, valid_acc):\n",
    "    plt.text(k, acc + 0.03, f'{acc:.2%}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# COMPREHENSIVE SUMMARY REPORT\n",
    "\n",
    "## ‚≠ê NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Generate comprehensive summary report\n",
    "\n",
    "def generate_comprehensive_report(self_sim, cca_corrs, retrieval_results, analysis_df):\n",
    "    \"\"\"\n",
    "    Generate final summary report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\" \"*20 + \"COMPREHENSIVE SIMILARITY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä DATASET SUMMARY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Total songs analyzed: {len(analysis_df)}\")\n",
    "    print(f\"   Audio feature dim:    64\")\n",
    "    print(f\"   Lyrics feature dim:   768\")\n",
    "    \n",
    "    print(f\"\\nüìè METHOD 1: COSINE SIMILARITY\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Self-similarity (audio vs own lyrics):\")\n",
    "    print(f\"     Mean: {self_sim.mean():.4f}\")\n",
    "    print(f\"     Std:  {self_sim.std():.4f}\")\n",
    "    \n",
    "    if self_sim.mean() > 0.7:\n",
    "        print(f\"   ‚úì STRONG alignment between audio and lyrics\")\n",
    "    elif self_sim.mean() > 0.5:\n",
    "        print(f\"   ‚úì MODERATE alignment\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK alignment - modalities encode different aspects\")\n",
    "    \n",
    "    print(f\"\\nüîó METHOD 2: CANONICAL CORRELATION ANALYSIS\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Top 3 canonical correlations:\")\n",
    "    for i in range(min(3, len(cca_corrs))):\n",
    "        print(f\"     Component {i+1}: {cca_corrs[i]:.4f}\")\n",
    "    \n",
    "    if cca_corrs[0] > 0.7:\n",
    "        print(f\"   ‚úì STRONG shared latent structure\")\n",
    "    elif cca_corrs[0] > 0.5:\n",
    "        print(f\"   ‚úì MODERATE shared structure\")\n",
    "    else:\n",
    "        print(f\"   ! LIMITED shared structure - complementary modalities\")\n",
    "    \n",
    "    print(f\"\\nüîç METHOD 3: CROSS-MODAL RETRIEVAL\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"   Audio ‚Üí Lyrics (Top-5): {retrieval_results['audio_to_lyrics_acc']:.2%}\")\n",
    "    print(f\"   Lyrics ‚Üí Audio (Top-5): {retrieval_results['lyrics_to_audio_acc']:.2%}\")\n",
    "    print(f\"   Top-1 exact match:      {retrieval_results['top1_acc']:.2%}\")\n",
    "    \n",
    "    if retrieval_results['audio_to_lyrics_acc'] > 0.5:\n",
    "        print(f\"   ‚úì GOOD cross-modal predictability\")\n",
    "    elif retrieval_results['audio_to_lyrics_acc'] > 0.2:\n",
    "        print(f\"   ‚úì MODERATE predictability\")\n",
    "    else:\n",
    "        print(f\"   ! WEAK predictability\")\n",
    "    \n",
    "    print(f\"\\nüí° OVERALL CONCLUSION\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    scores = [\n",
    "        self_sim.mean(),\n",
    "        cca_corrs[0],\n",
    "        retrieval_results['audio_to_lyrics_acc']\n",
    "    ]\n",
    "    avg_score = np.mean(scores)\n",
    "    \n",
    "    if avg_score > 0.6:\n",
    "        print(f\"   The audio and lyrics features show STRONG similarity and alignment.\")\n",
    "        print(f\"   They encode related semantic information and have predictive power.\")\n",
    "    elif avg_score > 0.4:\n",
    "        print(f\"   The audio and lyrics features show MODERATE similarity.\")\n",
    "        print(f\"   They share some structure but also encode unique information.\")\n",
    "    else:\n",
    "        print(f\"   The audio and lyrics features show LIMITED similarity.\")\n",
    "        print(f\"   They appear to encode COMPLEMENTARY rather than redundant information.\")\n",
    "        print(f\"   This suggests both modalities contribute unique value to emotion prediction.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate report\n",
    "generate_comprehensive_report(self_sim, cca_corrs, retrieval_results, analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SAVE RESULTS\n",
    "\n",
    "## ‚≠ê NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚≠ê NEW CODE - Save all results to Google Drive\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/content/drive/MyDrive/dissertation/similarity_analysis_results/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save similarity matrices\n",
    "np.save(os.path.join(output_dir, 'audio_similarity_matrix.npy'), audio_sim)\n",
    "np.save(os.path.join(output_dir, 'lyrics_similarity_matrix.npy'), lyrics_sim)\n",
    "np.save(os.path.join(output_dir, 'cross_modal_similarity_matrix.npy'), cross_modal_sim)\n",
    "\n",
    "# Save CCA results\n",
    "np.save(os.path.join(output_dir, 'cca_correlations.npy'), cca_corrs)\n",
    "np.save(os.path.join(output_dir, 'audio_canonical.npy'), audio_can)\n",
    "np.save(os.path.join(output_dir, 'lyrics_canonical.npy'), lyrics_can)\n",
    "\n",
    "# Save extracted features\n",
    "np.save(os.path.join(output_dir, 'audio_features.npy'), audio_features)\n",
    "np.save(os.path.join(output_dir, 'lyrics_features.npy'), lyrics_features)\n",
    "\n",
    "# Create summary CSV with per-song similarity scores\n",
    "results_df = analysis_df[[id_column_name, 'valence', 'arousal']].copy()\n",
    "results_df['self_similarity'] = self_sim\n",
    "results_df['valence_predicted'] = predictions[:, 0]\n",
    "results_df['arousal_predicted'] = predictions[:, 1]\n",
    "results_df.to_csv(os.path.join(output_dir, 'similarity_summary.csv'), index=False)\n",
    "\n",
    "# Save metrics summary as JSON\n",
    "metrics = {\n",
    "    'dataset': 'test_set',\n",
    "    'n_songs': len(analysis_df),\n",
    "    'mean_self_similarity': float(self_sim.mean()),\n",
    "    'std_self_similarity': float(self_sim.std()),\n",
    "    'cca_correlation_1': float(cca_corrs[0]),\n",
    "    'cca_mean_correlation': float(cca_corrs.mean()),\n",
    "    'retrieval_audio_to_lyrics': float(retrieval_results['audio_to_lyrics_acc']),\n",
    "    'retrieval_lyrics_to_audio': float(retrieval_results['lyrics_to_audio_acc']),\n",
    "    'retrieval_top1': float(retrieval_results['top1_acc'])\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'metrics_summary.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to: {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - audio_similarity_matrix.npy\")\n",
    "print(f\"  - lyrics_similarity_matrix.npy\")\n",
    "print(f\"  - cross_modal_similarity_matrix.npy\")\n",
    "print(f\"  - cca_correlations.npy\")\n",
    "print(f\"  - audio_canonical.npy\")\n",
    "print(f\"  - lyrics_canonical.npy\")\n",
    "print(f\"  - audio_features.npy\")\n",
    "print(f\"  - lyrics_features.npy\")\n",
    "print(f\"  - similarity_summary.csv\")\n",
    "print(f\"  - metrics_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ‚úÖ ANALYSIS COMPLETE!\n",
    "---\n",
    "---\n",
    "\n",
    "## Summary of What You Have\n",
    "\n",
    "You've now completed similarity analysis using 3 complementary methods:\n",
    "\n",
    "1. **Cosine Similarity**: Measured how similar audio and lyrics features are\n",
    "2. **CCA**: Found shared latent dimensions between modalities\n",
    "3. **Cross-Modal Retrieval**: Tested if audio can predict lyrics\n",
    "\n",
    "## Next Steps for Your Dissertation\n",
    "\n",
    "1. **Interpret Results**: Look at the metrics and visualizations\n",
    "2. **Write Analysis Section**: Use the comprehensive report as a starting point\n",
    "3. **Consider Siamese Networks** (optional): See the guidance below\n",
    "\n",
    "All results are saved to your Google Drive for later analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# OPTIONAL: Siamese Networks for Improved Similarity\n",
    "---\n",
    "---\n",
    "\n",
    "## What is a Siamese Network?\n",
    "\n",
    "Your current analysis uses features optimized for **emotion prediction** (valence/arousal). A Siamese network would learn features specifically optimized for **cross-modal similarity**.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Aspect | Current Analysis | Siamese Network |\n",
    "|--------|------------------|------------------|\n",
    "| **Features** | From emotion prediction model | Learned for similarity |\n",
    "| **Training** | No additional training | Requires training (~4-6 hours) |\n",
    "| **Retrieval** | Baseline (e.g., 20-40%) | Improved (e.g., 60-80%) |\n",
    "| **Complexity** | Simple (done!) | Moderate (~2-3 days work) |\n",
    "\n",
    "### What Would You Gain?\n",
    "\n",
    "1. **Better retrieval accuracy**: Features explicitly trained to match audio-lyrics pairs\n",
    "2. **Learned embeddings**: New embedding space optimized for similarity\n",
    "3. **Novel contribution**: Shows you can design and implement new architectures\n",
    "\n",
    "### Implementation Complexity\n",
    "\n",
    "**Effort**: MODERATE (2-3 days, ~250 new lines of code)\n",
    "\n",
    "**What you'd reuse from MODEL 4**:\n",
    "- ‚úÖ Your VGGish audio model\n",
    "- ‚úÖ Your BERT lyrics model\n",
    "- ‚úÖ Your data loading pipeline\n",
    "- ‚úÖ Your preprocessing code\n",
    "\n",
    "**What you'd need to add**:\n",
    "- Projection heads (map to shared 256-dim space)\n",
    "- InfoNCE loss function (~20 lines)\n",
    "- Training loop (~100 lines)\n",
    "- Evaluation code (~50 lines)\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**For your dissertation, I recommend doing BOTH:**\n",
    "\n",
    "1. **Chapter Section 1**: \"Similarity Analysis of Emotion Features\" (what you just did)\n",
    "   - Shows baseline similarity with existing features\n",
    "   - Quick to complete ‚úÖ\n",
    "\n",
    "2. **Chapter Section 2**: \"Learning Similarity-Optimized Embeddings\" (optional Siamese network)\n",
    "   - Shows improved similarity with learned features\n",
    "   - Demonstrates advanced ML skills\n",
    "\n",
    "This creates a strong narrative: **analyze ‚Üí design improvement ‚Üí demonstrate success**\n",
    "\n",
    "---\n",
    "\n",
    "**Want me to create the Siamese network code?** Let me know and I'll provide:\n",
    "- Complete implementation adapted to MODEL 4\n",
    "- Training script using your existing setup\n",
    "- Evaluation code for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
