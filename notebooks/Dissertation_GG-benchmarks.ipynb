{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9khe/mFQYLPFONcwrpJE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GemmaGorey/Dissertation/blob/main/notebooks/Dissertation_GG-benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Colab setup below - Run once only per session**\n"
      ],
      "metadata": {
        "id": "71xmFnae_UIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Setup - run once per session then run all below\n",
        "\n",
        "# Clone my github\n",
        "!git clone https://github.com/GemmaGorey/Dissertation.git\n",
        "\n",
        "# Install librabries\n",
        "print(\"Installing required library versions...\")\n",
        "# Downgrade spaCy and NumPy to be compatible with PyTorch 2.2.2 and its dependencies as conficts with colab\n",
        "!pip install \"spacy<3.8\" \"numpy<2\" -q\n",
        "\n",
        "# Install PyTorch, torchvision, torchaudio, and librosa\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "!pip install librosa -q\n",
        "print(\"Installation complete.\")\n",
        "\n",
        "# verify GPU and restart trigger\n",
        "import torch\n",
        "import os\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: GPU IS NOT AVAILABLE. Please go to Runtime > Change runtime type and select T4 GPU.\")\n",
        "else:\n",
        "    print(\"\\n GPU is available. Runtime will now restart to load correct library versions.\")\n",
        "    print(\" After restarting, you can run the rest of your notebook cells.\")\n",
        "    # Colab runtime restart.\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKTq_n_S8czf",
        "outputId": "2a5e5f0a-5d18-42e3-e28a-2a3d7e795186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Dissertation'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 49 (delta 16), reused 5 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 15.52 KiB | 3.88 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "Installing required library versions...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstallation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below is the rest of the script**\n",
        "\n",
        "Start once reconnected (tick on top RHS)\n"
      ],
      "metadata": {
        "id": "N-p9PP5E_Sta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "print(\"--- CNN Training Loop Benchmark ---\")\n",
        "\n",
        "# --- 1. Setup Device ---\n",
        "# This code automatically detects and uses the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Benchmark will use device: {device}\")\n",
        "\n",
        "\n",
        "# --- 2. Create Sample Data ---\n",
        "# We'll create a dummy batch of data that looks like spectrograms.\n",
        "# This simulates the input your model would receive.\n",
        "BATCH_SIZE = 32\n",
        "CHANNELS = 1  # Spectrograms are single-channel (like grayscale images)\n",
        "N_MELS = 128  # Height of the spectrogram (number of Mel bands)\n",
        "TIME_FRAMES = 431 # Width of the spectrogram (number of time frames)\n",
        "NUM_CLASSES = 4 # Example: 4 emotion classes\n",
        "\n",
        "# Create the dummy data and labels on the target device\n",
        "dummy_spectrograms = torch.randn(BATCH_SIZE, CHANNELS, N_MELS, TIME_FRAMES).to(device)\n",
        "dummy_labels = torch.randint(0, NUM_CLASSES, (BATCH_SIZE,)).to(device)\n",
        "\n",
        "print(f\"Simulated data shape: {dummy_spectrograms.shape}\")\n",
        "\n",
        "\n",
        "# --- 3. Define a Basic CNN Model ---\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # The input features to the linear layer will depend on the pooling output size\n",
        "        # For our input: 128x431 -> pool1 -> 64x215 -> pool2 -> 32x107\n",
        "        self.fc1 = nn.Linear(32 * 32 * 107, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to the GPU\n",
        "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
        "print(f\"\\nModel loaded onto {device}.\")\n",
        "\n",
        "\n",
        "# --- 4. Setup Training Components ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# --- 5. Run and Time the Training Loop ---\n",
        "print(\"Starting training loop for 10 epochs...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(10):\n",
        "    # In a real scenario, you'd loop through batches of a dataset here.\n",
        "    # For this benchmark, we'll just re-use the same dummy batch.\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(dummy_spectrograms)\n",
        "    loss = criterion(outputs, dummy_labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "print(\"\\n--- Benchmark Complete ---\")\n",
        "print(f\"Total training time for 10 epochs on {device}: {total_duration:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11mk6f3mHi25",
        "outputId": "d9272206-ec2e-452d-83ff-e20a8daf6dc2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CNN Training Loop Benchmark ---\n",
            "Benchmark will use device: cuda\n",
            "Simulated data shape: torch.Size([32, 1, 128, 431])\n",
            "\n",
            "Model loaded onto cuda.\n",
            "Starting training loop for 10 epochs...\n",
            "Epoch [1/10], Loss: 1.4453\n",
            "Epoch [2/10], Loss: 30.1388\n",
            "Epoch [3/10], Loss: 8.8652\n",
            "Epoch [4/10], Loss: 9.0162\n",
            "Epoch [5/10], Loss: 13.1328\n",
            "Epoch [6/10], Loss: 9.9128\n",
            "Epoch [7/10], Loss: 8.0377\n",
            "Epoch [8/10], Loss: 4.3783\n",
            "Epoch [9/10], Loss: 2.3709\n",
            "Epoch [10/10], Loss: 0.8175\n",
            "\n",
            "--- Benchmark Complete ---\n",
            "Total training time for 10 epochs on cuda: 1.20 seconds\n"
          ]
        }
      ]
    }
  ]
}